<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> the probabilistic method is pretty funny | lukshya ganjoo </title> <meta name="author" content="lukshya ganjoo"> <meta name="description" content="reframing conventionally simple computations with limited models of computation"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lukshyaganjoo.github.io/blog/2024/streaming/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "the probabilistic method is pretty funny",
            "description": "reframing conventionally simple computations with limited models of computation",
            "published": "August 03, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> lukshya ganjoo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">mosaic </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>the probabilistic method is pretty funny</h1> <p>reframing conventionally simple computations with limited models of computation</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">introduction</a> </div> <div> <a href="#the-problem">the problem</a> </div> <div> <a href="#a-surprisingly-simple-solution">a surprisingly simple solution</a> </div> <div> <a href="#why-is-an-average-of-minimums-necessary">why is an average of minimums necessary?</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#a-potentially-flawed-solution">a potentially flawed solution</a> </li> </ul> <div> <a href="#the-proof">the proof</a> </div> <div> <a href="#theorems-and-references">theorems and references</a> </div> </nav> </d-contents> <h2 id="introduction">introduction</h2> <p>the object of consideration for today might be slightly different than what you’re used to. the computational model we will be using for this proof is the streaming model. in this model, we are given a sequence of elements in a “stream” and we have to process them one by one. given that the number of elements in our data stream could be significantly larger than the amount of memory we have, we aren’t allowed to store the entire stream in memory, and are instead allowed a small amount of space to store the information we deem relevant. the question now becomes; what functions of the input stream can we compute with what amount of time and model? while we will be focusing on space; similar considerations and conclusions can be made for time. under normal circumstances, the study of such a model would be motivated by introducing the notion of <strong>moments</strong> where you consider the stream as a high-dimensional vector and may want to compute the \(k\)-th moment of the vector. while this is useful, the motivating route we take is slightly different.</p> <h2 id="the-problem">the problem</h2> <blockquote> <p>consider the problem of finding the number of distinct elements in a stream of elements. while this is a trivial problem in the standard computational model since we can simply store the elements and count the number of distinct elements, the streaming model makes this problem significantly more challenging.</p> </blockquote> <blockquote> <p><strong>theorem</strong></p> <p>we present a streaming algorithm to estimate the number of distinct elements in the sequence with multiplicative error \(1 \pm \epsilon\) for some \(\epsilon \in (0, 1)\). for the algorithm, we assume access to \(k\) independent hash functions as described above. the number of hash functions required depends on the desired precision, i.e. \(k \leq \mathcal{O}(1/\epsilon^2)\) is sufficient to achieve the desired error guarantee.</p> </blockquote> <blockquote> <p><strong>definition</strong></p> <p>a <strong>hash function</strong> is a function \(h : A \to B\) where \(A = \{a \in \{0, 1\}^j \mid j \in \mathbb{N}\}\) is the set of all bit sequences of aritrary length and \(B = \{0, 1\}^k\) is the set of all bit sequences of a specific length \(k \in \mathbb{N}\). Note that it is not important to understand deeply the definition of a hash function, rather simply that with the appropriate independence assumptions, it essentially behaves like a uniform random variable.</p> </blockquote> <h2 id="a-surprisingly-simple-solution">a surprisingly simple solution</h2> <p style="overflow-x:auto"> $$ \begin{array}{l} \textbf{function} \; \texttt{estimateDistinct}(\text{stream}, k): \\ \quad \text{initialize } k \text{ independent hash functions } h_1, h_2, \dots, h_k, \text{ where } h_i : \{0, \dots, n - 1\} \to [0, 1] \\ \quad \texttt{val}_1 \leftarrow \infty, \texttt{ val}_2 \leftarrow \infty, \dots, \texttt{ val}_k \leftarrow \infty \\ \quad \text{for } i = 1 \text{ to } n: \\ \quad \quad x \leftarrow \texttt{stream}[i] \\ \quad \quad \text{for } j = 1 \text{ to } k: \\ \quad \quad \quad \texttt{val}_j \leftarrow \min\{\texttt{val}_j, h_j(x)\} \\ \quad Z \leftarrow \frac{1}{k} \sum \limits_{i = 1}^k \texttt{val}_i \\ \quad \text{return } \bigg\lfloor \frac{1}{Z} - 1 \bigg\rfloor \\ \end{array} $$ </p> <p>what the above algorithm is essentially doing is hashing each element in the stream \(k\) times and keeping track of the minimum hash value for each element. it is then taking the average of these minimum hash values and returning the appropriate estimate on the last line of the algorithm.</p> <h2 id="why-is-an-average-of-minimums-necessary">why is an average of minimums necessary?</h2> <p>in order to answer this question, we consider the following modification to the algorithm.</p> <h3 id="a-potentially-flawed-solution">a potentially flawed solution</h3> <p style="overflow-x:auto"> $$ \begin{array}{l} \textbf{function} \; \texttt{incorrectEstimateDistinct}(\text{stream}, k): \\ \quad \text{initialize } k \text{ independent hash functions } h_1, h_2, \dots, h_k, \text{ where } h_i : \{0, \dots, n - 1\} \to [0, 1] \\ \quad \texttt{val}_1 \leftarrow \infty, \texttt{ val}_2 \leftarrow \infty, \dots, \texttt{ val}_k \leftarrow \infty \\ \quad \text{for } i = 1 \text{ to } n: \\ \quad \quad x \leftarrow \texttt{stream}[i] \\ \quad \quad \text{for } j = 1 \text{ to } k: \\ \quad \quad \quad \texttt{val}_j \leftarrow \min\{\texttt{val}_j, h_j(x)\} \\ \quad Z \leftarrow \min \limits_{i \in [k]} \texttt{val}_i \\ \quad \text{return } \bigg\lfloor \frac{1}{Z} - 1 \bigg\rfloor \\ \end{array} $$ </p> <p>it turns out that the above algorithm produces the correct solution on average! (in fact we use this very fact in the proof of the main theorem).</p> <blockquote> <p><strong>lemma 1</strong></p> \[\begin{align*} \text{number of distinct elements in the data stream} = \frac{1}{\mathbb{E}[Z]} - 1\end{align*}\] </blockquote> <h2 id="the-proof">the proof</h2> <h2 id="theorems-and-references">theorems and references</h2> <blockquote> <p><strong>theorem 1</strong></p> <p>Let \(X_1, X_2, \dots, X_n\) be independent random variables uniformly distributed in \([0, 1]\) and let \(X := \min\{X_1, X_2, \dots, X_n\}\). Then</p> \[f_X(x) = \begin{cases}0 &amp; \text{if } x \notin [0, 1] \\ n (1 - x)^{n - 1} &amp; \text{if } x \in [0, 1]\end{cases}\] </blockquote> <p><strong>proof</strong> We proceed as usual; by first computing the CDF, i.e. \(F_X(x) = \Pr[X \leq x]\). We have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[X \leq x] &amp;= 1 - \Pr[X \geq x] &amp;&amp; \text{by the complement rule} \\ &amp;= 1 - \Pr[\min\{X_1, X_2, \dots, X_n\} \geq x] &amp;&amp; \text{by definition of } X \\ &amp;= 1 - \Pr[X_1 \geq x, X_2 \geq x, \dots, X_n \geq x] \\ &amp;= 1 - \prod_{i = 1}^n \Pr[X_i \geq x] &amp;&amp; \text{by independence} \\ &amp;= 1 - \prod_{i = 1}^n (1 - \Pr[X_i \leq x]) &amp;&amp; \text{by the complement rule} \\ &amp;= 1 - \prod_{i = 1}^n (1 - x) &amp;&amp; \text{since } X_i \sim \text{Uniform}(0, 1) \\ &amp;= 1 - (1 - x)^n \end{align*} $$ </p> <p>Therefore we have that</p> \[F_X(x) = \begin{cases} 0 &amp; \text{if } x &lt; 0 \\ 1 - (1 - x)^n &amp; \text{if } x \in [0, 1] \\ 1 &amp; \text{if } x &gt; 1 \end{cases}\] <p>The result thus follows by taking the derivative of \(F_X(x)\) giving us</p> \[f_X(x) = \begin{cases} 0 &amp; \text{if } x \notin [0, 1] \\ n (1 - x)^{n - 1} &amp; \text{if } x \in [0, 1] \end{cases} \text{ as desired.}\] <blockquote> <p><strong>theorem 2</strong></p> <p>let \(X_1, X_2, \dots, X_n\) be independent random variables uniformly distributed in \([0, 1]\). let \(Y = \min \limits_{i \in [n]} X_i\). then \(\mathbb{E}[Y] = \frac{1}{n + 1}\) and \(\text{Var}(Y) \leq \frac{1}{(n + 1)^2}\).</p> </blockquote> <p><strong>proof</strong> we first compute the expectation, by definition</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y] &amp;= \int_{-\infty}^\infty x f_Y(x) dx = \int_{-\infty}^0 x f_Y(x) dx + \int_0^1 x f_Y(x) dx + \int_1^\infty x f_Y(x) dx &amp;&amp;\text{by definition} \\ &amp;= \int_0^1 x n (1 - x)^{n - 1} dx = n \int_0^1 x (1 - x)^{n - 1} dx &amp;&amp; \text{by theorem 1} \\ &amp;= n \int_{0}^{1} (1 - t) t^{n - 1} dt &amp;&amp; \text{by the substitution } t = 1 - x \\ &amp;= n \int_{0}^{1} t^{n - 1} - t^n dt = n \left( \frac{t^n}{n} - \frac{t^{n + 1}}{n + 1}\right) \Bigg|_{0}^{1} \\ &amp;= n \left(\frac{1}{n} - \frac{1}{n + 1}\right) = 1 - \frac{n}{n + 1} = \frac{1}{n + 1} \end{align*} $$ </p> <p>We now turn our attention to computing the variance. We need one more result before we can proceed. We compute \(\mathbb{E}[Y^2]\) and via a similar application of theorem 1 we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y^2] &amp;= \int_{-\infty}^\infty x^2 f_Y(x) dx = \int_{-\infty}^0 x^2 f_Y(x) dx + \int_0^1 x^2 f_Y(x) dx + \int_1^\infty x^2 f_Y(x) dx \\ &amp;= \int_0^1 x^2 n (1 - x)^{n - 1} dx = n \int_0^1 x^2 (1 - x)^{n - 1} dx &amp;&amp; \text{by theorem 1} \\ &amp;= n \int_{0}^{1} (1 - t)^2 t^{n - 1} dt &amp;&amp; \text{by the substitution } t = 1 - x \\ &amp;= n \int_{0}^{1} \bigg(1 - 2t + t^2\bigg) t^{n - 1} dt = n \int_{0}^{1} t^{n - 1} - 2t^n + t^{n + 1} dt \\ &amp;= n \left(\frac{t^n}{n} - \frac{2t^{n + 1}}{n + 1} + \frac{t^{n + 2}}{n + 2}\right)\Bigg|_{0}^{1} \\ &amp;= n \left(\frac{1}{n} - \frac{2}{n + 1} + \frac{1}{n + 2}\right) = n \left(\frac{(n + 1)(n + 2) - 2n(n + 2) + n(n + 1)}{n(n + 1)(n + 2)}\right) \\ &amp;= \frac{n^2 + 3n + 2 - 2n^2 - 4n + n^2 + n}{(n + 1)(n + 2)} = \frac{2}{(n + 1)(n + 2)} \end{align*} $$ </p> <p>We now finally compute the variance, which is given by</p> <p style="overflow-x:auto"> $$ \text{Var}(Y) = \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 = \frac{2}{(n + 1)(n + 2)} - \frac{1}{(n + 1)^2} \leq \frac{2}{(n + 1)^2} - \frac{1}{(n + 1)^2} = \frac{1}{(n + 1)^2} $$ </p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 lukshya ganjoo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>