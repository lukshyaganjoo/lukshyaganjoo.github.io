<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lukshyaganjoo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lukshyaganjoo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-08T00:05:10+00:00</updated><id>https://lukshyaganjoo.github.io/feed.xml</id><title type="html">lukshya ganjoo</title><subtitle>the personal website of lukshya ganjoo </subtitle><entry><title type="html">chebyshev ruins convocations</title><link href="https://lukshyaganjoo.github.io/blog/2024/chebyshev/" rel="alternate" type="text/html" title="chebyshev ruins convocations"/><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/chebyshev</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/chebyshev/"><![CDATA[<h2 id="some-background">some background</h2> <p>i was enrolled in this cs course titled “toolkit for modern algorithms” spring of this year. that class, more than anything else felt like the birth child of a standard algorithms class and techniques you’d learn in machine learning. i have several things i could say about that class, but that’s not what the point of this post is. similar to several other classes in the cs department here, we had weekly assignments. in the penultimate week of the class, there was an extra credit problem assigned that i spent a lot of time pondering about. providing some more context for why the title of this proof/blogpost is what it is, i was not able to figure out the problem in time. to make things more agonizing, i flew to maryland the same week for my brother’s convocation! while it was great seeing my brother graduate, it would have been significantly better had i not spent as much time as i did thinking about how a simple inequality lived in my head rent-free.</p> <h2 id="the-actual-problem">the actual problem</h2> <p>now that i’m done rambling about the context, the problem in and of itself is actually quite interesting, and i think there’s something about the somewhat unusual combination techniques used that is useful to know. the problem is as follows:</p> <blockquote> <p>For any \(n\) real numbers \(a_1, a_2, \dots a_n\) satisfying \(a_1^2 + a_2^2 + \dots + a_n^2 = 1\), if \(\sigma_1 \sigma_2, \dots, \sigma_n \in \{-1, 1\}\) are i.i.d uniform random signs, then</p> <p style="overflow-x:auto"> $$ \Pr \left[\left \vert \sum_{i = 1}^{n} a_i \sigma_i\right \vert \leq 1\right] \geq \frac{1}{32} $$ </p> </blockquote> <h2 id="the-proof">the proof</h2> <p>we split the proof into two cases based on the size of coefficients</p> <blockquote> <p><strong>Case 1: a big coefficient</strong></p> <p>For simplicity, we may sort so that \(\vert a_1 \vert \geq \vert a_2 \vert \geq \dots \vert a_n \vert\).</p> <p>Additionally we define the random variable \(X = \sum \limits_{i = 1}^{n} a_i \sigma_i\). Based on the assumption of the case, we assume without loss of generality that \(\vert a_1 \vert \geq 1/\sqrt{2}\) and apply Chebyshev’s inequality on \(X_1 = X - a_1 \sigma_1\) to conclude that</p> <p style="overflow-x:auto"> $$\Pr[\vert X \vert \leq 1] \geq \frac{1}{4}$$ </p> </blockquote> <p><strong>proof:</strong> We first compute out the expectation of \(X_1\) where we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[X_1] &amp;= \mathbb{E}\left[X - a_1 \sigma_1 \right] &amp;&amp;\text{by definition of } X_1 \\\ &amp;= \mathbb{E}\left[\sum_{i = 1}^{n} a_i \sigma_i - a_1 \sigma_1 \right] &amp;&amp;\text{by definition of } X \\\ &amp;= \mathbb{E}\left[\sum_{i = 2}^{n} a_i \sigma_i\right] \\\ &amp;= \sum_{i = 2}^{n} a_i \mathbb{E}[\sigma_i] = 0 &amp;&amp;\text{linearity of expectation and Theorem 1} \end{align*} $$ </p> <p>Now we define the event \(\mathcal{A}\) as the event that \(X\) and \(\sigma_1 a_1\) have the same sign. Note that since \(\sigma_1\) is a uniformly random sign, \(\Pr[\mathcal{A}] = 1/2\). We therefore have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[\vert X\vert \leq 1] &amp;= \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1] \\\ &amp;= \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}] \cdot \Pr[\mathcal{A}] \\\ &amp;+ \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}^c] \cdot \Pr[\mathcal{A}^c] &amp;&amp;\text{law of total probability} \\\ &amp;\geq \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}] \cdot \Pr[\mathcal{A}] \\\ &amp;= \frac{1}{2} \cdot \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}] &amp;&amp;\text{since } \Pr[\mathcal{A}] = 1/2 \\\ &amp;= \frac{1}{2} \cdot \Pr[\vert X_1\vert - \vert a_1 \vert \leq \vert X_1 + a_1 \sigma_1\vert \leq 1] &amp;&amp;\text{triangle inequality} \\\ &amp;\geq \frac{1}{2} \cdot \Pr[\vert X_1\vert - \vert a_1 \vert \leq 1] \\\ &amp;= \frac{1}{2} \cdot \Pr[\vert X_1\vert \leq 1 + \vert a_1 \vert ] \\\ &amp;= \frac{1}{2} \left(1 - \Pr[\vert X_1\vert \geq 1 + \vert a_1 \vert ]\right) &amp;&amp;\text{complementation} \\\ &amp;\geq \frac{1}{2} \left(1 - \frac{\text{Var}(X_1)}{(1 + \vert a_1 \vert )^2}\right) &amp;&amp;\text{Chebyshev's inequality} \\\ &amp;\geq \frac{1}{2} \left(1 - \frac{1/2}{(1 + \vert a_1 \vert )^2}\right) &amp;&amp;\text{Theorem 2} \\\ &amp;\geq \frac{1}{2} \left(1 - \frac{1/2}{(1 + \vert 1 / \sqrt{2}\vert )^2}\right) &amp;&amp;\text{since } \vert a_1 \vert \geq 1 / \sqrt{2} \\\ &amp;\geq 1/4 \end{align*} $$ </p> <blockquote> <p><strong>Case 2: all small coefficients</strong></p> <p>We now assume that \(1/\sqrt{2} &gt; \vert a_1 \vert \geq \vert a_2 \vert \geq \dots \geq \vert a_n \vert\).</p> <p>The really clever thing we can do is to split the sum \(X = Y + Z\) into two pieces and apply Chebyshev’s inequality separately to \(Y\) and \(Z\) to conclude that</p> <p style="overflow-x:auto"> $$\Pr[\vert X\vert \leq 1] \geq \frac{1}{32}$$ </p> </blockquote> <p><strong>proof:</strong> We define the random variables \(Y, Z\) as follows</p> <p style="overflow-x:auto"> $$ Y = \sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1} \quad \text{and} \quad Z = \sum_{i = 1}^{n/2} a_{2i} \sigma_{2i} $$ </p> <p>We have that</p> <p style="overflow-x:auto"> $$ Y + Z = \sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1} + \sum_{i = 1}^{n/2} a_{2i} \sigma_{2i} = \sum_{i = 1}^{n} a_i \sigma_i = X $$ </p> <p>This necessarily implies as a consequence of the linearity of variance for independent random variables that</p> <p style="overflow-x:auto"> $$ \begin{align} 1 = \text{Var}\left(\sum_{i = 1}^{n} a_i \sigma_i\right)= \text{Var}(X) = \text{Var}(Y) + \text{Var}(Z) \end{align} $$ </p> <p>As a consequence of theorem 4 and \((1)\), we have that</p> <p style="overflow-x:auto"> $$ \text{Var}(Y) + \text{Var}(Z) = 1 \quad \text{and} \quad \text{Var}(Y) - \text{Var}(Z) &lt; 1/2 $$ </p> <p>Putting two and two together, we can finally ascertain that</p> <p style="overflow-x:auto"> $$ 1/4 \leq \text{Var}(Z) \leq \text{Var}(Y) \leq 3/4 $$ </p> <p>We now define the event \(\mathcal{S}\) to be the event that \(Y\) and \(Z\) have different signs. Note that since \(\sigma_1, \sigma_2, \dots, \sigma_n\) are uniformly random signs, \(\Pr[\mathcal{S}] = 1/2\). We therefore have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[\vert X \vert \leq 1] &amp;= \Pr[\vert Y + Z \vert \leq 1] \\\ &amp;= \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}] \cdot \Pr[\mathcal{S}] \\\ &amp;+ \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}^c] \cdot \Pr[\mathcal{S}^c] &amp;&amp;\text{law of total probability} \\\ &amp;\geq \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}] \cdot \Pr[\mathcal{S}] \\\ &amp;= \frac{1}{2} \cdot \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}] &amp;&amp;\text{since } \Pr[\mathcal{S}] = 1/2 \\\ &amp;= \frac{1}{2} \cdot \Pr[\big\vert \vert Y \vert - \vert Z \vert \big\vert \leq \vert Y + Z \vert \leq 1] &amp;&amp;\text{reverse triangle inequality} \\\ &amp;\geq \frac{1}{2} \cdot \Pr[\big\vert \vert Y \vert - \vert Z \vert \big\vert \leq 1] \\\ &amp;\geq \frac{1}{2} \cdot \Pr[\vert Y \vert \leq 1 \cap \vert Z \vert \leq 1] \\\ &amp;= \frac{1}{2} \cdot \Pr[\vert Y \vert \leq 1] \cdot \Pr[\vert Z \vert \leq 1] &amp;&amp;\text{since } Y, Z \text{ are independent} \\\ &amp;\geq \frac{1}{2} \cdot \big(1 - \Pr[\vert Y \vert \geq 1]\big) \cdot \big(1 - \Pr[\vert Z \vert \geq 1]\big) &amp;&amp;\text{complementation} \\\ &amp;= \frac{1}{2} \cdot \big(1 - \Pr[\vert Y - \mathbb{E}[Y]\vert \geq 1]\big) \cdot \\\ &amp;\big(1 - \Pr[\vert Z - \mathbb{E}[Z]\vert \geq 1]\big) &amp;&amp;\text{Theorem 3} \\\ &amp;\geq \frac{1}{2} \cdot \big(1 - \frac{\text{Var}(Y)}{1}\big) \cdot \big(1 - \frac{\text{Var}(Z)}{1}\big) &amp;&amp;\text{Chebyshev's inequality} \\\ &amp;= \frac{1}{2} \cdot \big(1 - \text{Var}(Y)\big) \cdot \big(1 - \text{Var}(Z)\big) \\\ &amp;\geq \frac{1}{2} \frac{1}{4} \frac{1}{4} = \frac{1}{32} \end{align*} $$ </p> <h2 id="theorems-and-references">theorems and references</h2> <blockquote> <p><strong>theorem 1</strong></p> <p>For uniformly random signs \(\sigma_1, \sigma_2 \dots, \sigma_n \in \{-1, 1\}\), \(\text{Var}(\sigma_i) = 1\) for all \(i \in [n]\) and \(\mathbb{E}[\sigma_i] = 0\).</p> </blockquote> <p><strong>proof:</strong> We first compute the expectation of \(\sigma_i\) which by definition comes out to be</p> <p style="overflow-x:auto"> $$ \mathbb{E}[\sigma_i] = \sum_{x \in \Omega_X} x \cdot \Pr[X = x] = 1/2 - 1/2 = 0 $$ </p> <p>The second step in computing the variance of \(\sigma_i\) is computing out \(\mathbb{E} [\sigma_i^{2}]\) which by definition is simply</p> <p style="overflow-x:auto"> $$ \mathbb{E}[\sigma_i^2] = \sum_{x \in \Omega_X} x^2 \cdot \Pr[X = x] = 1/2 + 1/2 = 1 $$ </p> <p>Therefore we have from the definition of variance that</p> <p style="overflow-x:auto"> $$ \text{Var}(\sigma_i) = \mathbb{E}[\sigma_i^2] - \mathbb{E}[\sigma_i]^2 = 1 - 0 = 1 $$ </p> <blockquote> <p><strong>theorem 2</strong></p> <p>If \(X = \sum \limits_{i = 1}^{n} a_i \sigma_i\) for uniformly random signs \(\sigma_1, \sigma_2, \dots, \sigma_n \in \{-1, 1\}\), where \(\vert a_1 \vert \geq 1/\sqrt{2}\) and \(X_1 = X - a_1 \sigma_1\), then we can establish an upper bound on the variance, more concretely, \(\text{Var}(X_1) \leq 1/2\). Note here that in this setting we also have that \(a_1^2 + \dots + a_n^2 = 1\).</p> </blockquote> <p><strong>proof:</strong> Note that the setting we’re interested in is when the \(\sigma_i\)’s are independently and identically distributed which makes our life a lot easier. We therefore have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(X_1) &amp;= \text{Var}\bigg(X - a_1 \sigma_1\bigg) &amp;&amp;\text{by definition of } X \\ &amp;= \text{Var}\left(\sum_{i = 1}^{n} a_i \sigma_i - a_1 \sigma_1\right) &amp;&amp;\text{by definition of } X_1 \\ &amp;= \text{Var}\left(\sum_{i = 2}^{n} a*i \sigma_i\right) &amp;&amp;\text{simplifying} \\ &amp;= \sum_{i = 2}^{n} \text{Var}(a_i \sigma_i) = \sum_{i = 2}^{n} a_i^2 \text{Var}(\sigma_i) &amp;&amp;\text{independence} \\ &amp;= \sum_{i = 2}^{n} a_i^2 = \sum_{i = 1}^{n} a_i^2 - a_1^2 &amp;&amp;\text{Theorem 1} \\ &amp;= 1 - a_{1}^2 = 1 - \vert a_{1}\vert ^{2} &amp;&amp;\text{since } a_1 \in \mathbb{R} \end{align*} $$ </p> <p>The rest of our argument follows from a simple bounding argument on \(\vert a_1 \vert\) therefore giving us</p> <p style="overflow-x:auto"> $$ \vert a_1 \vert \geq 1/\sqrt{2} \implies \vert a_1 \vert ^2 \geq 1/2 \implies - \vert a_1 \vert ^2 \leq 1/2 \implies \text{Var}(X_1) = 1 - \vert a_1 \vert ^2 \leq 1/2 $$ </p> <blockquote> <p><strong>theorem 3</strong></p> <p>If we define the random variable \(Y = \sum \limits_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1}\) and \(Z = \sum \limits_{i = 1}^{n/2} a_{2i} \sigma_{2i}\), then we have that \(\mathbb{E}[Y] = 0\) and \(\mathbb{E}[Z] = 0\).</p> </blockquote> <p><strong>proof:</strong> We have as a consequence of Theorem 1 and linearity of expectation that</p> <p style="overflow-x:auto"> $$ \mathbb{E}[Y] = \mathbb{E}\left[\sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1}\right] = \sum_{i = 1}^{n/2} a_{2i - 1} \mathbb{E}[\sigma_{2i - 1}] = 0 $$ </p> <p>Similarly, we have that</p> <p style="overflow-x:auto"> $$ \mathbb{E}[Z] = \mathbb{E}\left[\sum_{i = 1}^{n/2} a_{2i} \sigma_{2i}\right] = \sum_{i = 1}^{n/2} a_{2i} \mathbb{E}[\sigma_{2i}] = 0 $$ </p> <blockquote> <p><strong>theorem 4</strong></p> <p>Based on the above definitions of \(Y\) and \(Z\), we have that</p> <p style="overflow-x:auto"> $$\text{Var}(Y) - \text{Var}(Z) &lt; 1/2$$ </p> </blockquote> <p><strong>proof:</strong> We have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(Y) - \text{Var}(Z) &amp;= \text{Var}\left(\sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1}\right) - \text{Var}\left(\sum_{i = 1}^{n/2} a_{2i} \sigma_{2i}\right) \\ &amp;= \sum_{i = 1}^{n/2} a_{2i - 1}^2 \text{Var}(\sigma_{2i - 1}) - \sum_{i = 1}^{n/2} a_{2i}^2 \text{Var}(\sigma_{2i}) \\ &amp;= \sum_{i = 1}^{n/2} a_{2i - 1}^2 - \sum_{i = 1}^{n/2} a_{2i}^2 = \sum_{i = 1}^{n/2} (a_{2i - 1}^2 - a_{2i}^2) &amp;&amp;\text{Theorem 1} \\ &amp;= a_1^2 + a_3^2 + \dots + a_{n - 1}^2 - a_2^2 - a_4^2 - \dots - a_n^2 \ &amp;= a_1^2 + (a_3^2 - a_2^2) + (a_5^2 - a_4^2) + \dots + (a_{n - 1}^2 - a_n^2) \\ &amp;= a_1^2 + \sum_{i = 2}^{n/2} \underbrace{(a_{2i - 1}^2 - a_{2i - 2}^2)}_{&lt; 0} &lt; a_1^2 &lt; 1/2 \end{align*} $$ </p>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[i spent far too much time looking at normal distributions for this one]]></summary></entry><entry><title type="html">cauchy schwarz and beyond</title><link href="https://lukshyaganjoo.github.io/blog/2024/cauchy/" rel="alternate" type="text/html" title="cauchy schwarz and beyond"/><published>2024-07-30T00:00:00+00:00</published><updated>2024-07-30T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/cauchy</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/cauchy/"><![CDATA[<p>Let’s start from the beginning. We will state some different forms of the cauchy schwarz inequality, prove the form involving dot products and cover what I think is a really cool problem that uses the inequality.</p> <p>For the different forms of the cauchy schwarz inequality, we have</p> <ul> <li>Probability Theory: \(\lvert \mathbb{E}(XY) \rvert^{2} \leq \mathbb{E}(X^{2}) \mathbb{E}(Y^{2})\).</li> <li>\(\mathbb{R}^2\)-dot product: \(\vert {\bf a} \cdot {\bf b} \vert^{2} \leq ({\bf a} \cdot {\bf a}) \; ({\bf b} \cdot {\bf b})\) .</li> <li>\(R^{n} - \; n\) - dimensional Euclidean Space: \(\left(\sum \limits_{i = 1}^{n} u_{i} v_{i}\right)^{2} \leq \left(\sum \limits_{i = 1}^{n} u_i\right)^{2} \left(\sum \limits_{i = 1}^{n} v_i\right)^{2}\)</li> </ul> <blockquote> <p>cauchy schwarz theorem</p> <p>For all vectors \({\bf a}\) and \({\bf b}\) in \(\mathbb{R}^{n}\), we have that</p> <p style="overflow-x:auto"> $$\vert\ {\bf a} \cdot {\bf b} \vert^{2} \leq \vert \vert {\bf a} \vert \vert^{2} \vert \vert {\bf b} \vert \vert^{2}$$ </p> </blockquote> <p><strong>proof 1:</strong></p> <p>Let \(\mathbf{a}\) and \(\mathbf{b}\) be vectors living in \(\mathbb{R}^{n}\) such that \(\mathbf{a} = \langle a_1, a_2, a_3, \dots, a_n \rangle\) and \(\mathbf{b} = \langle b_1, b_2, b_3, \dots, b_n \rangle\). We first simplify both sides of the inequality, and proceed from there.</p> <p>We have that,</p> <p style="overflow-x:auto"> $$ \begin{align*} \lvert\ {\bf a} \cdot {\bf b}\rvert^{2} &amp;= ({\bf a} \cdot {\bf b}) ({\bf a} \cdot {\bf b}) \\ &amp;= \langle a_1, a_2, a_3, \dots, a_n \rangle \langle b_1, b_2, b_3, \dots, b_n \rangle \\ &amp;= (a_1 b_1 + a_2 b_2 + \dots + a_n b_n) (a_1 b_1 + a_2 b_2 + \dots + a_n b_n) \\ &amp;= (a_1 b_1 + a_2 b_2 + \dots + a_n b_n)^{2} \\ &amp;= \left(\sum_{i = 1}^{n} a_i b_i\right)^2 = D^2 \\ ({\bf a} \cdot {\bf a}) &amp;= \langle a_1, a_2, a_3, \dots, a_n \rangle \langle a_1, a_2, a_3, \dots, a_n \rangle \\ &amp;= a_{1}^{2} + a_{2}^{2} + \dots a_{n}^{2} \\ &amp;= \sum_{i = 1}^{n} a_{i}^{2} = A \\ ({\bf b} \cdot {\bf b}) &amp;= \langle b_1, b_2, b_3, \dots, b_n \rangle \langle b_1, b_2, b_3, \dots, b_n \rangle \\ ({\bf b} \cdot {\bf b}) &amp;= b_{1}^{2} + b_{2}^{2} + \dots + b_{n}^{2} \\ &amp;= \sum_{i = 1}^{n} b_{i}^{2} = B \end{align*} $$ </p> <p>So we are therefore tasked with proving the following inequality, \(D^{2} \leq AB\)</p> <p>Rearranging, we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} &amp;\ D^{2} \leq AB \\ &amp;\ D^{2} - AB \leq 0 \\ &amp;\ 4D^{2} - 4AB \leq 0 &amp;&amp;\text{Multiplying both sides by 4} \\ &amp;\ (2D)^{2} - 4AB \leq 0 \end{align*} $$ </p> <p>One of the coolest thing about math to me is how you can transform certain kinds of problems into problems that might be easier to solve or perhaps represent. This is one of those moments. This might look familiar to some people already but given a quadratic equation \(ax^{2} + bx + c = 0\), the discrimanant \(D_{Q}\) is given by \(b^{2} - 4ac\). The entire reason I changed the form of the inequalitywe had to prove was so that it could better mimic this form of the discriminant for something as widely ubiquitous as the quadratic formula.</p> <p>Given this observation, we can prove an equivalent statement to the inequality we wish to show, namely that the quadratic equation given by \(Ax^2 + 2Dx + B\) has no real roots or exactly one real root. This is because the discrimanant of this quadratic equation is given by \(4D^{2} - 4AB\), and the same would imply that \(4D^{2} - 4AB \leq 0\), which is exactly what we want :)</p> <p>Substituting in the values for \(A\), \(B\), and \(D\), we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} Ax^2 + 2Dx + B &amp;= \left(a_1^2 + \dots + a_n^2\right) x^2 + 2 \left(a_1 b_1 + \dots + a_n b_n\right) x + \left(b_1^2 + \dots + b_n^2\right) \\ &amp;= \left(a_1^2 x^2 + \dots + a_n x^2\right) + 2 \left(a_1 b_1 x + \dots + a_n b_n x\right) + \left(b_1^2 x^2 + \dots + b_n^2 x^2\right) \\ &amp;= \left(a_1^2 x^2 + 2a_1 b_1 x + b_1^2\right) + \dots + \left(a_n^2 x^2 + 2a_n b_n x + b_n^2\right) \\ &amp;= \left(a_1 x + b_1\right)^2 + \dots + \left(a_n x + b_n\right)^2 \\ &amp;\geq 0 \end{align*} $$ </p> <ul> <li><strong>case 1:</strong> \(Ax^2 + 2Dx + B &gt; 0\) In this case, we have that the quadratic equation has no real roots, which is exactly one of the conditions we want.</li> <li><strong>case 2:</strong> \(Ax^2 + 2Dx + B = 0\) Note that this is equivalent to saying that \(\left(a_1 x + b_1\right)^2 + \dots + \left(a_n x + b_n\right)^2 = 0\) which necessarily implies that for all \(i \in \{1, 2, \dots, n\}\), \(a_i x + b_i = 0\), which implies that the equation only has one real root, thereby satisfying the second condition we want and completing the proof.</li> </ul> <p><strong>proof 2:</strong> This is a proof that at least to me feels slightly more natural, and personally I think this proof technique is cooler (I may be biased here though).</p> <p>We begin by defining the function \(f(t) = \|\mathbf{x} - t\mathbf{y}\|^2 = \langle \mathbf{x} - t\mathbf{y}, \mathbf{x} - t\mathbf{y} \rangle\). Simplifying, this gives us</p> <p style="overflow-x:auto"> $$ \begin{align*} f(t) &amp;= \langle \mathbf{x} - ty, \mathbf{x} - t\mathbf{y} \rangle \\ &amp;= \langle \mathbf{x}, \mathbf{x} - t\mathbf{y} \rangle - t \langle y, \mathbf{y} - t\mathbf{y} \rangle \\ &amp;= \langle \mathbf{x}, \mathbf{x} \rangle - t\langle \mathbf{x}, \mathbf{y} \rangle - t \left(\langle \mathbf{y}, \mathbf{x} \rangle - t \langle \mathbf{y}, \mathbf{y} \rangle \right) \\ &amp;= \langle \mathbf{x}, \mathbf{x} \rangle - 2t \langle \mathbf{x}, \mathbf{y} \rangle + t^2 \langle \mathbf{y}, \mathbf{y} \rangle \\ &amp;= \|\mathbf{x}\|^2 - 2t \langle \mathbf{x}, \mathbf{y} \rangle + t^2 \|\mathbf{y}\|^2 \end{align*} $$ </p> <p>Since \(f(t) = \|\mathbf{x} - t\mathbf{y}\|^2\), we have that \(f(t) \geq 0\) for all \(t \in \mathbb{R}\). We make the clever observation of exploiting this fact for the minima of \(f\), and proceed to find the derivative, set it equal to 0, and solve for \(t_{\text{min}}\).</p> <p style="overflow-x:auto"> $$ \begin{align*} f'(t) &amp;= -2 \langle \mathbf{x}, \mathbf{y} \rangle + 2t \|\mathbf{y}\|^2\\ 0 &amp;= -2 \langle \mathbf{x}, \mathbf{y} \rangle + 2t_{\text{min}} \|\mathbf{y}\|^2 &amp;&amp;\text{Setting $$f'(t) = 0$$} \\ -2 \langle \mathbf{x}, \mathbf{y} \rangle &amp;= 2t_{\text{min}} \|\mathbf{y}\|^2 \\ \langle \mathbf{x}, \mathbf{y} \rangle &amp;= t_{\text{min}} \|\mathbf{y}\|^2 &amp;&amp;\text{Dividing both sides by 2} \\ t_{\text{min}} &amp;= \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2} &amp;&amp;\text{Dividing both sides by $$|\mathbf{y}|^2$$} \end{align*} $$ </p> <p>Evaluating, we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} f(t_{\text{min}}) &amp;= \|\mathbf{x}\|^2 - 2t_{\text{min}} \langle \mathbf{x}, \mathbf{y} \rangle + t_{\text{min}}^2 \|\mathbf{y}\|^2 \\ &amp;= \|\mathbf{x}\|^2 - 2 \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2} \langle \mathbf{x}, \mathbf{y} \rangle + \left(\frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2}\right)^2 \|\mathbf{y}\|^2 \\ &amp;= \|\mathbf{x}\|^2 - 2 \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2} \langle \mathbf{x}, \mathbf{y} \rangle + \left(\frac{\langle \mathbf{x}, \mathbf{y} \rangle^2}{\|\mathbf{y}\|^2}\right) \\ &amp;= \frac{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2 - 2 \langle \mathbf{x}, \mathbf{y} \rangle^2 + \langle \mathbf{x}, \mathbf{y}\rangle^2}{\|\mathbf{y}\|^2} \\ &amp;= \frac{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2 - \langle \mathbf{x}, \mathbf{y} \rangle^2}{\|\mathbf{y}\|^2} \geq 0 \\ &amp;\implies \|\mathbf{x}\|^2 \|\mathbf{y}\|^2 - \langle \mathbf{x}, \mathbf{y}\rangle^2 \geq 0 \\ &amp;\implies \langle \mathbf{x}, \mathbf{y} \rangle \leq \|\mathbf{x}\| \|\mathbf{y}\| \end{align*} $$ </p> <blockquote> <p><strong>fun lemma :D</strong></p> <p>For a non-negative random variable \(Y\), we have that</p> <p style="overflow-x:auto"> $$\frac{\mathbb{E}[Y]^2}{\mathbb{E}[Y^2]} \leq \Pr(Y \neq 0) \leq \mathbb{E}[Y]$$ </p> </blockquote> <p><strong>proof:</strong> We first prove the right-hand side of the inequality, i.e. \(\Pr(Y \neq 0) \leq \mathbb{E}[Y]\). Using the definition of expectation, we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y] &amp;= \sum_{y \in \Omega_{y}} y \cdot \Pr(Y = y) \\ &amp;= \sum_{\substack{y \in \Omega_{y} \\ y \neq 0}} y \cdot \Pr(Y = y) \geq \sum_{y \in \Omega_{y}, y \neq 0} \Pr(Y = y) \geq \Pr(Y \neq 0) \end{align*} $$ </p> <p>We now prove the left-hand side of the inequality, Consider the expression \(\mathbb{E}[Y^2] \Pr(Y \neq 0)\). We have that the following is true</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y]^2 \Pr(Y \neq 0) &amp;= \sum_{y \neq 0} \underbrace{\Pr(Y = y)}_{a_i = \sqrt{\Pr(Y = y)}} \; \sum_{y \neq 0} \underbrace{y^{2} \Pr(Y = y)}_{b_i = y \sqrt{\Pr(Y = y)}} \\ &amp;\geq \left(\sum_{y \neq 0} y \sqrt{\Pr(Y = y)} \sqrt{\Pr(Y = y)}\right)^2 &amp;&amp;\text{Via Cauchy Schwarz} \\ &amp;\geq \left(\sum_{y \neq 0} y \Pr(Y = y)\right)^2 = \mathbb{E}[Y]^2 \end{align*} $$ which implies the result. </p>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[The object of today's study is the cauchy schwarz inequality specifically the version involving dot products. For those unaware, the cauchy schwarz is probably one of the most broken lemmas in mathematics. So ubiquitious is this theorem, that it is given different names depending on the form of the inequality we use and for what purpose specifically we're using it.]]></summary></entry><entry><title type="html">so i gave a theory talk</title><link href="https://lukshyaganjoo.github.io/blog/2024/so-i-gave-a-theory-talk/" rel="alternate" type="text/html" title="so i gave a theory talk"/><published>2024-07-30T00:00:00+00:00</published><updated>2024-07-30T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/so-i-gave-a-theory-talk</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/so-i-gave-a-theory-talk/"><![CDATA[<h2 id="notation-and-some-motivation">notation and some motivation</h2> <h3 id="notation">notation:</h3> <p>Before I get to some of the really cool motivation for this problem, I wanna make sure that everyone here is familiar with the the kinds of graphs we’ll be working with in the domain of this proof.</p> <p style="text-align:center;"> <img src="/assets/img/Petersen1_tiny.png" alt="petersen graph" style="width: 100%; margin: auto;"/> </p> <p>This is the Petersen graph. Veteran graph theoreticians will be familiar with this pentagram inscribed within the spokes of a regular pentagon. The Petersen graph is used as a counter example in tons of graph theory proofs which is one of the reasons it’s so well studied, partly what we’ll be doing here. The Petersen graph has \(10\) nodes and \(15\) edges, and is what we call \(3\)-regular.</p> <p><strong>Note:</strong> A graph is said to be \(d\)-regular iff every vertex has the same degree \(d\). (i.e. it is strongly regular)</p> <p style="text-align:center;"> <img src="/assets/img/full.png" alt="complete graph" style="width: 100%; margin: auto;"/> </p> <p>This is the complete graph on \(10\) vertices. For the sake of convenience, we will refer to this graph as \(K_{10}\). It has \(10\) nodes and \(45\) edges (i.e., all possible edges among \(10\) nodes). If we follow along from the same scheme of defining things, the number of edges in \(K_{n}\) (the complete graph with \(n\) vertices) is given by \(\binom{n}{2}\).</p> <p>In this proof, \(\mathbf{J}_{n}\) refers to the all ones matrix with dimension/size \(n \times n\).</p> <p>The row space of a matrix \(\mathbf{A}\) is defined as follows, \(\text{Row}(A) = \\{\mathbf{y}^{\mathbf{\top}} A : \mathbf{y} \in \mathbb{R}^{n}\\}\)</p> <blockquote> <p><strong>definition</strong></p> <p style="overflow-x:auto"> $$ \textsf{Trace}(\mathbf{A}) = \sum \limits_{x \in E_{x}(\mathbf{A})} \lambda_{x} $$ </p> </blockquote> <blockquote> <p><strong>lemma 1</strong></p> <p>For a symmetric matrix \(\mathbf{A}\), eigenvectors corresponding to <em>distinct</em> eigenvalues are orthogonal</p> </blockquote> <p><strong>proof</strong><br/> Let \(\mathbf{x}\) and \(\mathbf{y}\) be eigenvectors of the matrix \(\mathbf{A}\) corresponding to eigenvalues \(\lambda_1\) and \(\lambda_2\) respectively. Therefore we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbf{y}^{\top} \mathbf{Ax} = \mathbf{y}^{\top} \lambda_1 \mathbf{x} &amp;\implies \mathbf{y}^{\top} \mathbf{A}^\top \mathbf{x} = \mathbf{y}^{\top} \lambda_1 \mathbf{x} \\ &amp;\implies \mathbf{(Ay)}^{\top} \mathbf{x} = \lambda_1 \mathbf{y}^{\top}\mathbf{x} \\ &amp;\implies (\lambda_2 \mathbf{y})^\top \mathbf{x} = \lambda_2 \mathbf{y}^{\top}\mathbf{x} \\ &amp;\implies \mathbf{y}^{\top}\mathbf{x} (\lambda_2 - \lambda_1) = 0 \end{align*} $$ </p> <p>However, we already know that \(\lambda_2 \neq \lambda_1\). Therefore \(\lambda_2 - \lambda_1 \neq 0\), and it must be the case that \(\mathbf{y}^{\top}\mathbf{x} = 0\).</p> <p>Therefore \(x\) and \(y\) are orthogonal.</p> <p><strong>adjacency matrices:</strong></p> <blockquote> <p><strong>definition</strong></p> <p>We define the adjacency matrix of a graph \(G = (V, E)\) as the following,</p> <p style="overflow-x:auto"> $$ A_{ij} = \begin{cases}1 &amp; \text{there is an edge from} \; j \; \text{to} \; i\\0 &amp; \text{there is no edge from} \; j \; \text{to} \; i\\ \end{cases} $$ </p> </blockquote> <p>Since we’re dealing with an undirected graph in this proof, every edge \((a, b)\) is counted as the same edge as if there were an edge \((b, a)\) in the graph. As a consequence, the adjacency matrices for this family of graphs will be symmetric.</p> <p>The way to think about it is that the adjacency matrix of a graph \(G\) encodes information about the number of length \(1\) paths between any pair of vertices \(i\) and \(j\), i.e. information about the vertices immediately “adjacent” to each other.</p> <h3 id="motivation">motivation:</h3> <p>Each node in \(K_{10}\) has \(9\) edges incident to it while each node in the Petersen graph has \(3\) edges incident to it. So it is plausible that \(K_{10}\) can be covered perfectly by \(3\) Petersen graphs. This means that you can lay down three Petersens on \(K_{10}\) so that vertices go to vertices and each edge of \(K_{10}\) lies under an edge of exactly one of the three Petersens. This hints at some pretty cutting-edge stuff in the field of graph decomposition and graph coloring and the parallels in higher dimensions is absolutely incredible. With some linear algebra at our disposal, we will show that it is not possible to completely cover \(K_{10}\) with three Petersen graphs.</p> <hr/> <h2 id="eigenvalues-of-the-petersen-graph">eigenvalues of the petersen graph</h2> <p>Finding the eigenvalues of any matrix, let alone one that of size \(10 \times 10\) is a fairly computational task and isn’t enlightening in any form whatsoever. So why are we trying to find the eigenvalues of the petersen graph? Well we can take advantage of the fact that the adjacency matrix represents information that carries physical meaning (vertex-edge relations of a graph) and can use the language of linear transformations and some geometric intuition to gain insight into the eigenvalues of this graph.</p> <p>Let \(\mathbf{A}\) be the adjacency matrix of the Petersen Graph. If \(\mathbf{A}\) encodes information about the number of length \(1\), \(\mathbf{A}^2\) represents information about the number of length \(2\) paths, and in general, \(A^k\) encodes information about the number of \(k\)-length paths in a given graph. With this information in our belt, we know can make a pretty nifty observation.</p> <p>Consider the \(i, j^{th}\) entry of the matrix \(\mathbf{A}^2 + \mathbf{A}\) (the sum of the matrices representing information about the number of length \(2\) paths and the regular adjacency matrix of the petersen graph). Since, every vertex in the petersen graph has degree \(3\), each vertex has \(3\) length \(2\) paths back itself whereas there are no length \(1\) paths from a vertex to itself. With this information under our belt, we can say that</p> <p style="overflow-x:auto"> $$ (A^2 + A)_{ij} = \begin{cases} 3 &amp; \forall i = j \\ 1 &amp; \forall i \neq j \end{cases} $$ </p> <p>This follows from noticing that any pair of vertices not already joined by an edge are joined by a unique path of length \(2\); namely there is a unique vertex which is joined to both.</p> <p>We can therefore conclude that the matrix \(\mathbf{}\) satisfies the matrix equation \(\mathbf{A}^2 + \mathbf{A} = 2\mathbf{I} + \mathbf{J}\)</p> <p>Note that \(\mathbf{J}\) has eigenvalue 10 (of multiplicity 1 with eigenvector 1) and eigenvalue 0 of multiplicity 9. Thus \(2\mathbf{I} + \mathbf{J}\) has eigenvalue 12 of multiplicity 1 and eigenvalue 2 of multiplicity 9. Now if \(x\) is an eigenvector of A of eigenvalue \(\lambda\) then \(\mathbf{x}\) is an eigenvector of \(\mathbf{A}^2 + \mathbf{A}\) of eigenvalue \(\lambda^2 + \lambda\).</p> <p>Thus the possible eigenvectors for \(\mathbf{A}\) are prescribed. We already know that \(\mathbf{1}\) is an eigenvector of \(\mathbf{A}\) of eigenvalue 3 (or at least it is easy to check). The other 9 eigenvalues must satisfy \(\lambda^2 + \lambda = 2\) thus either 1 or -2 with total multiplicity being 9. Now the trace of \(\mathbf{A}\), which is 0, is the sum of the eigenvalues and so we deduce that 1 has multiplicity 5 and -2 has multiplicity 4.</p> <hr/> <h2 id="the-actual-proof">the actual proof</h2> <blockquote> <p><strong>lemma 2</strong></p> <p>If \(\mathbf{K}_{10}\) represents the adjaceny matrix of the complete graph on 10 vertices, \(\mathbf{J}_{10}\) represents the matrix with all its entries being equal to \(1\) and \(\mathbf{I}_{10}\) represents the identity matrix of size \(10 \times 10\), then</p> <p style="overflow-x:auto"> $$ \begin{align*}\mathbf{K}_{10} = \mathbf{J}_{10} - \mathbf{I}_{10}\end{align*} $$ </p> </blockquote> <p><strong>proof:</strong> The adjacency matrix of \(K_{10}\) has all but its diagonal entries being equal to \(1\), since every node in the full graph is connected to every other node except itself. The diagonal entries of \(\mathbf{K}_{10}\) are filled in with \(0\)’s since there is no path from any of the nodes to itself. One can also clearly see that the matrix described by the equation \(\mathbf{J}_{10} - \mathbf{I}_{10}\) is a matrix that has all but its diagonal entries being equal to \(1\) and its diagonal entries being equal to \(0\).</p> <p>Therefore the adjacency matrix of \(\mathbf{K}_{10} = \mathbf{J}_{10} - \mathbf{I}_{10}\)</p> <blockquote> <p><strong>Assumption:</strong></p> <p style="overflow-x:auto:"> $$ \begin{align*}\mathbf{A}_P + \mathbf{A}_Q + \mathbf{A}_R = J_{10} - I_{10}\end{align*} $$ </p> </blockquote> <p>We consider three permutation matrices of the Petersen Graph \(A_P, A_Q\) and \(A_R\) (matrices formed by simultaneous row and column swaps in the original adjacency matrix of the petersen graph). We construct each of these permutation matrices such that \(A_Q = CAC^{-1}\) where \(C\) is some matrix. By this construction, \(A_Q\) and \(A\) are similar, and an important consequence is that they have the same eigenvalues. This assumption and construction is true for \(A_P\) and \(A_R\).</p> <blockquote> <p><strong>lemma 3</strong></p> <p style="overflow-x:auto"> $$ \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} $$ </p> </blockquote> <p><strong>proof:</strong> We wish to show that \(\mathbf{1}^{\top} = \mathbf{y}^{\top}(\mathbf{A}_P - I_{10})\) for some vector \(\mathbf{y} \in R^{10}\). Consider \(\mathbf{y} = \mathbf{1}^{\top}\). The matrix product \(\mathbf{1}^{\top}(\mathbf{A}_P - \mathbf{I}_{10})\) yields the vector whose entries are the sum of the entries of the columns of \(\mathbf{A}_P - \mathbf{I}_{10}\). for an arbitrary column \(i\) in its adjacency matrix, by the way the Petersen graph was defined we know that he graph has 3 outgoing edges which means that the sum of the entries of a column in its adjacency matrix is 3. However note that the diagonal entries of the adjacency matrix will be 0 since the Petersen graph does not contain self-loops. After subtracting the \(i\)th column of \(\mathbf{I}_{10}\), the sum of the columns of the matrix \(\mathbf{A}_P - \mathbf{I}_{10}\) is 2. Note that the identity matrix has 0 in every entry where \(i \neq j\). Since the above holds true for an arbitrary column of the matrix, We can conclude that every column of \(\mathbf{A}_P - \mathbf{I}_{10}\) sums up to a 2. More fundamentally, the product \(\mathbf{1}^{\top}(\mathbf{A}_{P} - \mathbf{I}_{10})\) generates the all twos vector or 2\(\mathbf{1}^{\top}\).</p> <p>Therefore we can assert that \(\text{Span}(\mathbf{1}) \subseteq \text{Row} (\mathbf{A}_P - \mathbf{I}_{10})\).</p> <blockquote> <p><strong>side proof</strong></p> <p>Let \(A\) and \(B\) be two subspaces such that \(A \subseteq B\), then</p> <p style="overflow-x:auto"> $$ B^{\perp} \subseteq A^{\perp} $$ </p> </blockquote> <p><strong>proof</strong> Let \(\mathbf{x} \in B^{\perp}\). By definition of the orthogonal complement, we have that \(\forall \mathbf{v} \in B, \mathbf{x}^{\top}\mathbf{v} = 0\). However we know that any vector contained in \(B\) will be a vector contained in \(A\), (\(A\) is a subset of the vectors contained in \(B\)). Therefore \(\mathbf{x}\) is orthogonal to every vector in \(A\). By definition, this means that \(\mathbf{x} \in A^{\perp}\) \(\therefore A \subseteq B \implies B^{\perp} \subseteq A^{\perp}\)</p> <p>Using the aforementioned proof and taking orthogonal complements, we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Span}(\mathbf{1}) \subseteq \text{Row} (\mathbf{A}_P - \mathbf{I}_{10}) &amp;\implies \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} \\ &amp;\implies \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} \end{align*} $$ </p> <p>The above results are also true for \(\mathbf{A}_Q - \mathbf{I}_{10}\) since \(Q\) is also a Petersen graph.</p> <blockquote> <p><strong>lemma 4</strong></p> <p>Assuming the standard definitions of the matrices \(A_P, A_Q\) and \(A_R\), we have that</p> <p style="overflow-x:auto"> $$ \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \cap \text{Null} (\mathbf{A}_Q - \mathbf{I}_{10}) \neq \{0\} $$ </p> </blockquote> <p><strong>proof</strong> For two subspaces \(R_1\) and \(R_2\) of \(\mathbb{R}^{9}\), if \(S_1 \cap S_2 = \{0\}\), then \(B_{S_1}\) and \(B_{S_2}\) are linearly independent. We therefore prove by contrapositive.</p> <p>We also know that</p> <p style="overflow-x:auto"> $$ \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} \quad \text{and} \quad \text{Null} (\mathbf{A}_Q - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} $$ </p> <p>\(B_{\text{Null} (\mathbf{A}_P - \mathbf{I}_{10})} \cup B_{\text{Null} (\mathbf{A}_Q - \mathbf{I}_{10})}\) has 10 vectors in \(\mathbb{R}^{9}\) since the bases of each of the null spaces has 5 vectors. Therefore \(B_{\text{Null} (\mathbf{A}_P - \mathbf{I}_{10})} \cup B_{\text{Null} (\mathbf{A}_Q - \mathbf{I}_{10})}\) must be linearly dependent (since they have more vectors than the dimension of the space they exist in).</p> <p>Therefore \(B_{S_1} \cap B_{S_2} \neq \{0\}\) where \(S_1\) and \(S_2\) refer to the two subspaces above. This implies the existence of a non-zero vector \(w\) that exists in their intersection.</p> <p>The larger consequence of this is that this vector \(\mathbf{w}\) is orthogonal to the all ones vector, i.e. \({\bf 1}^{\top} \mathbf{w} = 0\)</p> <hr/> <h2 id="the-final-stages">the final stages</h2> <p style="overflow-x:auto"> $$ \begin{align*} \mathbf{A}_P(\mathbf{v}) &amp;= \lambda \mathbf{v} = \mathbf{v} &amp;&amp;\text{Since 1 is an eigenvalue of $$\mathbf{A}_P$$} \\ \mathbf{A}_R(\mathbf{w}) &amp;= \mathbf{J}_{10}(\mathbf{w}) - \mathbf{I}_{10} (\mathbf{w}) - \mathbf{A}_P(\mathbf{w}) - \mathbf{A}_Q(\mathbf{w}) \\ &amp;= \mathbf{J}_{10}(\mathbf{w}) - \mathbf{w} - \mathbf{w} - \mathbf{w} \end{align*} $$ </p> <p>We now compute \(J_{10} (w)\) where \(w \in \text{Null} (A_P - I_{10})\)</p> <p style="overflow-x:auto"> $$ \mathbf{J}_{10}\mathbf{w} = \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \end{bmatrix} \mathbf{w} = \begin{bmatrix} \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \end{bmatrix}w = \begin{bmatrix} \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} $$ </p> \[\therefore \mathbf{A}_R(\mathbf{w}) = \mathbf{J}_{10}(\mathbf{w}) - \mathbf{w} - \mathbf{w} - \mathbf{w} = -3\mathbf{w}.\] <p>We have therefore arrived at a contradiction. In the beginning of the proof, we found all the eigenvalues of the Petersen Graph and concluded that -3 was not amongst that list. However as we can see, we have proved that -3 in fact is an eigenvalue for the adjacency matrix of a Petersen graph. Therefore it is not possible to cover the full graph or \(K_{10}\) with 3 Petersen graphs.</p> <hr/> <h2 id="key-takeaways">key takeaways</h2> <p>How cool is this!! All we did here was take advantage of some fairly simple linear algebra to prove a really fundamental fact about graphs that pop up quite frequently in the study of networks. For those interested, there is a conjecture in the “colorful” field of graph decomposition by the name of <a href="https://www.quantamagazine.org/mathematicians-prove-ringels-graph-theory-conjecture-20200219/">Ringel’s Conjecture</a> that actually illustrates how exactly you can cover complete graphs of this variety. If there’s anything I want you to take away from this proof, it’s that geometric intution is unparalleled and this technique of analyzing eigenvalues not only can save you computation, it gives you critical insight into what the graph actually represents, a deeper understanding of what it “does”, so to speak. It is so easy to get bogged down in what might just seem like computation after computation. But there is a bigger picture, and that bigger picture is usually a bigger graph.</p> <hr/> <h2 id="higher-dimensions-and-moore-graphs">higher dimensions and moore graphs</h2> <p><strong>degree-diameter problem:</strong> In graph theory, the degree diameter problem is the problem of finding the largest possible graph \(G\) (in terms of the size of its vertex set \(V\)) of diameter \(d\) such that the largest degree of the vertices in \(G\) is at most \(k\). The size of \(G\) is bounded above by the Moore bound; In general, the largest degree-diameter graphs are much smaller than as prescribed by the Moore bound.</p> <p><strong>the moore bound and moore graphs</strong> The moore bound gives us an upper bound on how many nodes a graph can contain with diameter \(d\) and maximum degree \(k\). An intuitive way to recognize what the moore bound actually tells us is how <em>wide</em> a graph can get. Let \(M\) be the moore bound that a graph of the above specifications can meet, we therefore have that</p> \[M = 1 + k \sum_{i = 0}^{d - 1} (k - 1)^i\] <p>Graphs that attain this bound \(M\) are known as Moore graphs.</p> <p><strong>obtaining the moore bound</strong></p> <p style="text-align:center;"> <img src="/assets/img/notes.jpg" alt="moore graphs" style="width: 100%; margin: auto;"/> </p> <p>Consider the breadth first search of a graph \(G\). From the above picture, we can clearly see that level 0 has only one node, this being the root of the tree generated by breadth first search. Level 1 has at most \(k\) nodes since the maximum degree of any node in the graph is given by \(k\). For each node \(i\) in Level 1, \(i\) can be connected to at most \(k - 1\) nodes (it’s important not to forget the node they are connected to in Level 0). Since there are \(k\) nodes in the first level, the maximum number of nodes in level 2 is given by \(k(k - 1)\).</p> <p>Generalizing, we have that the number of nodes in a level \(j\) \(\leq k(k - 1)^{j - 1}\).</p> <p>Notice that we can count the nodes from level 1 to \(d\) (we can’t have more than \(d\) levels) using a sum given by \(\sum_{i = 1}^{d} k(k - 1)^{i - 1}\). We also have to account for the root node which adds 1 to this total sum formula. After some substitution of variables, we get that the final expression for the upper bound comes out to be</p> \[M = 1 + k \sum_{i = 0}^{d - 1} (k - 1)^{i - 1}\] <p><strong>types of moore graphs and its relation to the petersen graph</strong> The entire reason we’re even talking about moore graphs is that the petersen graph is a very specific kind of moore graph. Remember that equation we found for the adjacency matrix of the petersen graph. Well for the moore graphs that are known to exist, they satisfy the same “general form” of that equation.</p> <blockquote> <p><strong>lemma 5</strong></p> <p>The only \(d\)-regular Moore graphs of girth 5 and diameter 2 exist for \(d = 2, 3, 7\) and possibly \(57\).</p> </blockquote> <p><strong>proof</strong> Assume \(G\) is a \(d\)-regular Moore graph of girth 5. A reminder that the girth of a graph is defined to be \(2k + 1\) where \(k\) is the diameter of the graph. Solving the above equation for the girth, we get that the diameter of the graph must be 2, i.e. \(k = 2\). Substituting in this value into the Moore bound, we get that the number of the vertices in the graph is given by</p> \[n = 1 + d + d(d- 1) = d^{2} + 1\] <p>As we did for the petersen graph, we consider the square of the adjacency matrix \(A^{2}\) once again. Notice that the adjacenct vertices don’t share any neighbours since if they did, there would be a triangle in \(G\). Non-adjacent vertices share exactly one neighbor, because the diameter of \(G\) is 2. Hence, \(A^{2}\) has \(d\) on the diagonal, 0 for edges and 1 for non-edges.</p> <p>In other words, we have that</p> <p style="overflow-x:auto"> $$ (A^{2} + A)_{i, j} = \begin{cases} d &amp; \forall i = j \\ 1 &amp; \forall i \neq j \end{cases} $$ </p> <p>We therefore have that \(\mathbf{A}\) satisfies the equation \(\mathbf{A}^{2} + \mathbf{A} = (d - 1)\mathbf{I} + \mathbf{J}\)</p> <blockquote> <p><strong>lemma 6</strong></p> <p>If \(\lambda\) is an eigenvalue of \(A\) different from \(d\), we get from the above equation that</p> \[\lambda^{2} + \lambda - (d - 1) = 0\] </blockquote> <p><strong>proof</strong> I suppose I should have talked about this earlier, but the all ones matrix \(\mathbf{J}_d\) has eigenvalues 0 and \(d\), where the eigenvalue \(d\) corresponds to an eigenspace with the all ones vector or \({\bf 1}\).</p> <p>Let \(x\) be the eigenvector corresponding to eigenvalue 0. Multiplying both sides of the equation we have that</p> <p style="overflow-x:auto"> $$\begin{align*} \mathbf{A}^{2}\mathbf{x} + \mathbf{A}x &amp;= (d - 1)\mathbf{x} = 0 \\ \lambda^{2}\mathbf{x} + \lambda \mathbf{x} &amp;= (d - 1)\mathbf{x} \implies \lambda^2 + \lambda - (d - 1) = 0 \end{align*} $$ </p> <p><strong>obtaining the parameters for which moore graphs exist</strong><br/> We can use the quadratic formula to ascertain that the roots of the equation \(ax^2 + bx + c = 0\) are given by \(x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\). Plugging in constants from the equation above, we have that \(\lambda = \frac{-1 \pm \sqrt{1 + 4(d - 1)}}{2} = - \frac{1}{2} \pm \frac{\sqrt{4d - 3}}{2}\).</p> <p>Assume that \(\frac{-1}{2} + \frac{\sqrt{4d - 3}}{2}\) has multiplicity \(a\) and \(\frac{-1}{2} - \frac{\sqrt{4d - 3}}{2}\) has multiplicity \(b\). Using the fact that the trace of a matrix \(A\) is the sum of its diagonal entries, we can ascertain that</p> <p style="overflow-x:auto"> $$\begin{align*} d(1) + \left(\frac{-1}{2} + \frac{\sqrt{4d - 3}}{2}\right)a + \left(\frac{-1}{2} - \frac{\sqrt{4d - 3}}{2}\right)b &amp;= 0 \implies d - \frac{a + b}{2} + \frac{1}{2} \; (a - b) \sqrt{4d - 3} = 0 \end{align*}$$ </p> <p>However we know that \(a + b = n - 1\) and from our previous computation of \(n\), we get that \(a + b = d^{2}\). Substituting in, we get that</p> <p style="overflow-x:auto"> $$\begin{align*} d - \frac{d^2}{2} + \frac{1}{2} \; (a - b) \sqrt{4d - 3} &amp;= 0 \implies (a - b) \sqrt{4d - 3} = d^{2} - 2d \end{align*}$$ </p> <p>This statement is only true if \(a = b\) and \(d = 2\) (the trivial case of both sides being 0) or else \(4d - 3\) is a square.</p> <p>Let \(4d - 3 = s^{2}\). Therefore, we have that</p> <p style="overflow-x:auto"> $$\begin{align*} d - \frac{d^2}{2} + \frac{s}{2} \; (a - b) &amp;= 0 \implies d = \frac{s^{2} + 3}{4} \\ \frac{1}{4} \; (s^{2} + 3) - \frac{1}{2} \; (s^2 + 3)^{2} + \frac{s}{2} \; (2a - \frac{1}{16} \; (s^2 + 3)^{2}) &amp;= 0 &amp;&amp;\text{Substituting in $$d$$} \\ s^{5} + s^{4} + 6s^{3} - 2s^2 + (9 - 32a)s - 15 &amp;= 0 &amp;&amp;\text{After an ungodly amount of math :(} \end{align*} $$ </p> <p>To satisfy the above equation, we have that \(s\) must divide 15, i.e. \(s \in \{1, 3, 5, 15 \}\). Since \(s^2 = 4d - 3\), we have that \(d \in \{1, 3, 7, 57 \}\).</p> <p>When \(d = 1\), we get the complete graph on 2 vertices, i.e. \(K_2\) which is not a Moore graph since it doesn’t meet the Moore bound.</p> <p>For all the other cases, we have that</p> <ul> <li>\(d = 2\): \(C_5\)</li> <li>\(d = 3\): Petersen Graph</li> <li>\(d = 7\): Hoffman Singleton Graph</li> <li>\(d = 57\): Open problem if this graph exists :0</li> </ul> <p>There’s a lemma floating around the internet somewhere where it states that Moore graphs don’t exist for diameters greater than 2 which is kind of insane if you think about it. It’s also the reason their existence is so fascinating and why we’ve listed most of if not all the Moore graphs there can be.</p>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[the petersen graph is in almost every sense of the word; a universal counter-example]]></summary></entry><entry><title type="html">finding balance</title><link href="https://lukshyaganjoo.github.io/blog/2024/balance/" rel="alternate" type="text/html" title="finding balance"/><published>2024-02-15T00:00:00+00:00</published><updated>2024-02-15T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/balance</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/balance/"><![CDATA[<p>i find myself in somewhat of a strange position today. awake at around 3 am, with this inexplicable urge to pen down my state of mind right now. under normal circumstances, the feelings that need to be stirring in the pot that is my frazzled self have to be ones borne of guilt, angst, sadness or a linear combination of the above. not today though, today is different. quite recently, my friend informed me of this delightful pair of words that i think should be used in daily parlance, namely “obscure sorrow”, this perchance quaint happiness that words can’t quite explain? yeah, today is one of those days.</p> <p>so what’s changed? well my brother recently graduated with a masters in telecommunications with his commencement happening on the 20th of last month. the ceremonial aspect of graduation has always been bittersweet, but given that i’ve never really experienced it in full effect (at least not in a conventional sense) the emotional weight of seeing your 25 year old brother graduating was new and unfamiliar. sitting amongst that crowd of people with my family, screaming his name and clapping like one of those marching band toys you’d get for your kid, yeah needless to say i was excited. the celebratory clapping receded, the chatter of the guest speakers playing the role of background noise, and things finally started moving into perspective for me.</p> <p>all my life, i’ve been waiting for the chance to grow up. when i was in middle school, i would oftentimes look at the things my elder siblings would do and yearn for the chance to perhaps one day become more mature, more responsible and perhaps have a more interesting life. it all seemed so far away, an abstraction of a life far different from mine. looking back, the novelty might have been the main appeal as opposed to some intrinsic unhappiness with my childhood and/or youth. perhaps for a lack of trying, i hadn’t seen it all this while but hearing your siblings name called out on stage to be awarded a diploma thrust me into the realization that i’m finally here. i’m in this position that i’ve waited for all my life, i’m in the institution of university doing the majors i’ve wanted to all my life and it’s only now starting to settle in.</p> <p>more to follow (stay tuned :))</p>]]></content><author><name></name></author><category term="essays"/><summary type="html"><![CDATA[a departure from the grief that typically gives birth to writing]]></summary></entry><entry><title type="html">answers</title><link href="https://lukshyaganjoo.github.io/blog/2024/answers/" rel="alternate" type="text/html" title="answers"/><published>2024-01-27T00:20:00+00:00</published><updated>2024-01-27T00:20:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/answers</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/answers/"><![CDATA[<p>Human beings are weird. We’re filled with this strange sense of inadequacy when placed in a situation of laying ourselves bare; to people, we know very little about. There’s a prevalent thought process among us, a hindering fear of perfection in others that often times stops us from being truly ourselves with people. It’s unfortunate really considering that everyone has something they’re hiding from these seemingly perfect creatures. But you have to admit it is one of the greatest ironies that the very people who just might be able to understand the true nature of the self, are often the ones we lie most afraid of.</p> <p>Perhaps it’s just a nihilistic approach of life to be grateful of your inadequacies, but I’m comforted by the fact that there’s a lot of things I’m abysmal at, it does accentuate the perpetuity of the hamster wheel model of endlessly running and drives people mad with the point of their existence</p> <p>While my interpretation of life as a whole and what it offers is borne out of looking at the world through an inexperienced and naive lens, to be forever the learner and experience some of the most surreal things this world has to offer has been what drives me.</p> <p>Its perhaps a key driving force behind my subconscious pick of friends miles ahead of me in intellect, and sometimes the inadequacy sucks, it leads you to question why you’re here or what’s even the point of doing whatever you are, for there are several better than you.</p> <p>But just as true is perhaps the childlike joy of seeing things to do, things to look forward to, a perpetual and life-long Indiana Jones adventure.</p> <p>Maybe our physical appearance and mental appearance fails to please us, maybe we’re unhappy with whatever we’ve achieved but I can’t help but wonder if it’s a subconscious driving force behind wanting to better ourself.</p> <p>I don’t know about you but I can’t find a better motivator than wanting to be the best you can for the people you love; superficial or not, it shows you care.</p> <p>That’s something huge, and although it might sound trivial its one of the most important things to learn.</p> <p>We might not be the smartest, maybe not even the greatest at physical speciality, or several other things, but we’re not bereft of the human emotion of caring.</p> <p>Maybe that’s all we really need.</p> <p>~Lukshya</p>]]></content><author><name></name></author><category term="essays"/><summary type="html"><![CDATA[a half-baked attempt at philosophy]]></summary></entry><entry><title type="html">fever dream</title><link href="https://lukshyaganjoo.github.io/blog/2024/fever-dream/" rel="alternate" type="text/html" title="fever dream"/><published>2024-01-26T17:57:00+00:00</published><updated>2024-01-26T17:57:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/fever-dream</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/fever-dream/"><![CDATA[<p>Dealing in delirium is toying with death<br/> Desperation for absolution of sin is the door to damnation<br/> The dread of the desert had found its next victim<br/> A poetic man, a dullard dominated by delusions of degeneracy</p> <p>Cumbersome circumstances leave one with the illusion of free will<br/> No stranger to the cruel command of creation, he raised his hands<br/> Carefully cupped by his chest, directed his bones towards the vast expanse<br/> He knew the labyrinth-like ways of the desert, the abominable alleys<br/> How else could he have escaped from the mindless tittering of insanity?</p> <p>He walked past the jagged crevices; the forlorn paths Marched to the beat, a soldier’s loyalty<br/> Incisions of human frailty, if only the privilege of the aftermath<br/> Steps in a precise cadence, a forced fealty</p> <p>He made his way, arriving at an isolated cave for a moment’s respite<br/> For he, not a moment had passed; he was summoned yet again<br/> Accepted unreasonable responsibilities dutifully, not a moment’s hesitation<br/> After all, how could he who forgot have known any better</p> <p>He looked upon his infinite world, the consequence of his actions<br/> Filled with horror, yet simultaneously the greatest delight<br/> This? This was his Frankenstein’s monster<br/> His greatest creation, his final undoing.</p>]]></content><author><name></name></author><category term="poetry"/><summary type="html"><![CDATA[i had a lot of fun with this one]]></summary></entry><entry><title type="html">archaic lanterns</title><link href="https://lukshyaganjoo.github.io/blog/2024/lanterns/" rel="alternate" type="text/html" title="archaic lanterns"/><published>2024-01-26T16:03:00+00:00</published><updated>2024-01-26T16:03:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/lanterns</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/lanterns/"><![CDATA[<p>Blazes of glory at midnight<br/> Flicker ever so slightly in misfit murkiness<br/> A gilded cage, his own creation<br/> The deleterious flames; a gratifying nectar</p> <p>The traveller gripped his lantern by his breast<br/> Sputters of blood dripped from his chafed lips<br/> Respiratory constitution, all for momentary warmth<br/> Mystifying effects, that of which he was aware</p> <p>He stopped, a moment of rest<br/> Besides him, the lantern grew a bloodshot orange<br/> He stole a glance at the shattered glass; a horrible transfiguration<br/> The serenity of rationalism and courage<br/> Overpowered urges, forged in heresy</p> <p>Lanterns were known for their lead traced fumes<br/> A cautionary tale; appreciable distance, quite the necessity</p> <p>A quick dust off the cloak<br/> He tossed aside the wavering blemish<br/> The cold took over his limbs; rendered his journey impossible<br/> He much preferred the continual frost, than cruel moments of comfort<br/> An iron-willed being, he found the lock<br/> Weary, fatigued; yet finally free</p> <p>Untethered strings, now free of knots<br/> To be or not to be healed, can they ever?</p> <p>~Lukshya</p>]]></content><author><name></name></author><category term="poetry"/><summary type="html"><![CDATA[an outsiders perspective on addiction]]></summary></entry><entry><title type="html">you’re not the main character in everyone’s story</title><link href="https://lukshyaganjoo.github.io/blog/2024/main-character/" rel="alternate" type="text/html" title="you’re not the main character in everyone’s story"/><published>2024-01-26T01:04:00+00:00</published><updated>2024-01-26T01:04:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2024/main-character</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2024/main-character/"><![CDATA[<p>Now before anyone gets mad, I’m not saying that you’re not important, or that you don’t matter. In fact, what I’m saying is the complete opposite. I suppose picking up on subtlety is often times hard when one is filled with illusions of grandeur. In fact, this is something that is quite personal for me. In some strange way, despite my social anxiety and my desire for conscientuousness, I’ve often time craved the attention of those I’m close with in more ways than one.</p> <p>Everyone desires love, in fact I’d go so far as to say, that there is no other reason we continue to dredge on with what sometimes feels like a “meaningless existence”. That might be a loaded statement, but I think there’s at least a modicum of truth to my words here. It’s so incredibly easy to get attach your idea of self worth to the opinions of others, and to some extent, I’d argue that that’s a good thing! It drives us to be better, to do better, to be more than what we are right now, to be more than what we were yesterday. Like all things in life however, there is a fine line between being driven by the opinions of others and letting the opinions of others drive you.</p> <p>Balance is hard! I’m not going to sit here and pretend like I have it all figured out. Despite a newfound self awareness on my part, I still grapple with the same issues I’ve had for the past few years. The same insecurities, the same doubts, the same fears. Most nights, I ask myself if I’m doing enough for the people I care about, if I’m doing enough for myself, if I’m proud of the person I’ve become. Facing up to the kind of person you really are, flaws and all is a difficult thing to do. It’s easy to hide behind a facade, the character you’ve created for yourself, the person you wish you were. The allure of the perfect avatar is powerful, and while self deception releases dopamine like nothing else, it’s short-lived, and the crash that follows is often times worse than the high. You won’t feel shitty about yourself in the conventional sense, but you’ll lose a part of your identity, a much more insidious fate if you ask me.</p> <p>So, what does this off-tangent tirade have to do with the title of this post? Well, I think the reason we’re as concerned as we are with the opinion of others is because we’re trying to play a central part in their lives. I don’t know if I’m speaking for everybody here, but this has at least been somewhat true for me. I’d like to think that I’m naturally an empathetic person, but sometimes I’ve found that my desire for helping others out trumps what they’re asking for in the first place. It’s not your job to fix everyone’s problems, and while this is not to say you shouldn’t help out when you can, it’s to say that the most important things in these types of situations is to listen, and give people the space to figure things out for themselves.</p> <p>So say it with me folks; you’re not the main character in everyone’s story. In fact it’s almost unereasonable to think that you are. The world is a big place, and there are a lot of people in it. Behind every single one of them is a story, a story that is just as important as yours, a culture just as rich. There is something so incredibly powerful about letting other people take the reigns of their own lives. There is something masculine, something feminine, and if you happen to identify with gender identities that are outside the conventional dichotomy, there is something humane about being an observer to the lives of others. Everyone wants to help the world. Start instead with helping a single person in it. I promise you, that the rest will follow and while you may not be the protagonist in everyone’s story, you are the narrator of your own. I don’t know about you, but I think that’s something worth celebrating.</p> <p>~Lukshya</p>]]></content><author><name></name></author><category term="essays"/><summary type="html"><![CDATA[liberating yourself from the shackles of self importance]]></summary></entry><entry><title type="html">my take on much of modern philosophy</title><link href="https://lukshyaganjoo.github.io/blog/2023/phil/" rel="alternate" type="text/html" title="my take on much of modern philosophy"/><published>2023-12-12T22:25:00+00:00</published><updated>2023-12-12T22:25:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2023/phil</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2023/phil/"><![CDATA[<p>One of the things that has constantly befuddled me, is how the decision-making process works in our brain. How exactly is the ability to discern between right and wrong developed? How does our brain know what’s right and what’s wrong? And if I’m right we’re probably just slaves to an evolutionary trait.</p> <p>Don your monocle, suit yourself up with the most expensive Prada you can find and arm yourself with a smug expression that would make your ancestors proud. We’re going to pretentious land, and this time we’re going in deep.</p> <p>Free will is perhaps one of the most complicated areas of research in modern psychology, and I am but an uneducated swine. I do not have a degree in psychology nor do I have any qualifications in philosophy, so a fair warning: take my words with a grain of salt.</p> <p>Charles Darwin was an English naturalist, geologist and biologist, and in addition to having some of the most kick-ass verses in ERB’s matchup against Ash Ketchum, he was responsible for propounding the “Theory of Natural Selection”, which explains how genetic traits change over time relative to habitat and environment.</p> <p>Some of you may know where I’m going with this, in essence, if the development of our brain is a genetic trait, then well the argument ends there: we’re at the mercy of the environment, both physically and mentally.</p> <p>But it’s interesting to note, that people now aren’t any more intelligent than they were before, the standard model of the distribution of people concerning their Intelligence Quotient has remained the same with slight changes here and there. It was a bell curve and continues to remain so. While IQ is an imperfect metric for measuring intelligence, it is the only one we possess and serves a purpose for some form of evidence to the point I’m trying to make.</p> <p>People aren’t any smarter than they were before, but the psyche behind making decisions has definitely evolved concerning changing time and circumstances. So do we chart the former to natural selection and the other to be a clandestine constant? Well, we could, but it’s fascinating that these two deeply intertwined processes have completely different origins and paths followed.</p> <p>Intelligence is a whole another issue, one I haven’t thought enough about, and I am in no position to speak about. Perhaps, however, the psyche behind discerning right and wrong, the question I raised at the beginning of the passage, is more interesting to think about.</p> <p>If what I’ve said is true and the ability to distinguish between what is black and what is white is a genetic trait, then that’s a whole new curveball to our existence. Everything we’ve ever done, all the opinions we’ve forged, the discussions we’ve had, everything is just a result of adaption.</p> <p>Survival and sustenance, perhaps the only reasons why people have different opinions. Our morality is forged from circumstance, and our circumstance is forged primarily from geographical barriers and socio-political constraints borne as a result of that primary binding. The reason why everyone has a different perspective on life, or different opinions on issues affecting us, is all a result of their fragmented understanding of how best to survive: establishing a connection with humans. We were a social animal, we will continue to remain so.</p> <p>Strength has an odd role to play in this, but it ties into the whole scope of how some people are non-confrontational or some people are malleable in their opinion, while some are as stubborn as the mule that assisted Shrek on his journey. It’s all essentially a by-product of establishing a connection with other human beings, who ironically are doing the very same.</p> <p>Perhaps that’s enough for today. If we do indeed have no free will, I’m not really worried. The same force has led to us making memes and sub-1-minute youtube clips from 2010 which are in every sense the most hysterical things I’ve ever seen.</p> <p>For now, farewell and if ever in doubt, there’s always the lad on YouTube who thought he was a bush.</p> <p>~Lukshya</p>]]></content><author><name></name></author><category term="essays"/><summary type="html"><![CDATA[a pretentious look at free will where i say a lot of nothing]]></summary></entry><entry><title type="html">the probabilistic method is pretty funny</title><link href="https://lukshyaganjoo.github.io/blog/2023/streaming/" rel="alternate" type="text/html" title="the probabilistic method is pretty funny"/><published>2023-10-23T00:00:00+00:00</published><updated>2023-10-23T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2023/streaming</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2023/streaming/"><![CDATA[<h2 id="introduction">introduction</h2> <p>the object of consideration for today might be slightly different than what you’re used to. the computational model we will be using for this proof is the streaming model. in this model, we are given a sequence of elements in a “stream” and we have to process them one by one. given that the number of elements in our data stream could be significantly larger than the amount of memory we have, we aren’t allowed to store the entire stream in memory, and are instead allowed a small amount of space to store the information we deem relevant. the question now becomes; what functions of the input stream can we compute with what amount of time and model? while we will be focusing on space; similar considerations and conclusions can be made for time. under normal circumstances, the study of such a model would be motivated by introducing the notion of <strong>moments</strong> where you consider the stream as a high-dimensional vector and may want to compute the \(k\)-th moment of the vector. while this is useful, the motivating route we take is slightly different.</p> <h2 id="the-problem">the problem</h2> <blockquote> <p>consider the problem of finding the number of distinct elements in a stream of elements. while this is a trivial problem in the standard computational model since we can simply store the elements and count the number of distinct elements, the streaming model makes this problem significantly more challenging.</p> </blockquote> <blockquote> <p><strong>theorem</strong></p> <p>we present a streaming algorithm to estimate the number of distinct elements in the sequence with multiplicative error \(1 \pm \epsilon\) for some \(\epsilon \in (0, 1)\). for the algorithm, we assume access to \(k\) independent hash functions as described above. the number of hash functions required depends on the desired precision, i.e. \(k \leq \mathcal{O}(1/\epsilon^2)\) is sufficient to achieve the desired error guarantee.</p> </blockquote> <blockquote> <p><strong>definition</strong></p> <p>a <strong>hash function</strong> is a function \(h : A \to B\) where \(A = \{a \in \{0, 1\}^j \mid j \in \mathbb{N}\}\) is the set of all bit sequences of arbitrary length and \(B = \{0, 1\}^k\) is the set of all bit sequences of a specific length \(k \in \mathbb{N}\). Note that it is not important to understand deeply the definition of a hash function, rather simply that with the appropriate independence assumptions, it essentially behaves like a uniform random variable.</p> </blockquote> <h2 id="a-surprisingly-simple-solution">a surprisingly simple solution</h2> <p style="overflow-x:auto"> $$ \begin{array}{l} \textbf{function} \; \texttt{estimateDistinct}(\text{stream}, k): \\ \quad \text{initialize } k \text{ independent hash functions } h_1, h_2, \dots, h_k : \{0, \dots, n - 1\} \to [0, 1] \\ \quad \texttt{val}_1 \leftarrow \infty, \texttt{ val}_2 \leftarrow \infty, \dots, \texttt{ val}_k \leftarrow \infty \\ \quad \text{for } i = 1 \text{ to } n: \\ \quad \quad x \leftarrow \texttt{stream}[i] \\ \quad \quad \text{for } j = 1 \text{ to } k: \\ \quad \quad \quad \texttt{val}_j \leftarrow \min\{\texttt{val}_j, h_j(x)\} \\ \quad Z \leftarrow \frac{1}{k} \sum \limits_{i = 1}^k \texttt{val}_i \\ \quad \text{return } \bigg\lfloor \frac{1}{Z} - 1 \bigg\rfloor \\ \end{array} $$ </p> <p>what the above algorithm is essentially doing is hashing each element in the stream \(k\) times and keeping track of the minimum hash value for each element. it is then taking the average of these minimum hash values and returning the appropriate estimate on the last line of the algorithm.</p> <h2 id="why-is-an-average-of-minimums-necessary">why is an average of minimums necessary?</h2> <p>in order to answer this question, we consider the following modification to the algorithm. \(\begin{array}{l} \textbf{function } \texttt{incorrectEstimateDistinct(stream, k)} \\ \quad \text{initialize a hash function } h : \{0, \dots, n - 1\} \to [0, 1] \\ \quad \texttt{val} \gets \infty \\ \quad \textbf{for } i = 1 \textbf{ to } n: \\ \quad\quad x \gets \texttt{stream}[i] \\ \quad\quad \texttt{val}_i \gets \min(\texttt{val}_i, h(x)) \\ \quad Z \gets \min_{i \in [k]} \texttt{val}_i \\ \quad \textbf{return } \left\lfloor \frac{1}{Z} - 1 \right\rfloor \end{array}\)</p> <p>while the above algorithm is not sufficient for our purposes, it does produces the correct solution on average! (in fact we use this very fact in the proof of the main theorem).</p> <blockquote> <p><strong>lemma 1</strong></p> <p style="overflow-x:auto"> $$\begin{align*} \text{number of distinct elements in the data stream} = \frac{1}{\mathbb{E}[Z]} - 1\end{align*} $$ </p> </blockquote> <p><strong>proof</strong> Indeed from Theorem 2, we have that \(\mathbb{E}[Z] = \frac{1}{k + 1}\) and a matter of simple algebra yields that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Z] &amp;= \frac{1}{k + 1} \implies \frac{1}{\mathbb{E}[Z]} = k + 1 \\ \end{align*} $$ thereby ensuring that we return the correct estimate on average. </p> <p>So what goes wrong in the above algorithm. The issue is that the variance of the estimator is too high. In fact, defining \(Y = \min (h(x_1), h(x_2), \dots, h(x_n))\) as above in the algorithm; following similar steps to the variance computation in Theorem 2 yields</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(Y) &amp;= \frac{k}{(k + 1)^2 (k + 2)} \approx \frac{1}{(k + 1)^2} \end{align*} $$ </p> <p>and therefore by Chebyshev’s inequality, the probability that the estimate is off by more than a factor of \(1 \pm \epsilon\) is \(\mathcal{O}(1/\epsilon^2)\).</p> <h2 id="the-proof">the proof</h2> <p>firstly we verify that this modification does not alter the correctness of the estimation on average. this is easy because</p> <p style="overflow-x:auto"> $$ \begin{align*} Z = \frac{1}{k} \sum_{i = 1}^k Y_i \quad \text{ where } Y_i = \min\{h_i(x_1), h_i(x_2), \dots, h_i(x_m)\} \end{align*} $$ </p> <p>and therefore</p> <p style="overflow-x:auto"> $$ \mathbb{E}[Z] = \mathbb{E}\left[\frac{1}{k} \sum_{i = 1}^k Y_i\right] = \frac{1}{k} \sum_{i = 1}^k \mathbb{E}[Y_i] = \frac{1}{k} \sum_{i = 1}^k \frac{1}{m + 1} = \frac{1}{k} \cdot \frac{k}{m + 1} = \frac{1}{m + 1} $$ </p> <p>Turning our attention to the problem child of it all, we now compute the variance of $Z$. We have that $$</p> <p style="overflow-x:auto"> \begin{align*} \text{Var}(Z) &amp;= \text{Var}\left(\frac{1}{k} \sum_{i = 1}^k Y_i\right) = \frac{1}{k^2} \text{Var}\left(\sum_{i = 1}^k Y_i\right) = \frac{1}{k^2} \sum_{i = 1}^k \text{Var}(Y_i) \\ &amp;\leq \frac{1}{k^2} \sum_{i = 1}^k \frac{1}{(m + 1)^2} = \frac{1}{k^2} \cdot \frac{k}{(m + 1)^2} = \frac{1}{k(m + 1)^2} &amp;&amp;\text{by theorem 2} \end{align*} $$ </p> <h2 id="appendix">appendix</h2> <blockquote> <p><strong>theorem 1</strong></p> <p>Let \(X_1, X_2, \dots, X_n\) be independent random variables uniformly distributed in \([0, 1]\) and let \(X := \min\{X_1, X_2, \dots, X_n\}\). Then</p> <p style="overflow-x:auto"> $$ f_X(x) = \begin{cases}0 &amp; \text{if } x \notin [0, 1] \\ n (1 - x)^{n - 1} &amp; \text{if } x \in [0, 1]\end{cases} $$ </p> </blockquote> <p><strong>proof</strong> We proceed as usual; by first computing the CDF, i.e. \(F_X(x) = \Pr[X \leq x]\). We have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[X \leq x] &amp;= 1 - \Pr[X \geq x] = 1 - \Pr[\min\{X_1, X_2, \dots, X_n\} \geq x] \\ &amp;= 1 - \Pr[X_1 \geq x, X_2 \geq x, \dots, X_n \geq x] = 1 - \prod_{i = 1}^n \Pr[X_i \geq x] \\ &amp;= 1 - \prod_{i = 1}^n (1 - x) = 1 - (1 - x)^n \end{align*} $$ </p> <p>Therefore we have that</p> \[F_X(x) = \begin{cases} 0 &amp; \text{if } x &lt; 0 \\ 1 - (1 - x)^n &amp; \text{if } x \in [0, 1] \\ 1 &amp; \text{if } x &gt; 1 \end{cases}\] <p>The result thus follows by taking the derivative of \(F_X(x)\).</p> <blockquote> <p><strong>theorem 2</strong></p> <p>let \(X_1, X_2, \dots, X_n\) be independent random variables uniformly distributed in \([0, 1]\). let \(Y = \min \limits_{i \in [n]} X_i\). then \(\mathbb{E}[Y] = \frac{1}{n + 1}\) and \(\text{Var}(Y) \leq \frac{1}{(n + 1)^2}\).</p> </blockquote> <p><strong>proof</strong> we first compute the expectation, by definition</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y] &amp;= \int_0^1 x n (1 - x)^{n - 1} dx = n \int_0^1 x (1 - x)^{n - 1} dx = \frac{1}{n + 1} &amp;&amp; \text{by theorem 1} \end{align*} $$ </p> <p>We now turn our attention to computing the variance. We need one more result before we can proceed. We compute \(\mathbb{E}[Y^2]\) and via a similar application of theorem 1 we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y^2] &amp;= \int_0^1 x^2 n (1 - x)^{n - 1} dx &amp;&amp; \text{by theorem 1} \\ &amp;= n \int_0^1 x^2 (1 - x)^{n - 1} dx = \frac{2}{(n + 1)(n + 2)} \end{align*} $$ </p> <p>We now finally compute the variance, which is given by</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(Y) &amp;= \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 = \frac{2}{(n + 1)(n + 2)} - \frac{1}{(n + 1)^2} \\ &amp;\leq \frac{2}{(n + 1)^2} - \frac{1}{(n + 1)^2} = \frac{1}{(n + 1)^2} \end{align*} $$ </p> <h2 id="references">references</h2> <ul> <li><a href="https://courses.cs.washington.edu/courses/cse521/23au/521-lecture-6.pdf">uw’s exposition on sketching and hashing algorithms</a></li> <li><a href="https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15850-f20/www/notes/lec11.pdf">some lovely notes on variance reduction and streaming</a></li> <li><a href="https://courses.cs.washington.edu/courses/cse312/20su/files/student_drive/section4_distinct_elements_notes.pdf">count min-hash by alex tsun @ stanford</a></li> </ul>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[reframing conventionally simple computations with limited models of computation]]></summary></entry></feed>