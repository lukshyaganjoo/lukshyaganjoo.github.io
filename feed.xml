<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lukshyaganjoo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lukshyaganjoo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-19T07:12:41+00:00</updated><id>https://lukshyaganjoo.github.io/feed.xml</id><title type="html">lukshya ganjoo</title><subtitle>the personal website of lukshya ganjoo </subtitle><entry><title type="html">unstructured search, but smarter</title><link href="https://lukshyaganjoo.github.io/blog/2025/grover/" rel="alternate" type="text/html" title="unstructured search, but smarter"/><published>2025-07-13T00:00:00+00:00</published><updated>2025-07-13T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2025/grover</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2025/grover/"><![CDATA[<h2 id="introduction">introduction</h2> <p>It seems there’s been a paradigm shift in the way popular media talks about quantum computing. Initially pessimistic, even revolutionary tech figures like <a href="https://en.wikipedia.org/wiki/Jensen_Huang">Jensen Huang</a> have turned heel on what they now believe to be the viability of large-scale quantum computers in the near future. Yet today there are several compelling reasons to take quantum seriously; the “elevator pitch” is not exclusively an appeal to authority. I won’t dive into those reasons here, but if you’re curious why I think this is such an exciting area of computer science, feel free to email me and I’d be happy to explain in detail.</p> <p>For those already interested in quantum, demonstrating quantum advantage remains paramount. After all, quantum computing was originally sold as a model that could deliver dramatically faster speed‑ups across a wide class of problems. In some cases, these speed‑ups are exponential: for example, John Watrous’s excellent video on quantum‑query algorithms in IBM’s “Fundamentals of Quantum Algorithms” series. However, many of these algorithms feel somewhat artificially constructed to highlight extreme cases. Today, we’ll discuss an algorithm whose speed‑up is more modest in scale but applies to a much broader class of problems.</p> <h2 id="the-problem-itself">the problem itself</h2> <blockquote> <p>Given black-box access to a function \(f : \{0, 1\}^n \to \{0, 1\}\), output a <strong>marked input</strong> \(\mathbf {x}^*\) such that \(f(\mathbf{x}^*) = 1\), if one exists.</p> </blockquote> <p>here, black-box access means that we can only interact with the function \(f\) via querying \(f\) on bit-strings in \(\{0, 1\}^n\). we do not have access to any algorithm or circuit that can compute said function. Assume that \(N := 2^n\).</p> <h2 id="a-classical-lower-bound">a classical lower bound</h2> <p>the best known classical algorithm for this setting of problem is unfortunately no better than brute-force. given that there are \(N\) \(n\)-bit binary strings, this provides a run-time of \(\Omega(N)\).</p> <h2 id="querying-a-function-quantumly">querying a function quantumly</h2> <p>operations in the quantum world are mostly<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> governed by <a href="https://en.wikipedia.org/wiki/Unitary_transformation">unitary transformations</a> and <a href="https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics">measurements</a>. querying a function $f$ quantumly is specified as follows</p> <p style="overflow-x:auto"> $$ \begin{align*} \ket{x} \ket{y} &amp;\substack{\mathbf{U}_f \\ \mapsto} \ket{x} \ket{y \oplus f(x)} \end{align*} $$ </p> <p>where \(\mathbf{U}_f\) is the corresponding unitary that implements the function \(f\) and \(x, y \in \{0, 1\}^n\).</p> <h2 id="the-phase-oracle">the phase oracle</h2> <p>given a function \(f\), it turns out assuming oracle access to \(\mathbf{U}_f\) as prescribed above is equivalent to <strong>querying the function in phase</strong>. what i mean by this is having access to a unitary \(\mathbf{O}_f\) such that</p> <p style="overflow-x:auto"> $$\begin{align*} \ket{x} &amp;\substack{\mathbf{O}_f \\ \mapsto} (-1)^{f(x)} \ket{x} \end{align*}$$ </p> <p><strong>proof</strong> this turns out to use a trick that shows up quite often in the field; namely querying the unitary in superposition. we have that</p> <p style="overflow-x:auto"> $$\begin{align*} \mathbf{U}_f \ket{x} \ket{-} &amp;= \mathbf{U}_f \left[\frac{1}{\sqrt{2}}\left(\ket{0} - \ket{1}\right)\right] \\ &amp;= \frac{1}{\sqrt{2}} \mathbf{U}_f \ket{x}\ket{0} - \frac{1}{\sqrt{2}} \mathbf{U}_f \ket{x} \ket{1} \\ &amp;= \frac{1}{\sqrt{2}} \ket{x}\ket{f(x)} - \frac{1}{\sqrt{2}} \ket{x} \ket{1 \oplus f(x)} \\ &amp;= \ket{x} \otimes \left(\frac{1}{\sqrt{2}} \ket{f(x)} - \frac{1}{\sqrt{2}} \ket{1 \oplus f(x)}\right) \\ &amp;= (-1)^{f(x)} \ket{x} \ket{-} \end{align*}$$ </p> <p>where we made use of the definition and linearity of \(\mathbf{U}_f\), thereby settling the claim.</p> <h2 id="grovers-insight">grover’s insight</h2> <p><a href="https://en.wikipedia.org/wiki/Lov_Grover">grover</a> proposed the following circuit to solve the problem</p> <p style="text-align:center;"> <img src="/assets/img/grover.jpg" alt="grover's circuit" style="width: 100%; margin: auto;"/> </p> <p>where \(\mathbf{R}\) is the \(n\)-qubit unitary acting as</p> <p style="overflow-x:auto"> $$ \begin{align*} \ket{+}^{\otimes n} &amp;\mapsto \ket{+}^{\otimes n} \\ \ket{\phi} &amp;\mapsto - \ket{\phi} &amp;&amp;\text{ for all } \phi \perp \ket{+}^{\otimes n} \end{align*} $$ </p> <p>which describes a reflection about \(\ket{+}^{\otimes n}\) or a uniform superposition over all \(n\)-bit strings. for those of us who like thinking of transformations as matrices; \(\mathbf{R}\) can be explicitly written as the matrix \(2 \ket{+}\bra{+}^{\otimes n} - \mathbb{I}\).</p> <h2 id="analysis">analysis</h2> <p>The time has now come to analyze the circuit for Grover’s algorithm. Let us assume for simplicity, that there is a single marked input \(\mathbf{x^*} \in \{0, 1\}^n\). We start with \(\ket{0}^{n}\) and prepare a uniform superposition by applying \(\mathbf{H}^{\otimes n}\);</p> <p style="overflow-x:auto"> $$ \begin{align*} \ket{0}^n &amp;\substack{\mathbf{H}^{\otimes n} \\ \mapsto} \frac{1}{\sqrt{N}} \sum_{x \in \{0, 1\}^n} := \ket{\psi} \end{align*} $$ </p> <p>For the sake of notational convenience, we will write the state \(\ket{\psi}\) in terms of the marked input and the rest of the \(n\)-bit strings. This provides</p> <p style="overflow-x:auto"> $$ \begin{align*} \ket{\psi} &amp;= \frac{1}{\sqrt{N}} \ket{x^*} + \frac{1}{\sqrt{N}} \sum_{x \neq x^*} \ket{x} = \frac{1}{\sqrt{N}} \ket{G} + \sqrt{\frac{N - 1}{N}} \ket{B} \end{align*} $$ </p> <p>where \(\ket{G} := \ket{x^*}\) is the “good part” of the superposition and \(\ket{B} := \frac{1}{\sqrt{N - 1}} \sum \limits_{x \neq x^*} \ket{x}\) is the “bad part” of the superposition; the part of the input that we don’t care about. Note that the normalization here is just so that \(\ket{G}\) and \(\ket{B}\) are valid quantum states.</p> <p>Now given that this is a quantum algorithm, we have to make some kind of measurement towards the end. Were we to measure now, we would get the marked input \(\mathbf{x}^*\) with very low probability, precisely \(\frac{1}{N}\). At a high-level, our hope is to somehow “amplify” the amplitude of this coefficient so as to ensure we get the marked input string with very high probability when we measure. This is exactly what the <strong>Grover iterates</strong> or the repeated application of \(\mathbf{O_f} \mathbf{R}\) achieves for us.</p> <p>Before we dive into what the Grover iterate does for us, let us first massage our state into a more convenient expression. We write</p> <p style="overflow-x:auto"> $$\begin{align*} \ket{\psi} &amp;= \frac{1}{\sqrt{2^n}} \ket{G} + \sqrt{\frac{2^n - 1}{2^n}} \ket{B} = \sin(\theta) \ket{G} + \cos(\theta) \ket{B} &amp;&amp;\text{ where } \sin(\theta) := \frac{1}{\sqrt{2^n}} \end{align*}$$ </p> <h3 id="the-effect-of-a-single-grover-iterate">the effect of a single grover iterate</h3> <p>We first analyze the action of \(\mathbf{R}\) on the states \(\ket{G}, \ket{B}\).</p> <p style="overflow-x:auto"> $$\begin{align*} \mathbf{R} \ket{G} &amp;= (2 \ket{+}\bra{+}^n - \mathbb{I}) \ket{x^*} \\ &amp;= 2 \braket{+^n, x^*} \ket{+^n} - \ket{x^*} = \frac{2}{\sqrt{N}} \ket{+}^n - \ket{x^*} \\ &amp;= \frac{2}{N} \sum_{x \in \{0, 1\}^n} \ket{x} - \ket{x^*} \\ &amp;= \left(\frac{2}{N} - 1\right) \ket{G} + \frac{2 \sqrt{N - 1}}{N} \ket{B} \\ \mathbf{R} \ket{B} &amp;= (2 \ket{+}\bra{+}^n - \mathbb{I}) \ket{B} \\ &amp;= 2 \braket{+^n, B} \ket{+}^n - \ket{B} = 2 \sqrt{\frac{N - 1}{N}} \ket{+}^n - \ket{B} \\ &amp;= \frac{2 \sqrt{N - 1}}{N} \left(\sum_{x \neq x^*} \ket{x} + \ket{x^*}\right) - \ket{B} \\ &amp;= \frac{2 \sqrt{N - 1}}{N} \ket{G} + \left[\frac{2(N - 1)}{N} - 1\right] \ket{B} \\ &amp;= \frac{2 \sqrt{N - 1}}{N} \ket{G} + \frac{N - 2}{N} \ket{B} \end{align*}$$ </p> <p>We are now ready to analyze the effect of a single grover iterate. We have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \ket{\psi} &amp;= \frac{1}{\sqrt{N}} \ket{G} + \sqrt{\frac{N - 1}{N}} \ket{B} \\ &amp;\substack{\mathbf{O}_f \\ \mapsto} - \frac{1}{\sqrt{N}} \ket{G} + \sqrt{\frac{N - 1}{N}} \ket{B} \\ &amp;\substack{\mathbf{R} \\ \mapsto} - \frac{1}{\sqrt{N}} \left(\frac{2 - N}{N} \ket{G} + \frac{2\sqrt{N - 1}}{N} \ket{B}\right) \\ &amp;+ \sqrt{\frac{N - 1}{N}} \left(\frac{2 \sqrt{N - 1}}{N} \ket{G} + \frac{N - 2}{N} \ket{B}\right) \\ \end{align*} $$ </p> <h2 id="references">references</h2> <ul> <li><a href="https://quantum.cloud.ibm.com/learning/en/courses/fundamentals-of-quantum-algorithms/quantum-query-algorithms/introduction">quantum query algorithms by john watrous</a></li> </ul> <h2 id="footnotes">footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p><a href="https://www.math.columbia.edu/~plei/docs/quantum/nick.pdf">quantum channels</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[i finally made a quantum post]]></summary></entry><entry><title type="html">the probabilistic method is pretty funny</title><link href="https://lukshyaganjoo.github.io/blog/2023/streaming/" rel="alternate" type="text/html" title="the probabilistic method is pretty funny"/><published>2023-10-23T00:00:00+00:00</published><updated>2023-10-23T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2023/streaming</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2023/streaming/"><![CDATA[<h2 id="introduction">introduction</h2> <p>the object of consideration for today might be slightly different than what you’re used to. the computational model we will be using for this proof is the streaming model. in this model, we are given a sequence of elements in a “stream” and we have to process them one by one. given that the number of elements in our data stream could be significantly larger than the amount of memory we have, we aren’t allowed to store the entire stream in memory, and are instead allowed a small amount of space to store the information we deem relevant. the question now becomes; what functions of the input stream can we compute with what amount of time and model? while we will be focusing on space; similar considerations and conclusions can be made for time. under normal circumstances, the study of such a model would be motivated by introducing the notion of <strong>moments</strong> where you consider the stream as a high-dimensional vector and may want to compute the \(k\)-th moment of the vector. while this is useful, the motivating route we take is slightly different.</p> <h2 id="the-problem">the problem</h2> <blockquote> <p>consider the problem of finding the number of distinct elements in a stream of elements. while this is a trivial problem in the standard computational model since we can simply store the elements and count the number of distinct elements, the streaming model makes this problem significantly more challenging.</p> </blockquote> <blockquote> <p><strong>theorem</strong></p> <p>we present a streaming algorithm to estimate the number of distinct elements in the sequence with multiplicative error \(1 \pm \epsilon\) for some \(\epsilon \in (0, 1)\). for the algorithm, we assume access to \(k\) independent hash functions as described above. the number of hash functions required depends on the desired precision, i.e. \(k \leq \mathcal{O}(1/\epsilon^2)\) is sufficient to achieve the desired error guarantee.</p> </blockquote> <blockquote> <p><strong>definition</strong></p> <p>a <strong>hash function</strong> is a function \(h : A \to B\) where \(A = \{a \in \{0, 1\}^j \mid j \in \mathbb{N}\}\) is the set of all bit sequences of arbitrary length and \(B = \{0, 1\}^k\) is the set of all bit sequences of a specific length \(k \in \mathbb{N}\). Note that it is not important to understand deeply the definition of a hash function, rather simply that with the appropriate independence assumptions, it essentially behaves like a uniform random variable.</p> </blockquote> <h2 id="a-surprisingly-simple-solution">a surprisingly simple solution</h2> <p style="overflow-x:auto"> $$ \begin{array}{l} \textbf{function} \; \texttt{estimateDistinct}(\text{stream}, k): \\ \quad \text{initialize } k \text{ independent hash functions } h_1, h_2, \dots, h_k : \{0, \dots, n - 1\} \to [0, 1] \\ \quad \texttt{val}_1 \leftarrow \infty, \texttt{ val}_2 \leftarrow \infty, \dots, \texttt{ val}_k \leftarrow \infty \\ \quad \text{for } i = 1 \text{ to } n: \\ \quad \quad x \leftarrow \texttt{stream}[i] \\ \quad \quad \text{for } j = 1 \text{ to } k: \\ \quad \quad \quad \texttt{val}_j \leftarrow \min\{\texttt{val}_j, h_j(x)\} \\ \quad Z \leftarrow \frac{1}{k} \sum \limits_{i = 1}^k \texttt{val}_i \\ \quad \text{return } \bigg\lfloor \frac{1}{Z} - 1 \bigg\rfloor \\ \end{array} $$ </p> <p>what the above algorithm is essentially doing is hashing each element in the stream \(k\) times and keeping track of the minimum hash value for each element. it is then taking the average of these minimum hash values and returning the appropriate estimate on the last line of the algorithm.</p> <h2 id="why-is-an-average-of-minimums-necessary">why is an average of minimums necessary?</h2> <p>in order to answer this question, we consider the following modification to the algorithm. \(\begin{array}{l} \textbf{function } \texttt{incorrectEstimateDistinct(stream, k)} \\ \quad \text{initialize a hash function } h : \{0, \dots, n - 1\} \to [0, 1] \\ \quad \texttt{val} \gets \infty \\ \quad \textbf{for } i = 1 \textbf{ to } n: \\ \quad\quad x \gets \texttt{stream}[i] \\ \quad\quad \texttt{val}_i \gets \min(\texttt{val}_i, h(x)) \\ \quad Z \gets \min_{i \in [k]} \texttt{val}_i \\ \quad \textbf{return } \left\lfloor \frac{1}{Z} - 1 \right\rfloor \end{array}\)</p> <p>while the above algorithm is not sufficient for our purposes, it does produces the correct solution on average! (in fact we use this very fact in the proof of the main theorem).</p> <blockquote> <p><strong>lemma 1</strong></p> <p style="overflow-x:auto"> $$\begin{align*} \text{number of distinct elements in the data stream} = \frac{1}{\mathbb{E}[Z]} - 1\end{align*} $$ </p> </blockquote> <p><strong>proof</strong> Indeed from Theorem 2, we have that \(\mathbb{E}[Z] = \frac{1}{k + 1}\) and a matter of simple algebra yields that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Z] &amp;= \frac{1}{k + 1} \implies \frac{1}{\mathbb{E}[Z]} = k + 1 \\ \end{align*} $$ thereby ensuring that we return the correct estimate on average. </p> <p>So what goes wrong in the above algorithm. The issue is that the variance of the estimator is too high. In fact, defining \(Y = \min (h(x_1), h(x_2), \dots, h(x_n))\) as above in the algorithm; following similar steps to the variance computation in Theorem 2 yields</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(Y) &amp;= \frac{k}{(k + 1)^2 (k + 2)} \approx \frac{1}{(k + 1)^2} \end{align*} $$ </p> <p>and therefore by Chebyshev’s inequality, the probability that the estimate is off by more than a factor of \(1 \pm \epsilon\) is \(\mathcal{O}(1/\epsilon^2)\).</p> <h2 id="the-proof">the proof</h2> <p>firstly we verify that this modification does not alter the correctness of the estimation on average. this is easy because</p> <p style="overflow-x:auto"> $$ \begin{align*} Z = \frac{1}{k} \sum_{i = 1}^k Y_i \quad \text{ where } Y_i = \min\{h_i(x_1), h_i(x_2), \dots, h_i(x_m)\} \end{align*} $$ </p> <p>and therefore</p> <p style="overflow-x:auto"> $$ \mathbb{E}[Z] = \mathbb{E}\left[\frac{1}{k} \sum_{i = 1}^k Y_i\right] = \frac{1}{k} \sum_{i = 1}^k \mathbb{E}[Y_i] = \frac{1}{k} \sum_{i = 1}^k \frac{1}{m + 1} = \frac{1}{k} \cdot \frac{k}{m + 1} = \frac{1}{m + 1} $$ </p> <p>Turning our attention to the problem child of it all, we now compute the variance of $Z$. We have that $$</p> <p style="overflow-x:auto"> \begin{align*} \text{Var}(Z) &amp;= \text{Var}\left(\frac{1}{k} \sum_{i = 1}^k Y_i\right) = \frac{1}{k^2} \text{Var}\left(\sum_{i = 1}^k Y_i\right) = \frac{1}{k^2} \sum_{i = 1}^k \text{Var}(Y_i) \\ &amp;\leq \frac{1}{k^2} \sum_{i = 1}^k \frac{1}{(m + 1)^2} = \frac{1}{k^2} \cdot \frac{k}{(m + 1)^2} = \frac{1}{k(m + 1)^2} &amp;&amp;\text{by theorem 2} \end{align*} $$ </p> <h2 id="appendix">appendix</h2> <blockquote> <p><strong>theorem 1</strong></p> <p>Let \(X_1, X_2, \dots, X_n\) be independent random variables uniformly distributed in \([0, 1]\) and let \(X := \min\{X_1, X_2, \dots, X_n\}\). Then</p> <p style="overflow-x:auto"> $$ f_X(x) = \begin{cases}0 &amp; \text{if } x \notin [0, 1] \\ n (1 - x)^{n - 1} &amp; \text{if } x \in [0, 1]\end{cases} $$ </p> </blockquote> <p><strong>proof</strong> We proceed as usual; by first computing the CDF, i.e. \(F_X(x) = \Pr[X \leq x]\). We have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[X \leq x] &amp;= 1 - \Pr[X \geq x] = 1 - \Pr[\min\{X_1, X_2, \dots, X_n\} \geq x] \\ &amp;= 1 - \Pr[X_1 \geq x, X_2 \geq x, \dots, X_n \geq x] = 1 - \prod_{i = 1}^n \Pr[X_i \geq x] \\ &amp;= 1 - \prod_{i = 1}^n (1 - x) = 1 - (1 - x)^n \end{align*} $$ </p> <p>Therefore we have that</p> \[F_X(x) = \begin{cases} 0 &amp; \text{if } x &lt; 0 \\ 1 - (1 - x)^n &amp; \text{if } x \in [0, 1] \\ 1 &amp; \text{if } x &gt; 1 \end{cases}\] <p>The result thus follows by taking the derivative of \(F_X(x)\).</p> <blockquote> <p><strong>theorem 2</strong></p> <p>let \(X_1, X_2, \dots, X_n\) be independent random variables uniformly distributed in \([0, 1]\). let \(Y = \min \limits_{i \in [n]} X_i\). then \(\mathbb{E}[Y] = \frac{1}{n + 1}\) and \(\text{Var}(Y) \leq \frac{1}{(n + 1)^2}\).</p> </blockquote> <p><strong>proof</strong> we first compute the expectation, by definition</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y] &amp;= \int_0^1 x n (1 - x)^{n - 1} dx = n \int_0^1 x (1 - x)^{n - 1} dx = \frac{1}{n + 1} &amp;&amp; \text{by theorem 1} \end{align*} $$ </p> <p>We now turn our attention to computing the variance. We need one more result before we can proceed. We compute \(\mathbb{E}[Y^2]\) and via a similar application of theorem 1 we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y^2] &amp;= \int_0^1 x^2 n (1 - x)^{n - 1} dx &amp;&amp; \text{by theorem 1} \\ &amp;= n \int_0^1 x^2 (1 - x)^{n - 1} dx = \frac{2}{(n + 1)(n + 2)} \end{align*} $$ </p> <p>We now finally compute the variance, which is given by</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(Y) &amp;= \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 = \frac{2}{(n + 1)(n + 2)} - \frac{1}{(n + 1)^2} \\ &amp;\leq \frac{2}{(n + 1)^2} - \frac{1}{(n + 1)^2} = \frac{1}{(n + 1)^2} \end{align*} $$ </p> <h2 id="references">references</h2> <ul> <li><a href="https://courses.cs.washington.edu/courses/cse521/23au/521-lecture-6.pdf">uw’s exposition on sketching and hashing algorithms</a></li> <li><a href="https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15850-f20/www/notes/lec11.pdf">some lovely notes on variance reduction and streaming</a></li> <li><a href="https://courses.cs.washington.edu/courses/cse312/20su/files/student_drive/section4_distinct_elements_notes.pdf">count min-hash by alex tsun @ stanford</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[reframing conventionally simple computations with limited models of computation]]></summary></entry><entry><title type="html">perfect matchings and how most graphs are connected</title><link href="https://lukshyaganjoo.github.io/blog/2023/matchings/" rel="alternate" type="text/html" title="perfect matchings and how most graphs are connected"/><published>2023-10-09T00:00:00+00:00</published><updated>2023-10-09T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2023/matchings</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2023/matchings/"><![CDATA[<h2 id="introduction-and-some-background">introduction and some background</h2> <p>i genuinely think that there isn’t a single person who wouldn’t like graph theory if given the chance. if you aren’t familiar with the concept, don’t worry; just trust me on this one. my first proper exposure to graph theory funnily enough was in a randomized algorithms course i took fairly recently. i have a lot to say about how wicked the probabilistic method is but let’s save that for another day. the moral of this story and maybe the purpose of this somewhat convoluted introduction is that “combining certain kinds of networks” (whatever that means) has interesting connections to whether a person who were stuck in said network can get from any point to any other point.</p> <p>given that the above discussion was quite vague, it may help to see some concrete notions of the tools we’ll be using in the problem.</p> <blockquote> <p><strong>definitions</strong></p> <ul> <li> <p>a graph \(\mathcal{G}\) is a collection of vertices \(V\) and edges \(E\) such that each edge connects two vertices, where notationally we write \(\mathcal{G} = (V, E)\). a <strong>multi-graph</strong> is a graph that allows for multiple edges between the same pair of vertices.</p> </li> <li> <p>a graph is said to be <strong>connected</strong> if there exists a path between any two vertices in the graph, i.e. for any two vertices \(u, v \in V\), there exists a sequence of vertices \(u = v_0, v_1, \ldots, v_k = v\) such that \((v_i, v_{i+1}) \in E\) and \(v_i \in v\) for all \(i = 0, 1, \ldots, k-1\).</p> </li> <li> <p>a <strong>matching</strong> in a graph \(\mathcal{G}\) is a set of edges \(M \subseteq E\) such that no two edges in \(M\) share a vertex. a matching is said to be <strong>perfect</strong> if every vertex in \(V\) is incident to exactly one edge in \(M\).</p> </li> </ul> </blockquote> <h2 id="the-problem-itself">the problem itself</h2> <blockquote> <p>let \(n\) be an even integer and let \(\mathcal{G}_k\) be the multi-graph on \(n\) vertices formed by taking the union of \(k\) (possibly overlapping) perfect matchings, which are chosen uniformly at random from the set of all perfect matchings on \(n\) vertices. show that for \(k \geq 3\), the probability that \(\mathcal{G}_k\) is <strong>connected</strong> tends to \(1\) as \(n \to \infty\).</p> </blockquote> <h2 id="the-proof">the proof</h2> <p>it may not be immediately clear how to proceed with this problem, in fact it may not even be readily apparent why the hypothesis of having \(k \geq 3\) is necessary. the necessity of this condition is stated below</p> <h3 id="identifying-the-problem-child">identifying the problem-child</h3> <blockquote> <p><strong>theorem</strong></p> <p>for \(k = 2\), the probability that \(\mathcal{G}_k\) is connected tends to \(0\) as \(n \to \infty\).</p> </blockquote> <p>When \(k = 2\), we can see that the multi-graph formed by the union of two perfect matchings results in the degree of every vertex being 2 (since each vertex in a perfect matching has degree 1). Following this reasoning, the graph is a union of disjoint cycles, and it is equivalent to say that the graph is connected if and only if there exists a <strong>Hamiltonian cycle</strong>.</p> <blockquote> <p><strong>definition</strong></p> <p>a <strong>cycle</strong> in a graph \(\mathcal{G}\) is a sequence of vertices \(v_1, v_2, \ldots, v_k\) such that \((v_i, v_{i+1}) \in E\) for all \(i = 1, 2, \ldots, k - 1\) and \((v_k, v_1) \in E\). a cycle is said to be <strong>Hamiltonian</strong> if it visits every vertex exactly once.</p> </blockquote> <p>We now fix the first perfect matching. We proceed by computing the probability that the second perfect matching does not close a shorter cycle. We start with the first vertex \(v_1\) and the endpoint of the first edge \((v_1, u_1)\). \(u_1\) must be connected to some other vertex \(v_2\) in the second perfect matching, in order to not close a cycle, let’s call this vertex \(v_2\). Since there are \(n\) vertices, there are \(n - 2\) ways to construct an edge from \(v_2\) to another vertex. Therefore the probability is given by \((n - 2)/(n - 1)\) (since we don’t consider self-loops) Now \(v_2\) is part of some other edge \((v_2, u_2)\). \(u_2\) must be connected to some other vertex \(v_3\), and not any of the previous vertices. This gives us \((n - 4)/(n - 3)\) choices where the denominator is \(n - 3\) since we don’t consider edges going from \(u_2\) to itself, \(v_1\), or \(v_2\). We continue this process until we reach the last vertex \(v_{n - 1}\), giving us</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[\mathcal{G}_k \text{ is connected}] = \frac{n - 2}{n - 1} \cdot \frac{n - 4}{n - 3} \cdots \frac{2}{1} = \frac{(n - 2)!!}{(n - 1)!!} \end{align*} $$ </p> <p>Since \(n = 2t\) (there are an even number of vertices in a perfect matching), we have</p> <p style="overflow-x:auto"> $$ \begin{align*} \frac{(n - 2)!!}{(n - 1)!!} &amp;= \frac{(2t - 2)!!}{(2t - 1)!!} = \frac{(2t - 2) \cdot (2t - 4) \cdots 2}{(2t - 1) \cdot (2t - 3) \cdots 1} \\ &amp;= \frac{2^{t - 1} (2t)!! (t - 1)!}{(2t)!} = 2^{2t - 1} \cdot \frac{(t - 1)!}{(2t)!} \cdot k! \\ &amp;\approx 2^{2t - 1} \cdot \frac{\left(\frac{t - 1}{e}\right)^{t - 1}}{\left(\frac{2t}{e}\right)^{2t}} \cdot \frac{t^{t}}{e^{t}} = \frac{e}{2} \cdot \frac{(t - 1)^{t - 1}}{t^{2t}} \cdot t^{t} = \frac{e}{2} \cdot \frac{(t - 1)^{t - 1}}{t^t} \\ &amp;= \frac{e}{2} \cdot \frac{\left(1 - \frac{1}{t}\right)^{t - 1}}{t} \approx e \cdot \frac{e^{-1/t (t - 1)}}{2k} = e \cdot \frac{e^{1/t - 1}}{2t} \\ &amp;= e/n \cdot e^{1/t - 1} = e^{1/t} / n \end{align*} $$ </p> <p>where all the approximations were via Stirlings and we made use of the fact that \(n = 2t\). The desired result follows by noting that \(\lim \limits_{n \to \infty} \frac{e^{1/t}}{n} = 0\).</p> <h3 id="the-main-result">the main result</h3> <p>now that we have established the necessity of the condition \(k \geq 3\), we can proceed with the main result. specifically we show that the claim is true for \(k = 3\) and our method immediately generalizes to any \(k &gt; 3\).</p> <p>Observe that a graph is said to be connected if and only if every cut contains an edge. We define \(\mathcal{G}_k = (V, E)\) to be a multi-graph with \(2n\) vertices (since perfect matchings only exist on graphs with an even number of vertices). Let \(\Omega\) be the set of all possible cuts in \(\mathcal{G}_k\). We then have</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr\bigg[\mathcal{G}_k \text{ is connected}\bigg] &amp;= \Pr\bigg[\bigcap_{\omega \in \Omega} \omega \text{ contains an edge}\bigg] \end{align*} $$ </p> <p>We want to show that this probability approaches 1 for \(n \to \infty\). Note that this is equivalent to saying</p> <p style="overflow-x:auto"> $$ \Pr\bigg[G_k \text{ is disconnected}\bigg] \to 0 \text{ as } n \to \infty $$ </p> <p>Now using De Morgan’s Law and simplifying slightly, we have</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr\bigg[\mathcal{G}_k \text{ is disconnected}\bigg] &amp;= \Pr\bigg[\left(\bigcap_{\omega \in \Omega} \omega \text{ contains an edge}\right)^c\bigg] \\ &amp;= \Pr\bigg[\bigcup_{\omega \in \Omega} \omega \text{ contains no edges}\bigg] \\ &amp;\leq \sum_{\omega \in \Omega} \Pr\bigg[\omega \text{ contains no edges}\bigg] &amp;&amp; \text{Union Bound} \end{align*} $$ </p> <p>We are now interested in the probability that a cut doesn’t have an edge. Therefore we fix some cut \(C = (S, V \setminus S)\) such that \(\lvert S \rvert = l\) and \(\lvert V \setminus S \rvert = 2n - l\). We’re interested in computing the probability that there is no edge between \(S\) and \(V \setminus S\), i.e. \(\Pr[\lvert E(S, V \setminus S) \rvert = 0]\). We make the observation that if \(\lvert E(S, V \setminus S) \rvert = 0\), then the perfect matchings restricted to \(S\) are also a perfect matching. Note that since we have perfect matchings, any cut of odd size is assured to have size greater than zero. Thus, it suffices to consider the cases where the cut \(C\) divides graph into a subsets of size \(2i, 2n - 2i\) where \(1 \leq i \leq n - 1\), allowing us to conclude that \(l = 2i\). Note that the number of perfect matchings on \(2i\) vertices is given by</p> <p style="overflow-x:auto"> $$ \# \text{ of perfect matchings on } 2i \text{ vertices} = \frac{(2i)!}{2^i i!} = (2i - 1)!! $$ </p> <p>where \(!!\) indicates the double factorial. This number can be found via picking a vertex \(v_1\), noting that there are \(2i - 1\) choices for the vertex \(v_2\) as its endpoint (since it cannot be itself), then find another vertex \(v_3\), observing that it has \(2i - 3\) vertices to choose from (cannot be any of the previous 2 or itself) and so on. Therefore, all this implies that finding the number of perfect matchings on \(n\) vertices with no edges is equivalent to finding a perfect matching on \(S\) and a perfect matching on \(V \setminus S\) where \(\lvert S \rvert = 2i, \lvert V \setminus S \rvert = 2n - 2i\).</p> <p>We partition over the size of all possible cuts. Considering the cuts of size \(2i\) and \(2n - 2i\) where \(i \in \{1, \dots, n - 1\}\) and then noting that for a single perfect matching, the probability that a cut of size \(2i\) doesn’t have an edge is given by</p> <p style="overflow-x:auto"> $$ \Pr[\text{cut of size } 2i \text{ has no edges for a perfect matching}] = \frac{\frac{(2i)!}{2^i i!} \cdot \frac{(2n - 2i)!}{2^{n - i} (n - i)!}}{\frac{(2n)!}{2^n n!}} $$ </p> <p>The probability that all 3 perfect matchings have no edges crossing this cut is given by</p> <p style="overflow-x:auto"> $$ \Pr[\text{cut of size } 2i \text{ has no edges for 3 perfect matchings}] = \left(\frac{\frac{(2i)!}{2^i i!} \cdot \frac{(2n - 2i)!}{2^{n - i} (n - i)!}}{\frac{(2n)!}{2^n n!}}\right)^3 $$ </p> <p>since the matchings are drawn uniformly at random. We also note that there are \(\binom{2n}{2i}\) ways to choose the vertices for the cut. Therefore the algebra in the sum we had earlier gives us</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr\bigg[G_k \text{ is disconnected}\bigg] &amp;\leq \sum_{i = 1}^{n - 1} \Pr[\text{cut of size } 2i \text{ has no edges}] \\ &amp;= \sum_{i = 1}^{n - 1} \binom{2n}{2i} \cdot \left(\frac{\frac{(2i)!}{2^i i!} \cdot \frac{(2n - 2i)!}{2^{n - i} (n - i)!}}{\frac{(2n)!}{2^n n!}}\right)^3 \\ &amp;\approx \sum_{i = 1}^{n - 1} \frac{\left(\left(\frac{n}{e}\right)^n \cdot \left(\frac{e}{i}\right)^i \cdot \left(\frac{e}{n - i}\right)^{n - i}\right)^3}{\left(\left(\frac{2n}{e}\right)^{2n} \cdot \left(\frac{e}{2i}\right)^{2i} \cdot \left(\frac{e}{2n - 2i}\right)^{2n - 2i}\right)^2} \\ &amp;= \sum_{i = 1}^{n - 1} \frac{n^{3n}}{i^{3i} (n - i)^{3(n - i)}} \cdot \frac{i^{4i} (n - i)^{4(n - i)}}{n^{4n}} = \sum_{i = 1}^{n - 1} \frac{i^i (n - i)^{n - i}}{n^n} \end{align*} $$ </p> <p>where we have once again made use of Striling’s approximation. Note that this is slightly overcounting (by a factor of 2 in fact) since for a cut of size \(2k\), and the complementary cut of size \(2n - 2k\), we count this cut \((S, V \setminus S)\) twice once for the \(i = k\) case and another time for the \(i = n - k\). However, this only changes a factor of 2 and asymptotically derives the same result, so we push through unfazed.</p> <p>We now turn our attention to bounding the following sum, for \(u = 3\).</p> <p style="overflow-x:auto"> $$ \Pr[G_k \text{ is disconnected}] \leq \frac{1}{n^{n}} \sum_{i = 1}^{n - 1} i^{i} (n - i)^{n - 1} $$ </p> <p>Therefore, the expression of interest has now become</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr\bigg[G_k \text{ is disconnected}\bigg] &amp;\leq \sum_{i = 1}^{n - 1} \frac{i^i (n - i)^{n - i}}{n^n} = \frac{1}{n^n} \sum_{i = 1}^{n - 1} i^i (n - i)^{n - i} \\ &amp;\approx \frac{2}{n^n} \left(\sum_{i = 1}^{n/2} i^i (n - i)^{n - i} \right) \\ &amp;= \frac{2}{n^n} \left((n - 1)^{n - 1} + \sum_{i = 2}^{n/2} i^i (n - i)^{n - i} \right) \end{align*} $$ </p> <p>Now, getting slightly more formal with things. We define \(f : [2, n/2] \to \mathbb{R}\) where \(f(x) = x^x (n - x)^{n - x}\). Since \(f\) is continuous, we also have that \(f\) is bounded which is quite nice! Taking logs on both sides gives us</p> <p style="overflow-x:auto"> $$ \begin{align*} f(x) &amp;= x^x (n - x)^{n - x} \\ \log f(x) &amp;= \log \bigg(x^x (n - x)^{n - x}\bigg) \\ &amp;= \log \bigg(x^x\bigg) + \log \bigg((n - x)^{n - x}\bigg) \\ \log f(x) &amp;= x \log x + (n - x) \log (n - x) \end{align*} $$ </p> <p>We now argue that the function is decreasing, i.e. \(f'(x) &lt; 0\). We have</p> <p style="overflow-x:auto"> $$ \begin{align*} f'(x) / f(x) &amp;= \frac{d}{dx} \left(\left(x \log x\right) + (n - x) \log (n - x)\right) \\ &amp;= \frac{d}{dx} \left(x \log x\right) + \frac{d}{dx} \left((n - x) \log (n - x)\right) \\ &amp;= 1 + \log x - \log (n - x) - 1 = \log x - \log (n - x) \\ &amp;= \log \left(\frac{x}{n - x}\right) &lt; 0 &amp;&amp; \text{since } x &lt; n - x \end{align*} $$ </p> <p>where the last inequality follows from the fact that \(x \leq n/2\), and so we have \(\frac{x}{n - x} &lt; 1\) and therefore \(\log \left(\frac{x}{n - x}\right) &lt; 0\).</p> <p>Therefore the maximum of \(f\) for \(2 \leq x \leq n/2\) is attained at \(x = 2\), which gives us the very useful fact that for all \(x \in [2, n/2]\), \(f(x) \leq f(2)\), i.e.</p> <p style="overflow-x:auto"> $$ f(x) \leq 2^2 (n - 2)^{n - 2} = 4 (n - 2)^{n - 2} $$ </p> <p>Putting all these facts together, we have</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr\bigg[G_k \text{ is disconnected}\bigg] &amp;\leq \frac{2}{n^n} \left((n - 1)^{n - 1} + \sum_{i = 2}^{n/2} i^i (n - i)^{n - i} \right) \\ &amp;\leq \frac{2}{n^n} \left((n - 1)^{n - 1} + \sum_{i = 2}^{n/2} 4 (n - 2)^{n - 2} \right) \\ &amp;\leq \frac{2}{n^n} \left((n - 1)^{n - 1} + 4 (n - 2)^{n - 2} \cdot \frac{n}{2} \right) \\ &amp;= \frac{2 (n - 1)^{n - 1}}{n^n} + \frac{4 (n - 2)^{n - 2}}{n^{n - 1}} \\ &amp;= \frac{2 \left(\frac{n - 1}{n}\right)^{n - 1}}{n} + \frac{4 \left(\frac{n - 2}{n}\right)^{n - 2}}{n} \end{align*} $$ </p> <p>Taking limits, we get</p> <p style="overflow-x:auto"> $$ \begin{align*} \lim_{n \to \infty} \frac{2 \left(\frac{n - 1}{n}\right)^{n - 1}}{n} + \frac{4 \left(\frac{n - 2}{n}\right)^{n - 2}}{n} = \lim_{n \to \infty} \left(\frac{2}{en} + \frac{4}{e^2 n}\right) = 0 \end{align*} $$ </p> <p>which finally implies</p> <p style="overflow-x:auto"> $$ \Pr\bigg[\mathcal{G}_k \text{ is disconnected}\bigg] \to 0 \text{ as } n \to \infty \Longleftrightarrow \Pr\bigg[\mathcal{G}_k \text{ is connected}\bigg] \to 1 \text{ as } n \to \infty $$ </p> <p>and we are finally done. We have emerged victorious.</p> <h2 id="references">references</h2> <ul> <li><a href="https://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture7.pdf">high-dimensional expanders</a></li> <li><a href="https://people.math.sc.edu/lu/teaching/2006spring_777/math776/lecture7.pdf">perfect matchings</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[a taste of the types of results one works with when dealing with expander graphs]]></summary></entry><entry><title type="html">chebyshev ruins convocations</title><link href="https://lukshyaganjoo.github.io/blog/2023/chebyshev/" rel="alternate" type="text/html" title="chebyshev ruins convocations"/><published>2023-06-15T00:00:00+00:00</published><updated>2023-06-15T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2023/chebyshev</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2023/chebyshev/"><![CDATA[<h2 id="some-background">some background</h2> <p>i was enrolled in this cs course titled “toolkit for modern algorithms” spring of this year. that class, more than anything else felt like the birth child of a standard algorithms class and techniques you’d learn in machine learning. i have several things i could say about that class, but that’s not what the point of this post is. similar to several other classes in the cs department here, we had weekly assignments. in the penultimate week of the class, there was an extra credit problem assigned that i spent a lot of time pondering about. providing some more context for why the title of this proof/blogpost is what it is, i was not able to figure out the problem in time. to make things more agonizing, i flew to maryland the same week for my brother’s convocation! while it was great seeing my brother graduate, it would have been significantly better had i not spent as much time as i did thinking about how a simple inequality lived in my head rent-free.</p> <h2 id="the-actual-problem">the actual problem</h2> <p>now that i’m done rambling about the context, the problem in and of itself is actually quite interesting, and i think there’s something about the somewhat unusual combination techniques used that is useful to know. the problem is as follows:</p> <blockquote> <p>For any \(n\) real numbers \(a_1, a_2, \dots a_n\) satisfying \(a_1^2 + a_2^2 + \dots + a_n^2 = 1\), if \(\sigma_1 \sigma_2, \dots, \sigma_n \in \{-1, 1\}\) are i.i.d uniform random signs, then</p> <p style="overflow-x:auto"> $$ \Pr \left[\left \vert \sum_{i = 1}^{n} a_i \sigma_i\right \vert \leq 1\right] \geq \frac{1}{32} $$ </p> </blockquote> <h2 id="the-proof">the proof</h2> <p>we split the proof into two cases based on the size of coefficients</p> <blockquote> <p><strong>Case 1: a big coefficient</strong></p> <p>For simplicity, we may sort so that \(\vert a_1 \vert \geq \vert a_2 \vert \geq \dots \vert a_n \vert\).</p> <p>Additionally we define the random variable \(X = \sum \limits_{i = 1}^{n} a_i \sigma_i\). Based on the assumption of the case, we assume without loss of generality that \(\vert a_1 \vert \geq 1/\sqrt{2}\) and apply Chebyshev’s inequality on \(X_1 = X - a_1 \sigma_1\) to conclude that</p> <p style="overflow-x:auto"> $$\Pr[\vert X \vert \leq 1] \geq \frac{1}{4}$$ </p> </blockquote> <p><strong>proof:</strong> We first compute out the expectation of \(X_1\) where we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[X_1] &amp;= \mathbb{E}\left[X - a_1 \sigma_1 \right] &amp;&amp;\text{by definition of } X_1 \\\ &amp;= \mathbb{E}\left[\sum_{i = 1}^{n} a_i \sigma_i - a_1 \sigma_1 \right] &amp;&amp;\text{by definition of } X \\\ &amp;= \mathbb{E}\left[\sum_{i = 2}^{n} a_i \sigma_i\right] \\\ &amp;= \sum_{i = 2}^{n} a_i \mathbb{E}[\sigma_i] = 0 &amp;&amp;\text{linearity of expectation and Theorem 1} \end{align*} $$ </p> <p>Now we define the event \(\mathcal{A}\) as the event that \(X\) and \(\sigma_1 a_1\) have the same sign. Note that since \(\sigma_1\) is a uniformly random sign, \(\Pr[\mathcal{A}] = 1/2\). We therefore have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[\vert X\vert \leq 1] &amp;= \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1] \\\ &amp;= \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}] \cdot \Pr[\mathcal{A}] \\\ &amp;+ \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}^c] \cdot \Pr[\mathcal{A}^c] \\\ &amp;\geq \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}] \cdot \Pr[\mathcal{A}] \\\ &amp;= \frac{1}{2} \cdot \Pr[\vert X_1 + a_1 \sigma_1\vert \leq 1 \mid \mathcal{A}] \\\ &amp;\geq \frac{1}{2} \cdot \Pr[\vert X_1\vert - \vert a_1 \vert \leq 1] = \frac{1}{2} \cdot \Pr[\vert X_1\vert \leq 1 + \vert a_1 \vert ] &amp;&amp;\text{triangle inequality} \\\ &amp;= \frac{1}{2} \left(1 - \Pr[\vert X_1\vert \geq 1 + \vert a_1 \vert ]\right) \geq \frac{1}{2} \left(1 - \frac{\text{Var}(X_1)}{(1 + \vert a_1 \vert )^2}\right) &amp;&amp;\text{Chebyshev's and Theorem 2} \\\ &amp;\geq 1/4 \end{align*} $$ </p> <blockquote> <p><strong>Case 2: all small coefficients</strong></p> <p>We now assume that \(1/\sqrt{2} &gt; \vert a_1 \vert \geq \vert a_2 \vert \geq \dots \geq \vert a_n \vert\).</p> <p>The really clever thing we can do is to split the sum \(X = Y + Z\) into two pieces and apply Chebyshev’s inequality separately to \(Y\) and \(Z\) to conclude that</p> <p style="overflow-x:auto"> $$\Pr[\vert X\vert \leq 1] \geq \frac{1}{32}$$ </p> </blockquote> <p><strong>proof:</strong> We define the random variables \(Y, Z\) as follows</p> <p style="overflow-x:auto"> $$ Y = \sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1} \quad \text{and} \quad Z = \sum_{i = 1}^{n/2} a_{2i} \sigma_{2i} $$ </p> <p>We have that</p> <p style="overflow-x:auto"> $$ Y + Z = \sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1} + \sum_{i = 1}^{n/2} a_{2i} \sigma_{2i} = \sum_{i = 1}^{n} a_i \sigma_i = X $$ </p> <p>This necessarily implies as a consequence of the linearity of variance for independent random variables that</p> <p style="overflow-x:auto"> $$ \begin{align} 1 = \text{Var}\left(\sum_{i = 1}^{n} a_i \sigma_i\right) = \text{Var}(X) = \text{Var}(Y) + \text{Var}(Z) \end{align} $$ </p> <p>As a consequence of theorem 4 and \((1)\), we have that</p> <p style="overflow-x:auto"> $$ \text{Var}(Y) + \text{Var}(Z) = 1 \quad \text{and} \quad \text{Var}(Y) - \text{Var}(Z) &lt; 1/2 $$ </p> <p>Putting two and two together, we can finally ascertain that</p> <p style="overflow-x:auto"> $$ 1/4 \leq \text{Var}(Z) \leq \text{Var}(Y) \leq 3/4 $$ </p> <p>We now define the event \(\mathcal{S}\) to be the event that \(Y\) and \(Z\) have different signs. Note that since \(\sigma_1, \sigma_2, \dots, \sigma_n\) are uniformly random signs, \(\Pr[\mathcal{S}] = 1/2\). We therefore have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \Pr[\vert X \vert \leq 1] &amp;= \Pr[\vert Y + Z \vert \leq 1] \\\ &amp;= \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}] \cdot \Pr[\mathcal{S}] \\\ &amp;+ \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}^c] \cdot \Pr[\mathcal{S}^c] \\\ &amp;\geq \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}] \cdot \Pr[\mathcal{S}] = \frac{1}{2} \cdot \Pr[\vert Y + Z \vert \leq 1 \mid \mathcal{S}] &amp;&amp;\text{since } \Pr[\mathcal{S}] \\\ &amp;= \frac{1}{2} \cdot \Pr[\big\vert \vert Y \vert - \vert Z \vert \big\vert \leq \vert Y + Z \vert \leq 1] &amp;&amp;\text{reverse triangle inequality} \\\ &amp;\geq \frac{1}{2} \cdot \Pr[\big\vert \vert Y \vert - \vert Z \vert \big\vert \leq 1] \\\ &amp;\geq \frac{1}{2} \cdot \Pr[\vert Y \vert \leq 1 \cap \vert Z \vert \leq 1] \\\ &amp;= \frac{1}{2} \cdot \Pr[\vert Y \vert \leq 1] \cdot \Pr[\vert Z \vert \leq 1] &amp;&amp;\text{since } Y, Z \text{ are independent} \\\ &amp;\geq \frac{1}{2} \cdot \big(1 - \Pr[\vert Y \vert \geq 1]\big) \cdot \big(1 - \Pr[\vert Z \vert \geq 1]\big) &amp;&amp;\text{complementation} \\\ &amp;= \frac{1}{2} \cdot \big(1 - \Pr[\vert Y - \mathbb{E}[Y]\vert \geq 1]\big) \cdot \\\ &amp;\big(1 - \Pr[\vert Z - \mathbb{E}[Z]\vert \geq 1]\big) &amp;&amp;\text{Theorem 3} \\\ &amp;\geq \frac{1}{2} \cdot \big(1 - \frac{\text{Var}(Y)}{1}\big) \cdot \big(1 - \frac{\text{Var}(Z)}{1}\big) &amp;&amp;\text{Chebyshev's inequality} \\\ &amp;= \frac{1}{2} \cdot \big(1 - \text{Var}(Y)\big) \cdot \big(1 - \text{Var}(Z)\big) \\\ &amp;\geq \frac{1}{2} \frac{1}{4} \frac{1}{4} = \frac{1}{32} \end{align*} $$ </p> <h2 id="theorems-and-references">theorems and references</h2> <blockquote> <p><strong>theorem 1</strong></p> <p>For uniformly random signs \(\sigma_1, \sigma_2 \dots, \sigma_n \in \{-1, 1\}\), \(\text{Var}(\sigma_i) = 1\) for all \(i \in [n]\) and \(\mathbb{E}[\sigma_i] = 0\).</p> </blockquote> <p><strong>proof:</strong> We first compute the expectation of \(\sigma_i\) which by definition comes out to be</p> <p style="overflow-x:auto"> $$ \mathbb{E}[\sigma_i] = \sum_{x \in \Omega_X} x \cdot \Pr[X = x] = 1/2 - 1/2 = 0 $$ </p> <p>The second step in computing the variance of \(\sigma_i\) is computing out \(\mathbb{E} [\sigma_i^{2}]\) which by definition is simply</p> <p style="overflow-x:auto"> $$ \mathbb{E}[\sigma_i^2] = \sum_{x \in \Omega_X} x^2 \cdot \Pr[X = x] = 1/2 + 1/2 = 1 $$ </p> <p>Therefore we have from the definition of variance that</p> <p style="overflow-x:auto"> $$ \text{Var}(\sigma_i) = \mathbb{E}[\sigma_i^2] - \mathbb{E}[\sigma_i]^2 = 1 - 0 = 1 $$ </p> <p>thereby settling the proof.</p> <blockquote> <p><strong>theorem 2</strong></p> <p>If \(X = \sum \limits_{i = 1}^{n} a_i \sigma_i\) for uniformly random signs \(\sigma_1, \sigma_2, \dots, \sigma_n \in \{-1, 1\}\), where \(\vert a_1 \vert \geq 1/\sqrt{2}\) and \(X_1 = X - a_1 \sigma_1\), then we can establish an upper bound on the variance, more concretely, \(\text{Var}(X_1) \leq 1/2\). Note here that in this setting we also have that \(a_1^2 + \dots + a_n^2 = 1\).</p> </blockquote> <p><strong>proof:</strong> Note that the setting we’re interested in is when the \(\sigma_i\)’s are independently and identically distributed which makes our life a lot easier. We therefore have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(X_1) &amp;= \text{Var}\bigg(X - a_1 \sigma_1\bigg) &amp;&amp;\text{by definition of } X \\ &amp;= \text{Var}\left(\sum_{i = 1}^{n} a_i \sigma_i - a_1 \sigma_1\right) &amp;&amp;\text{by definition of } X_1 \\ &amp;= \text{Var}\left(\sum_{i = 2}^{n} a*i \sigma_i\right) &amp;&amp;\text{simplifying} \\ &amp;= \sum_{i = 2}^{n} \text{Var}(a_i \sigma_i) = \sum_{i = 2}^{n} a_i^2 \text{Var}(\sigma_i) &amp;&amp;\text{independence} \\ &amp;= \sum_{i = 2}^{n} a_i^2 = \sum_{i = 1}^{n} a_i^2 - a_1^2 &amp;&amp;\text{Theorem 1} \\ &amp;= 1 - a_{1}^2 = 1 - \vert a_{1}\vert ^{2} &amp;&amp;\text{since } a_1 \in \mathbb{R} \end{align*} $$ </p> <p>The rest of our argument follows from a simple bounding argument on \(\vert a_1 \vert\) therefore giving us</p> <p style="overflow-x:auto"> $$ \vert a_1 \vert \geq 1/\sqrt{2} \implies \vert a_1 \vert ^2 \geq 1/2 \implies - \vert a_1 \vert ^2 \leq 1/2 \implies \text{Var}(X_1) = 1 - \vert a_1 \vert ^2 \leq 1/2 $$ </p> <p>thereby settling the proof.</p> <blockquote> <p><strong>theorem 3</strong></p> <p>If we define the random variable \(Y = \sum \limits_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1}\) and \(Z = \sum \limits_{i = 1}^{n/2} a_{2i} \sigma_{2i}\), then we have that \(\mathbb{E}[Y] = 0\) and \(\mathbb{E}[Z] = 0\).</p> </blockquote> <p><strong>proof:</strong> We have as a consequence of Theorem 1 and linearity of expectation that</p> <p style="overflow-x:auto"> $$ \mathbb{E}[Y] = \mathbb{E}\left[\sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1}\right] = \sum_{i = 1}^{n/2} a_{2i - 1} \mathbb{E}[\sigma_{2i - 1}] = 0 $$ </p> <p>Similarly, we have that</p> <p style="overflow-x:auto"> $$ \mathbb{E}[Z] = \mathbb{E}\left[\sum_{i = 1}^{n/2} a_{2i} \sigma_{2i}\right] = \sum_{i = 1}^{n/2} a_{2i} \mathbb{E}[\sigma_{2i}] = 0 $$ </p> <p>thereby settling the proof.</p> <blockquote> <p><strong>theorem 4</strong></p> <p>Based on the above definitions of \(Y\) and \(Z\), we have that</p> <p style="overflow-x:auto"> $$\text{Var}(Y) - \text{Var}(Z) &lt; 1/2$$ </p> </blockquote> <p><strong>proof:</strong> We have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Var}(Y) - \text{Var}(Z) &amp;= \text{Var}\left(\sum_{i = 1}^{n/2} a_{2i - 1} \sigma_{2i - 1}\right) - \text{Var}\left(\sum_{i = 1}^{n/2} a_{2i} \sigma_{2i}\right) \\ &amp;= \sum_{i = 1}^{n/2} a_{2i - 1}^2 \text{Var}(\sigma_{2i - 1}) - \sum_{i = 1}^{n/2} a_{2i}^2 \text{Var}(\sigma_{2i}) \\ &amp;= \sum_{i = 1}^{n/2} a_{2i - 1}^2 - \sum_{i = 1}^{n/2} a_{2i}^2 = \sum_{i = 1}^{n/2} (a_{2i - 1}^2 - a_{2i}^2) &amp;&amp;\text{Theorem 1} \\ &amp;= a_1^2 + a_3^2 + \dots + a_{n - 1}^2 - a_2^2 - a_4^2 - \dots - a_n^2 \ &amp;= a_1^2 + (a_3^2 - a_2^2) + (a_5^2 - a_4^2) + \dots + (a_{n - 1}^2 - a_n^2) \\ &amp;= a_1^2 + \sum_{i = 2}^{n/2} \underbrace{(a_{2i - 1}^2 - a_{2i - 2}^2)}_{&lt; 0} &lt; a_1^2 &lt; 1/2 \end{align*} $$ </p> <p>thereby settling the proof.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[i spent far too much time looking at normal distributions for this one]]></summary></entry><entry><title type="html">why gym rats love ford-fulkerson</title><link href="https://lukshyaganjoo.github.io/blog/2023/fulkerson/" rel="alternate" type="text/html" title="why gym rats love ford-fulkerson"/><published>2023-02-24T00:00:00+00:00</published><updated>2023-02-24T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2023/fulkerson</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2023/fulkerson/"><![CDATA[<p>In any standard algorithms class, there’s going to be an entire section of the course dedicated to what computer scientists refer to as “flow”. Besides being creatively named, it is also quite powerful in solving a variety of problems. For the purposes of motivation, I will go over some background, one of the more important network flow algorithms, and an application of this algorithm in the context of proving a seemingly unnatural equivalence.</p> <h2 id="background-and-definitions">background and definitions</h2> <ul> <li>a <strong>flow network</strong> is a directed graph \(G = (V, E)\) with a capacity function \(c : E \rightarrow \mathbb{R}^+\) and a source \(s \in V\) and a sink \(t \in V\).</li> <li>in the context of this definition, a <strong>flow</strong> moves units of water from the source \(s\) to the sink \(t\) in the graph \(G\). water can only be created from the source \(s\) and can only disappear at the sink \(t\).</li> <li>every edge \(e = (u, v) \in E\) has a capacity \(c(e)\) and the flow on a given edge \(e\), has to be less than or equal to the capacity of that given edge, i.e. \(0 \leq f(e) \leq c(e)\). (note here that based on these constraints, edges can’t have negative flow).</li> <li>for every vertex \(u \in V \setminus \\{s, t\\}\), the total flow entering the vertex \(u\) must be equal to the total flow leaving the vertex \(u\), i.e. \(\sum \limits_{v : e = (v, u) \in E} f(e) = \sum \limits_{u : e = (u, t) \in E} f(e)\).</li> <li>the value of a flow is defined as the net flow leaving the vertex \(s\) or equivalently the flow entering the vertex \(t\) (note this equivalence exists because of the conservation constraint we’ve defined above).</li> </ul> <h2 id="the-problem">the problem</h2> <ul> <li>given a flow network \(G = (V, E)\) with a capacity function \(c : E \rightarrow \mathbb{R}^+\) and a source \(s \in V\) and a sink \(t \in V\), it isn’t unereasonable to try and find the maximum flow that can travel through the network. this is a somewhat interesting question to ask because the naive idea of simply sending as much flow as you can on a given edge doesn’t really work out.</li> <li>consider the following flow network</li> </ul> <p style="text-align:center;"> <img src="/assets/img/network1.png" alt="flow network 1" style="width: 100%; margin: auto;"/> </p> <ul> <li>the naive idea of finding a path from \(s\) to \(t\) and sending as much flow as possible on the edges given by that path doesn’t quite work. Consider the following path from \(s\) to \(t\) given by</li> </ul> <p style="text-align:center;"> <img src="/assets/img/network2.png" alt="flow network 2" style="width: 100%; margin: auto;"/> </p> <ul> <li>since the minimimum edge capacity on the given path is 20 units of flow, we send 20 units of flow from \(s\) to \(t\) on the given path, and given that all the edges have the same capacity, there is no redirection happening here, where we send exactly 20 units from \(s\) to \(t\) as shown below</li> </ul> <p style="text-align:center;"> <img src="/assets/img/network3.png" alt="flow network 3" style="width: 100%; margin: auto;"/> </p> <h2 id="the-solution-and-where-ford-and-fulkerson-come-in">the solution and where ford and fulkerson come in</h2> <ul> <li>we introduce a new concept called a <strong>residual graph</strong>. the residual graph of a graph \(G\) is defined as a graph \(\mathcal{R} = (V, E)\), where it has the exact same vertices and edges as the original graph, but differs from the original graph with regards to the capacities along each of the edges.</li> <li>if the original graph has an edge \((u, v)\) with capacity \(c\), and the flow sends along \(f_{u, v}\) along the given edge, we do the following <ul> <li>we include the edge \((u, v)\) in \(\mathcal{R}\) with capacity \(c - f_{u, v}\) as long as \(c - f_{u, v} &gt; 0\).</li> <li>we include the edge \((v, u)\) in \(\mathcal{R}\) with capacity \(f_{u, v}\) as long as \(f_{u, v} &gt; 0\).</li> </ul> </li> <li> <h3 id="the-algorithm-ford-and-fulkerson">the algorithm (ford and fulkerson)</h3> <ul> <li>while (\(\texttt{true}\)) <ul> <li>run \(\texttt{BFS}\) on \(\mathcal{R}\) starting from \(s\).</li> <li>record the predecessors to find an \(s-t\) path.</li> <li>if you can’t reach \(t\), then \(\texttt{break}\).</li> <li>iterate through the path and find the minimum residual capacity edge \(c\).</li> <li>add \(c\) to the flow on the path (on every edge in the path).</li> <li>update the residual graph \(\mathcal{R}\).</li> </ul> </li> <li> <h2 id="runtime">runtime:</h2> <ul> <li>\(\mathcal{O}(\vert E \vert \cdot f)\) where \(f\) is the maximum flow.</li> </ul> </li> </ul> </li> <li>we define an <strong>\((s, t)\)-cut</strong> for a graph \(G\) to be a split of vertices into two sets \((S, T)\) where \(S \subseteq V\) and \(T \subseteq V\) such that \(s \in S\) and \(t \in T\) and every other vertex in \(V\) is in one of \(S\) or \(T\). more formally, to be a cut, the following requirements need to be satisfied <ul> <li> \[S \cup T = V\] </li> <li> \[S \cap T = \varphi\] </li> <li> \[s \in S, t \in T\] </li> </ul> </li> <li> <p>the <strong>capacity</strong> of a cut \(C(S, T)\) denoted \(c(S, T)\) is defined as the sum of the capacities of the edges \((u, v)\) with \(u \in S\) and \(v \in T\), i.e.</p> \[c(S, T) = \sum_{u \in S} \sum_{v \in T} c(u, v)\] </li> <li>this definition of capacity is somewhat intuitive and in fact it suggests that we can define the flow of a cut in a similar manner.</li> <li> <p>given a cut \(C(S, T)\), with capacity \(c(S, T)\) and a flow \(f\), the <strong>flow</strong> of a cut \(C(S, T)\) is defined as the flow going from \(S\) to \(T\) minus the flow going from \(T\) to \(S\), i.e.</p> \[f(S, T) = \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{u \in S} \sum_{v \in T} f(v, u)\] </li> </ul> <p>we finally stop the dilly-dallying and get to the main point of this post, the max-flow min-cut theorem.</p> <blockquote> <p>max-flow min-cut theorem</p> <p>Let \((\mathcal{G}, s, t, c)\) be a flow network, \(C(S, T)\) be an \(s-t\) cut, and \(f\) be a flow in \(\mathcal{G}\), then the following are equivalent:</p> <ul> <li>\(f\) is a maximum flow in \(\mathcal{G}\).</li> <li>\(\mathcal{G}_f\) contains no augmenting paths.</li> <li>there exists a cut \(C(S, T)\) such that \(c(S, T) = \text{val}(f)\).</li> </ul> </blockquote> <h2 id="the-proof">the proof</h2> <blockquote> <p>Lemma 1</p> <p>Let \((\mathcal{G}, s, t, c)\) be a flow network, \(C(S, T)\) be an \(s-t\) cut, and \(f\) be a flow in \(\mathcal{G}\), then:</p> <p style="overflow-x:auto"> $$\begin{align*}f(S, T) \leq c(S, T)\end{align*}$$ </p> </blockquote> <p><strong>proof:</strong> For every edge \((u, v)\) by a consequence of the capacity constraint, we know that \(f(u, v) \leq c(u, v)\). Therefore, we can ascertain</p> <p style="overflow-x:auto"> $$ \begin{align*} f(S, T) &amp;= \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{u \in S} \sum_{v \in T} f(v, u) &amp;&amp; \text{definition of flow} \\ &amp;\leq \sum_{u \in S} \sum_{v \in T} f(u, v) &amp;&amp;\text{non-negativity of a flow} \\ &amp;\leq \sum_{u \in S} \sum_{v \in T} c(u, v) &amp;&amp;\text{capacity constraint} \\ &amp;= c(S, T) &amp;&amp;\text{definition of capacity} \end{align*} $$ </p> <p>The larger consequence of this lemma is that for <strong>any cut</strong> you choose in the flow network, the flow going through \(C(S, T)\) is always bounded by its capacity.</p> <blockquote> <p>Lemma 2</p> <p>Let \((\mathcal{G}, s, t, c)\) be a flow network, \(C(S, T)\) be an \(s-t\) cut, and \(f\) be a flow in \(\mathcal{G}\), with \(v \in T\), then:</p> <p style="overflow-x:auto"> $$ \begin{align*}f(S, T) = f(S \cup \{v\}, T \setminus \{v\}) \end{align*} $$ </p> </blockquote> <p><strong>proof:</strong> Let \(C(S, T)\) be any \(s-t\) cut and \(v\) an element in \(T\). Remove \(v\) from \(T\) and place it in \(S\). Let us now evaluate the flow of the new cut (call it \(C'(S', T')\)) where \(S' = S \cup \{v\}\) and \(T' = T \setminus \{v\}\).</p> <p>For the sake of notational convenience, we define the following sets</p> <p style="overflow-x:auto"> $$ \textsf{In}(v) = \{(u, v) \in E \mid u \in V\} \quad \text{and} \quad \textsf{Out}(v) = \{(v, w) \in E \mid w \in V\} $$ </p> <p>Intuitively, \(\textsf{In}(v)\) is the set of edges going into \(v\) and \(\textsf{Out}(v)\) is the set of edges going out of \(v\). We know that from the conservation of flow</p> \[\sum \limits_{(u, v) \in \textsf{In}(v)} f(u, v) = \sum \limits_{(v, w) \in \textsf{Out}(v)} f(v, w)\] <p>We now partition \(\textsf{In}(v)\) and \(\textsf{Out}(v)\) based on where the end-points of the edges in the set fall as follows</p> <p style="overflow-x:auto"> $$ \begin{align*} \textsf{In}_{S}(v) &amp;= \big\{(u, v) \in E \mid u \in S\big\} \\ \textsf{In}_{T}(v) &amp;= \big\{(u, v) \in E \mid u \in T\big\} \\ \textsf{Out}_{S}(v) &amp;= \big\{(v, w) \in E \mid w \in S\big\} \\ \textsf{Out}_{T}(v) &amp;= \big\{(v, w) \in E \mid w \in T\big\} \end{align*} $$ </p> <p>Finally let us evaluate the flow of this new cut. Moving \(v\) into \(S\) result in losing \(v\)’s contribution in the original cut capacity but also in gaining the contribution of the edges going out of \(v\), and therefore:</p> <p style="overflow-x:auto"> $$ \begin{align*} f(S', T') &amp;= f(S, T) - \sum \limits_{(u, v) \in \textsf{In}_{S}(v)} f(u, v) - \sum \limits_{(u, v) \in \textsf{In}_{T}(v)} f(u, v) \\ &amp;+ \sum \limits_{(v, w) \in \textsf{Out}_{S}(v)} f(v, w) + \sum \limits_{(v, w) \in \textsf{Out}_{T}(v)} f(v, w) \\ &amp;= f(S, T) - \sum \limits_{(u, v) \in \textsf{In}(v)} f(u, v) + \sum \limits_{(v, w) \in \textsf{Out}(v)} f(v, w) &amp;&amp;\text{since } \textsf{In}_{S}(v) \cup \textsf{In}_{T}(v) = \textsf{In}(v) \\ &amp;= f(S, T) &amp;&amp;\text{by conservation of flow} \end{align*} $$ </p> <blockquote> <p>Lemma 3</p> <p>Let \((\mathcal{G}, s, t, c)\) be a flow network, \(C(S, T)\) be an \(s-t\) cut, and \(f\) be a flow in \(\mathcal{G}\), then:</p> <p style="overflow-x:auto"> $$\begin{align*} f(S, T) = \text{val}(f) \end{align*} $$ </p> </blockquote> <p><strong>proof:</strong> consider applying lemma 2 to the following cut \(C(S, T)\) where \(S = \{s\}\) and \(T = V \setminus \{s\}\), then:</p> \[f(\{s\}, V \setminus \{s\}) = f(\{s\} \cup S', V \setminus (\{s\} \cup S')) = f(S', T')\] <p>Essentially this implies that the flow of <strong>any cut</strong> is equal to the flow of the cut \(C(S, T)\) where \(S = \{s\}\) and \(T = V \setminus \{s\}\). How nice is that!! You may ask why is this nice? Well evaluating the flow of \(C(S, T)\) is much easier than evaluating the flow of any other cut. This is because the flow of \(C(S, T)\) is simply the flow leaving the source and this is the flow of the network :)</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{val}(f) &amp;= \sum_{(u, v) \in E} f(u, v) \\ &amp;= f(\{s\}, V \setminus \{s\}) = f(S, T) &amp;&amp;\text{by lemma 2} \end{align*} $$ </p> <blockquote> <p>Corollary 1</p> <p>Let \((\mathcal{G}, s, t, c)\) be a flow network, \(C(S, T)\) be an \(s-t\) cut, and \(f\) be a flow in \(\mathcal{G}\), then:</p> <p style="overflow-x:auto"> $$\begin{align*}\text{val}(f) \leq c(S, T)\end{align*}$$ </p> </blockquote> <p>now FINALLY, we can proceed with the proof of the max-flow min-cut theorem. we want to show that the 3 points of the theorem are equivalent. in the same vain,</p> <ul> <li>\(1 \implies 2:\) let \(f\) be a max flow and suppose that \(\mathcal{G}_f\) still has an augmenting path \(\mathcal{P}\). then we can increase the flow of \(f\) simply by augmenting \(f\) along \(\mathcal{P}\), which contradicts the assumption that \(f\) is a max flow. therefore \(G_f\) has no augmenting paths and \(f\) is a max flow.</li> <li> <p>\(2 \implies 3:\) suppose that \(\mathcal{G}_f\) has no augmenting paths. consider the cut \(C(S, T)\) such that \(c(S, T) = \text{val}(f)\). let \(S\) denote the set of vertices reachable from \(s\). if there is an augmenting \(s, t\) path from \(s\) to a vertex \(v\), then \(v \in S\). Since there are no augmenting paths, \(t \notin S\). let \(T = V \setminus S\) and therefore \(t \in T\). This implies that \(C(S, T)\) is a valid \(s-t\) cut. We now proceed with evaluating the flow across \(C(S, T)\).</p> <p>Consider an edge \((u, v)\) crossing the cut, i.e. \(u \in S\) and \(v \in T\). By definition \(c_f(u, v)\) denotes the capacity along the edge \((u, v)\) in the residual graph \(\mathcal{G}_f\). As a consequence of the terminating condition of \(\tt{ford-fulkerson}\) we have that \(c_f(u, v) = 0\) (otherwise there would have been an augmenting \(s, v\) path and \(v\) would be reachable from \(s\).)</p> <p>if \((u, v)\) is an edge of \(\mathcal{G}\) (the original graph), then since \(c_f(u, v) = 0\), by definition \(c(u, v) - f(u, v) = 0\) and therefore \(f(u, v) = c(u, v)\). if \((u, v)\) is not an edge of \(\mathcal{G}\) (it was added by the residual graph), then \(f(v, u) = 0\). this gives us that</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{val}(f) &amp;= f(S, T) &amp;&amp;\text{lemma 3} \\ &amp;= \sum \limits_{u \in S} \sum \limits_{v \in T} f(u, v) - \sum \limits_{u \in S} \sum \limits_{v \in T} f(v, u) \\ &amp;= \sum \limits_{u \in S} \sum \limits_{v \in T} f(u, v) &amp;&amp;\text{since } f(v, u) = 0 \\ &amp;= \sum \limits_{u \in S} \sum \limits_{v \in T} c(u, v) &amp;&amp;\text{since } f(u, v) = c(u, v) \\ &amp;= c(S, T) \end{align*} $$ </p> </li> <li>\(3 \implies 1:\) let \(C(S, T)\) be a cut with capacity \(c(S, T) = \text{val}(f)\) and let \(f'\) be the maximum flow in \(\mathcal{G}\), i.e. \(\text{val}(f) \leq \text{val}(f')\). Since \(\text{val}(f) = c(S, T)\), we have that \(c(S, T) \leq \text{val}(f')\). By corollary 1, we have that \(\text{val}(f') \leq c(S, T)\) and therefore \(\text{val}(f) = c(S, T) = \text{val}(f')\). This implies that \(f\) is a max flow and we’re done :)</li> </ul> <h2 id="references">references</h2> <ul> <li><a href="https://courses.cs.washington.edu/courses/cse421/23wi/lecture/18-flow2.pdf">robbie’s perspective on flow from uw’s algos class</a></li> <li><a href="https://www.cs.toronto.edu/~lalla/373s16/notes/MFMC.pdf">a self-contained proof @ toronto</a></li> <li><a href="https://www.math.uchicago.edu/~may/VIGRE/VIGRE2009/REUPapers/Staples-Moore.pdf">yet another proof of könig and menger’s theorems</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[what most people associate with the passage of water and time; a nightmarish combinatorial optimization problem for those in cs.]]></summary></entry><entry><title type="html">so i gave a theory talk</title><link href="https://lukshyaganjoo.github.io/blog/2022/so-i-gave-a-theory-talk/" rel="alternate" type="text/html" title="so i gave a theory talk"/><published>2022-08-12T00:00:00+00:00</published><updated>2022-08-12T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2022/so-i-gave-a-theory-talk</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2022/so-i-gave-a-theory-talk/"><![CDATA[<h2 id="notation-and-some-motivation">notation and some motivation</h2> <h3 id="notation">notation:</h3> <p>Before I get to some of the really cool motivation for this problem, I wanna make sure that everyone here is familiar with the the kinds of graphs we’ll be working with in the domain of this proof.</p> <p style="text-align:center;"> <img src="/assets/img/Petersen1_tiny.png" alt="petersen graph" style="width: 100%; margin: auto;"/> </p> <p>This is the Petersen graph. Veteran graph theoreticians will be familiar with this pentagram inscribed within the spokes of a regular pentagon. The Petersen graph is used as a counter example in tons of graph theory proofs which is one of the reasons it’s so well studied, partly what we’ll be doing here. The Petersen graph has \(10\) nodes and \(15\) edges, and is what we call \(3\)-regular.</p> <p><strong>Note:</strong> A graph is said to be \(d\)-regular iff every vertex has the same degree \(d\). (i.e. it is strongly regular)</p> <p style="text-align:center;"> <img src="/assets/img/full.png" alt="complete graph" style="width: 100%; margin: auto;"/> </p> <p>This is the complete graph on \(10\) vertices. For the sake of convenience, we will refer to this graph as \(K_{10}\). It has \(10\) nodes and \(45\) edges (i.e., all possible edges among \(10\) nodes). If we follow along from the same scheme of defining things, the number of edges in \(K_{n}\) (the complete graph with \(n\) vertices) is given by \(\binom{n}{2}\).</p> <p>In this proof, \(\mathbf{J}_{n}\) refers to the all ones matrix with dimension/size \(n \times n\).</p> <p>The row space of a matrix \(\mathbf{A}\) is defined as follows, \(\text{Row}(A) = \\{\mathbf{y}^{\mathbf{\top}} A : \mathbf{y} \in \mathbb{R}^{n}\\}\)</p> <blockquote> <p><strong>definition</strong></p> <p style="overflow-x:auto"> $$ \textsf{Trace}(\mathbf{A}) = \sum \limits_{x \in E_{x}(\mathbf{A})} \lambda_{x} $$ </p> </blockquote> <blockquote> <p><strong>lemma 1</strong></p> <p>For a symmetric matrix \(\mathbf{A}\), eigenvectors corresponding to <em>distinct</em> eigenvalues are orthogonal</p> </blockquote> <p><strong>proof</strong><br/> Let \(\mathbf{x}\) and \(\mathbf{y}\) be eigenvectors of the matrix \(\mathbf{A}\) corresponding to eigenvalues \(\lambda_1\) and \(\lambda_2\) respectively. Therefore we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbf{y}^{\top} \mathbf{Ax} = \mathbf{y}^{\top} \lambda_1 \mathbf{x} &amp;\implies \mathbf{y}^{\top} \mathbf{A}^\top \mathbf{x} = \mathbf{y}^{\top} \lambda_1 \mathbf{x} \\ &amp;\implies \mathbf{(Ay)}^{\top} \mathbf{x} = \lambda_1 \mathbf{y}^{\top}\mathbf{x} \\ &amp;\implies (\lambda_2 \mathbf{y})^\top \mathbf{x} = \lambda_2 \mathbf{y}^{\top}\mathbf{x} \\ &amp;\implies \mathbf{y}^{\top}\mathbf{x} (\lambda_2 - \lambda_1) = 0 \end{align*} $$ </p> <p>However, we already know that \(\lambda_2 \neq \lambda_1\). Therefore \(\lambda_2 - \lambda_1 \neq 0\), and it must be the case that \(\mathbf{y}^{\top}\mathbf{x} = 0\).</p> <p>Therefore \(x\) and \(y\) are orthogonal.</p> <p><strong>adjacency matrices:</strong></p> <blockquote> <p><strong>definition</strong></p> <p>We define the adjacency matrix of a graph \(G = (V, E)\) as the following,</p> <p style="overflow-x:auto"> $$ A_{ij} = \begin{cases}1 &amp; \text{there is an edge from} \; j \; \text{to} \; i\\0 &amp; \text{there is no edge from} \; j \; \text{to} \; i\\ \end{cases} $$ </p> </blockquote> <p>Since we’re dealing with an undirected graph in this proof, every edge \((a, b)\) is counted as the same edge as if there were an edge \((b, a)\) in the graph. As a consequence, the adjacency matrices for this family of graphs will be symmetric.</p> <p>The way to think about it is that the adjacency matrix of a graph \(G\) encodes information about the number of length \(1\) paths between any pair of vertices \(i\) and \(j\), i.e. information about the vertices immediately “adjacent” to each other.</p> <h3 id="motivation">motivation:</h3> <p>Each node in \(K_{10}\) has \(9\) edges incident to it while each node in the Petersen graph has \(3\) edges incident to it. So it is plausible that \(K_{10}\) can be covered perfectly by \(3\) Petersen graphs. This means that you can lay down three Petersens on \(K_{10}\) so that vertices go to vertices and each edge of \(K_{10}\) lies under an edge of exactly one of the three Petersens. This hints at some pretty cutting-edge stuff in the field of graph decomposition and graph coloring and the parallels in higher dimensions is absolutely incredible. With some linear algebra at our disposal, we will show that it is not possible to completely cover \(K_{10}\) with three Petersen graphs.</p> <hr/> <h2 id="eigenvalues-of-the-petersen-graph">eigenvalues of the petersen graph</h2> <p>Finding the eigenvalues of any matrix, let alone one that of size \(10 \times 10\) is a fairly computational task and isn’t enlightening in any form whatsoever. So why are we trying to find the eigenvalues of the petersen graph? Well we can take advantage of the fact that the adjacency matrix represents information that carries physical meaning (vertex-edge relations of a graph) and can use the language of linear transformations and some geometric intuition to gain insight into the eigenvalues of this graph.</p> <p>Let \(\mathbf{A}\) be the adjacency matrix of the Petersen Graph. If \(\mathbf{A}\) encodes information about the number of length \(1\), \(\mathbf{A}^2\) represents information about the number of length \(2\) paths, and in general, \(A^k\) encodes information about the number of \(k\)-length paths in a given graph. With this information in our belt, we know can make a pretty nifty observation.</p> <p>Consider the \(i, j^{th}\) entry of the matrix \(\mathbf{A}^2 + \mathbf{A}\) (the sum of the matrices representing information about the number of length \(2\) paths and the regular adjacency matrix of the petersen graph). Since, every vertex in the petersen graph has degree \(3\), each vertex has \(3\) length \(2\) paths back itself whereas there are no length \(1\) paths from a vertex to itself. With this information under our belt, we can say that</p> <p style="overflow-x:auto"> $$ (A^2 + A)_{ij} = \begin{cases} 3 &amp; \forall i = j \\ 1 &amp; \forall i \neq j \end{cases} $$ </p> <p>This follows from noticing that any pair of vertices not already joined by an edge are joined by a unique path of length \(2\); namely there is a unique vertex which is joined to both.</p> <p>We can therefore conclude that the matrix \(\mathbf{}\) satisfies the matrix equation \(\mathbf{A}^2 + \mathbf{A} = 2\mathbf{I} + \mathbf{J}\)</p> <p>Note that \(\mathbf{J}\) has eigenvalue 10 (of multiplicity 1 with eigenvector 1) and eigenvalue 0 of multiplicity 9. Thus \(2\mathbf{I} + \mathbf{J}\) has eigenvalue 12 of multiplicity 1 and eigenvalue 2 of multiplicity 9. Now if \(x\) is an eigenvector of A of eigenvalue \(\lambda\) then \(\mathbf{x}\) is an eigenvector of \(\mathbf{A}^2 + \mathbf{A}\) of eigenvalue \(\lambda^2 + \lambda\).</p> <p>Thus the possible eigenvectors for \(\mathbf{A}\) are prescribed. We already know that \(\mathbf{1}\) is an eigenvector of \(\mathbf{A}\) of eigenvalue 3 (or at least it is easy to check). The other 9 eigenvalues must satisfy \(\lambda^2 + \lambda = 2\) thus either 1 or -2 with total multiplicity being 9. Now the trace of \(\mathbf{A}\), which is 0, is the sum of the eigenvalues and so we deduce that 1 has multiplicity 5 and -2 has multiplicity 4.</p> <hr/> <h2 id="the-actual-proof">the actual proof</h2> <blockquote> <p><strong>lemma 2</strong></p> <p>If \(\mathbf{K}_{10}\) represents the adjaceny matrix of the complete graph on 10 vertices, \(\mathbf{J}_{10}\) represents the matrix with all its entries being equal to \(1\) and \(\mathbf{I}_{10}\) represents the identity matrix of size \(10 \times 10\), then</p> <p style="overflow-x:auto"> $$ \begin{align*}\mathbf{K}_{10} = \mathbf{J}_{10} - \mathbf{I}_{10}\end{align*} $$ </p> </blockquote> <p><strong>proof:</strong> The adjacency matrix of \(K_{10}\) has all but its diagonal entries being equal to \(1\), since every node in the full graph is connected to every other node except itself. The diagonal entries of \(\mathbf{K}_{10}\) are filled in with \(0\)’s since there is no path from any of the nodes to itself. One can also clearly see that the matrix described by the equation \(\mathbf{J}_{10} - \mathbf{I}_{10}\) is a matrix that has all but its diagonal entries being equal to \(1\) and its diagonal entries being equal to \(0\).</p> <p>Therefore the adjacency matrix of \(\mathbf{K}_{10} = \mathbf{J}_{10} - \mathbf{I}_{10}\)</p> <blockquote> <p><strong>Assumption:</strong></p> <p style="overflow-x:auto:"> $$ \begin{align*}\mathbf{A}_P + \mathbf{A}_Q + \mathbf{A}_R = J_{10} - I_{10}\end{align*} $$ </p> </blockquote> <p>We consider three permutation matrices of the Petersen Graph \(A_P, A_Q\) and \(A_R\) (matrices formed by simultaneous row and column swaps in the original adjacency matrix of the petersen graph). We construct each of these permutation matrices such that \(A_Q = CAC^{-1}\) where \(C\) is some matrix. By this construction, \(A_Q\) and \(A\) are similar, and an important consequence is that they have the same eigenvalues. This assumption and construction is true for \(A_P\) and \(A_R\).</p> <blockquote> <p><strong>lemma 3</strong></p> <p style="overflow-x:auto"> $$ \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} $$ </p> </blockquote> <p><strong>proof:</strong> We wish to show that \(\mathbf{1}^{\top} = \mathbf{y}^{\top}(\mathbf{A}_P - I_{10})\) for some vector \(\mathbf{y} \in R^{10}\). Consider \(\mathbf{y} = \mathbf{1}^{\top}\). The matrix product \(\mathbf{1}^{\top}(\mathbf{A}_P - \mathbf{I}_{10})\) yields the vector whose entries are the sum of the entries of the columns of \(\mathbf{A}_P - \mathbf{I}_{10}\). for an arbitrary column \(i\) in its adjacency matrix, by the way the Petersen graph was defined we know that he graph has 3 outgoing edges which means that the sum of the entries of a column in its adjacency matrix is 3. However note that the diagonal entries of the adjacency matrix will be 0 since the Petersen graph does not contain self-loops. After subtracting the \(i\)th column of \(\mathbf{I}_{10}\), the sum of the columns of the matrix \(\mathbf{A}_P - \mathbf{I}_{10}\) is 2. Note that the identity matrix has 0 in every entry where \(i \neq j\). Since the above holds true for an arbitrary column of the matrix, We can conclude that every column of \(\mathbf{A}_P - \mathbf{I}_{10}\) sums up to a 2. More fundamentally, the product \(\mathbf{1}^{\top}(\mathbf{A}_{P} - \mathbf{I}_{10})\) generates the all twos vector or 2\(\mathbf{1}^{\top}\).</p> <p>Therefore we can assert that \(\text{Span}(\mathbf{1}) \subseteq \text{Row} (\mathbf{A}_P - \mathbf{I}_{10})\).</p> <blockquote> <p><strong>side proof</strong></p> <p>Let \(A\) and \(B\) be two subspaces such that \(A \subseteq B\), then</p> <p style="overflow-x:auto"> $$ B^{\perp} \subseteq A^{\perp} $$ </p> </blockquote> <p><strong>proof</strong> Let \(\mathbf{x} \in B^{\perp}\). By definition of the orthogonal complement, we have that \(\forall \mathbf{v} \in B, \mathbf{x}^{\top}\mathbf{v} = 0\). However we know that any vector contained in \(B\) will be a vector contained in \(A\), (\(A\) is a subset of the vectors contained in \(B\)). Therefore \(\mathbf{x}\) is orthogonal to every vector in \(A\). By definition, this means that \(\mathbf{x} \in A^{\perp}\) \(\therefore A \subseteq B \implies B^{\perp} \subseteq A^{\perp}\)</p> <p>Using the aforementioned proof and taking orthogonal complements, we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \text{Span}(\mathbf{1}) \subseteq \text{Row} (\mathbf{A}_P - \mathbf{I}_{10}) &amp;\implies \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} \\ &amp;\implies \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} \end{align*} $$ </p> <p>The above results are also true for \(\mathbf{A}_Q - \mathbf{I}_{10}\) since \(Q\) is also a Petersen graph.</p> <blockquote> <p><strong>lemma 4</strong></p> <p>Assuming the standard definitions of the matrices \(A_P, A_Q\) and \(A_R\), we have that</p> <p style="overflow-x:auto"> $$ \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \cap \text{Null} (\mathbf{A}_Q - \mathbf{I}_{10}) \neq \{0\} $$ </p> </blockquote> <p><strong>proof</strong> For two subspaces \(R_1\) and \(R_2\) of \(\mathbb{R}^{9}\), if \(S_1 \cap S_2 = \{0\}\), then \(B_{S_1}\) and \(B_{S_2}\) are linearly independent. We therefore prove by contrapositive.</p> <p>We also know that</p> <p style="overflow-x:auto"> $$ \text{Null} (\mathbf{A}_P - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} \quad \text{and} \quad \text{Null} (\mathbf{A}_Q - \mathbf{I}_{10}) \subseteq \text{Span} (\mathbf{1})^{\perp} $$ </p> <p>\(B_{\text{Null} (\mathbf{A}_P - \mathbf{I}_{10})} \cup B_{\text{Null} (\mathbf{A}_Q - \mathbf{I}_{10})}\) has 10 vectors in \(\mathbb{R}^{9}\) since the bases of each of the null spaces has 5 vectors. Therefore \(B_{\text{Null} (\mathbf{A}_P - \mathbf{I}_{10})} \cup B_{\text{Null} (\mathbf{A}_Q - \mathbf{I}_{10})}\) must be linearly dependent (since they have more vectors than the dimension of the space they exist in).</p> <p>Therefore \(B_{S_1} \cap B_{S_2} \neq \{0\}\) where \(S_1\) and \(S_2\) refer to the two subspaces above. This implies the existence of a non-zero vector \(w\) that exists in their intersection.</p> <p>The larger consequence of this is that this vector \(\mathbf{w}\) is orthogonal to the all ones vector, i.e. \({\bf 1}^{\top} \mathbf{w} = 0\)</p> <hr/> <h2 id="the-final-stages">the final stages</h2> <p style="overflow-x:auto"> $$ \begin{align*} \mathbf{A}_P(\mathbf{v}) &amp;= \lambda \mathbf{v} = \mathbf{v} &amp;&amp;\text{Since 1 is an eigenvalue of $$\mathbf{A}_P$$} \\ \mathbf{A}_R(\mathbf{w}) &amp;= \mathbf{J}_{10}(\mathbf{w}) - \mathbf{I}_{10} (\mathbf{w}) - \mathbf{A}_P(\mathbf{w}) - \mathbf{A}_Q(\mathbf{w}) \\ &amp;= \mathbf{J}_{10}(\mathbf{w}) - \mathbf{w} - \mathbf{w} - \mathbf{w} \end{align*} $$ </p> <p>We now compute \(J_{10} (w)\) where \(w \in \text{Null} (A_P - I_{10})\)</p> <p style="overflow-x:auto"> $$ \mathbf{J}_{10}\mathbf{w} = \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \end{bmatrix} \mathbf{w} = \begin{bmatrix} \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \\ \mathbf{1}^{\top} \end{bmatrix}w = \begin{bmatrix} \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \\ \mathbf{1}^{\top} \mathbf{w} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} $$ </p> \[\therefore \mathbf{A}_R(\mathbf{w}) = \mathbf{J}_{10}(\mathbf{w}) - \mathbf{w} - \mathbf{w} - \mathbf{w} = -3\mathbf{w}.\] <p>We have therefore arrived at a contradiction. In the beginning of the proof, we found all the eigenvalues of the Petersen Graph and concluded that -3 was not amongst that list. However as we can see, we have proved that -3 in fact is an eigenvalue for the adjacency matrix of a Petersen graph. Therefore it is not possible to cover the full graph or \(K_{10}\) with 3 Petersen graphs.</p> <hr/> <h2 id="key-takeaways">key takeaways</h2> <p>How cool is this!! All we did here was take advantage of some fairly simple linear algebra to prove a really fundamental fact about graphs that pop up quite frequently in the study of networks. For those interested, there is a conjecture in the “colorful” field of graph decomposition by the name of <a href="https://www.quantamagazine.org/mathematicians-prove-ringels-graph-theory-conjecture-20200219/">Ringel’s Conjecture</a> that actually illustrates how exactly you can cover complete graphs of this variety. If there’s anything I want you to take away from this proof, it’s that geometric intution is unparalleled and this technique of analyzing eigenvalues not only can save you computation, it gives you critical insight into what the graph actually represents, a deeper understanding of what it “does”, so to speak. It is so easy to get bogged down in what might just seem like computation after computation. But there is a bigger picture, and that bigger picture is usually a bigger graph.</p> <hr/> <h2 id="higher-dimensions-and-moore-graphs">higher dimensions and moore graphs</h2> <p><strong>degree-diameter problem:</strong> In graph theory, the degree diameter problem is the problem of finding the largest possible graph \(G\) (in terms of the size of its vertex set \(V\)) of diameter \(d\) such that the largest degree of the vertices in \(G\) is at most \(k\). The size of \(G\) is bounded above by the Moore bound; In general, the largest degree-diameter graphs are much smaller than as prescribed by the Moore bound.</p> <p><strong>the moore bound and moore graphs</strong> The moore bound gives us an upper bound on how many nodes a graph can contain with diameter \(d\) and maximum degree \(k\). An intuitive way to recognize what the moore bound actually tells us is how <em>wide</em> a graph can get. Let \(M\) be the moore bound that a graph of the above specifications can meet, we therefore have that</p> \[M = 1 + k \sum_{i = 0}^{d - 1} (k - 1)^i\] <p>Graphs that attain this bound \(M\) are known as Moore graphs.</p> <p><strong>obtaining the moore bound</strong></p> <p style="text-align:center;"> <img src="/assets/img/notes.jpg" alt="moore graphs" style="width: 100%; margin: auto;"/> </p> <p>Consider the breadth first search of a graph \(G\). From the above picture, we can clearly see that level 0 has only one node, this being the root of the tree generated by breadth first search. Level 1 has at most \(k\) nodes since the maximum degree of any node in the graph is given by \(k\). For each node \(i\) in Level 1, \(i\) can be connected to at most \(k - 1\) nodes (it’s important not to forget the node they are connected to in Level 0). Since there are \(k\) nodes in the first level, the maximum number of nodes in level 2 is given by \(k(k - 1)\).</p> <p>Generalizing, we have that the number of nodes in a level \(j\) \(\leq k(k - 1)^{j - 1}\).</p> <p>Notice that we can count the nodes from level 1 to \(d\) (we can’t have more than \(d\) levels) using a sum given by \(\sum_{i = 1}^{d} k(k - 1)^{i - 1}\). We also have to account for the root node which adds 1 to this total sum formula. After some substitution of variables, we get that the final expression for the upper bound comes out to be</p> \[M = 1 + k \sum_{i = 0}^{d - 1} (k - 1)^{i - 1}\] <p><strong>types of moore graphs and its relation to the petersen graph</strong> The entire reason we’re even talking about moore graphs is that the petersen graph is a very specific kind of moore graph. Remember that equation we found for the adjacency matrix of the petersen graph. Well for the moore graphs that are known to exist, they satisfy the same “general form” of that equation.</p> <blockquote> <p><strong>lemma 5</strong></p> <p>The only \(d\)-regular Moore graphs of girth 5 and diameter 2 exist for \(d = 2, 3, 7\) and possibly \(57\).</p> </blockquote> <p><strong>proof</strong> Assume \(G\) is a \(d\)-regular Moore graph of girth 5. A reminder that the girth of a graph is defined to be \(2k + 1\) where \(k\) is the diameter of the graph. Solving the above equation for the girth, we get that the diameter of the graph must be 2, i.e. \(k = 2\). Substituting in this value into the Moore bound, we get that the number of the vertices in the graph is given by</p> \[n = 1 + d + d(d- 1) = d^{2} + 1\] <p>As we did for the petersen graph, we consider the square of the adjacency matrix \(A^{2}\) once again. Notice that the adjacenct vertices don’t share any neighbours since if they did, there would be a triangle in \(G\). Non-adjacent vertices share exactly one neighbor, because the diameter of \(G\) is 2. Hence, \(A^{2}\) has \(d\) on the diagonal, 0 for edges and 1 for non-edges.</p> <p>In other words, we have that</p> <p style="overflow-x:auto"> $$ (A^{2} + A)_{i, j} = \begin{cases} d &amp; \forall i = j \\ 1 &amp; \forall i \neq j \end{cases} $$ </p> <p>We therefore have that \(\mathbf{A}\) satisfies the equation \(\mathbf{A}^{2} + \mathbf{A} = (d - 1)\mathbf{I} + \mathbf{J}\)</p> <blockquote> <p><strong>lemma 6</strong></p> <p>If \(\lambda\) is an eigenvalue of \(A\) different from \(d\), we get from the above equation that</p> \[\lambda^{2} + \lambda - (d - 1) = 0\] </blockquote> <p><strong>proof</strong> I suppose I should have talked about this earlier, but the all ones matrix \(\mathbf{J}_d\) has eigenvalues 0 and \(d\), where the eigenvalue \(d\) corresponds to an eigenspace with the all ones vector or \({\bf 1}\).</p> <p>Let \(x\) be the eigenvector corresponding to eigenvalue 0. Multiplying both sides of the equation we have that</p> <p style="overflow-x:auto"> $$\begin{align*} \mathbf{A}^{2}\mathbf{x} + \mathbf{A}x &amp;= (d - 1)\mathbf{x} = 0 \\ \lambda^{2}\mathbf{x} + \lambda \mathbf{x} &amp;= (d - 1)\mathbf{x} \implies \lambda^2 + \lambda - (d - 1) = 0 \end{align*} $$ </p> <p><strong>obtaining the parameters for which moore graphs exist</strong><br/> We can use the quadratic formula to ascertain that the roots of the equation \(ax^2 + bx + c = 0\) are given by \(x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\). Plugging in constants from the equation above, we have that \(\lambda = \frac{-1 \pm \sqrt{1 + 4(d - 1)}}{2} = - \frac{1}{2} \pm \frac{\sqrt{4d - 3}}{2}\).</p> <p>Assume that \(\frac{-1}{2} + \frac{\sqrt{4d - 3}}{2}\) has multiplicity \(a\) and \(\frac{-1}{2} - \frac{\sqrt{4d - 3}}{2}\) has multiplicity \(b\). Using the fact that the trace of a matrix \(A\) is the sum of its diagonal entries, we can ascertain that</p> <p style="overflow-x:auto"> $$\begin{align*} d(1) + \left(\frac{-1}{2} + \frac{\sqrt{4d - 3}}{2}\right)a + \left(\frac{-1}{2} - \frac{\sqrt{4d - 3}}{2}\right)b &amp;= 0 \implies d - \frac{a + b}{2} + \frac{1}{2} \; (a - b) \sqrt{4d - 3} = 0 \end{align*}$$ </p> <p>However we know that \(a + b = n - 1\) and from our previous computation of \(n\), we get that \(a + b = d^{2}\). Substituting in, we get that</p> <p style="overflow-x:auto"> $$\begin{align*} d - \frac{d^2}{2} + \frac{1}{2} \; (a - b) \sqrt{4d - 3} &amp;= 0 \implies (a - b) \sqrt{4d - 3} = d^{2} - 2d \end{align*}$$ </p> <p>This statement is only true if \(a = b\) and \(d = 2\) (the trivial case of both sides being 0) or else \(4d - 3\) is a square.</p> <p>Let \(4d - 3 = s^{2}\). Therefore, we have that</p> <p style="overflow-x:auto"> $$\begin{align*} d - \frac{d^2}{2} + \frac{s}{2} \; (a - b) &amp;= 0 \implies d = \frac{s^{2} + 3}{4} \\ \frac{1}{4} \; (s^{2} + 3) - \frac{1}{2} \; (s^2 + 3)^{2} + \frac{s}{2} \; (2a - \frac{1}{16} \; (s^2 + 3)^{2}) &amp;= 0 &amp;&amp;\text{Substituting in $$d$$} \\ s^{5} + s^{4} + 6s^{3} - 2s^2 + (9 - 32a)s - 15 &amp;= 0 &amp;&amp;\text{After an ungodly amount of math :(} \end{align*} $$ </p> <p>To satisfy the above equation, we have that \(s\) must divide 15, i.e. \(s \in \{1, 3, 5, 15 \}\). Since \(s^2 = 4d - 3\), we have that \(d \in \{1, 3, 7, 57 \}\).</p> <p>When \(d = 1\), we get the complete graph on 2 vertices, i.e. \(K_2\) which is not a Moore graph since it doesn’t meet the Moore bound.</p> <p>For all the other cases, we have that</p> <ul> <li>\(d = 2\): \(C_5\)</li> <li>\(d = 3\): Petersen Graph</li> <li>\(d = 7\): Hoffman Singleton Graph</li> <li>\(d = 57\): Open problem if this graph exists :0</li> </ul> <p>There’s a lemma floating around the internet somewhere where it states that Moore graphs don’t exist for diameters greater than 2 which is kind of insane if you think about it. It’s also the reason their existence is so fascinating and why we’ve listed most of if not all the Moore graphs there can be.</p> <h2 id="references">references</h2> <ul> <li><a href="https://cp4space.hatsya.com/2013/09/06/ten-things-you-possibly-didnt-know-about-the-petersen-graph/">fun facts about the petersen graph</a></li> <li><a href="https://www.combinatorics.org/ds14">a survey on the degree-diameter problem</a></li> <li>progress on the degree-diameter problem <ul> <li><a href="https://www.tandfonline.com/doi/epdf/10.1080/09728600.2013.12088753?needAccess=true">trees and pseudotrees</a></li> <li><a href="https://arxiv.org/abs/2401.11187">plane graphs with pentagonal faces</a></li> </ul> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[the petersen graph is in almost every sense of the word; a universal counter-example]]></summary></entry><entry><title type="html">cauchy schwarz and beyond</title><link href="https://lukshyaganjoo.github.io/blog/2022/cauchy/" rel="alternate" type="text/html" title="cauchy schwarz and beyond"/><published>2022-07-30T00:00:00+00:00</published><updated>2022-07-30T00:00:00+00:00</updated><id>https://lukshyaganjoo.github.io/blog/2022/cauchy</id><content type="html" xml:base="https://lukshyaganjoo.github.io/blog/2022/cauchy/"><![CDATA[<h2 id="exposition">exposition</h2> <p>Let’s start from the beginning. We will state some different forms of the cauchy schwarz inequality, prove the form involving dot products and cover what I think is a really cool problem that uses the inequality.</p> <p>For the different forms of the cauchy schwarz inequality, we have</p> <ul> <li>Probability Theory: \(\lvert \mathbb{E}(XY) \rvert^{2} \leq \mathbb{E}(X^{2}) \mathbb{E}(Y^{2})\).</li> <li>\(\mathbb{R}^2\)-dot product: \(\vert {\bf a} \cdot {\bf b} \vert^{2} \leq ({\bf a} \cdot {\bf a}) \; ({\bf b} \cdot {\bf b})\) .</li> <li>\(\mathbb{R}^{n}\): \(n\)-dimensional Euclidean Space: \(\left(\sum \limits_{i = 1}^{n} u_{i} v_{i}\right)^{2} \leq \left(\sum \limits_{i = 1}^{n} u_i\right)^{2} \left(\sum \limits_{i = 1}^{n} v_i\right)^{2}\)</li> </ul> <h2 id="the-actual-theorem">the actual theorem</h2> <blockquote> <p>cauchy schwarz theorem</p> <p>For all vectors \({\bf a}\) and \({\bf b}\) in \(\mathbb{R}^{n}\), we have that</p> <p style="overflow-x:auto"> $$\vert\ {\bf a} \cdot {\bf b} \vert^{2} \leq \vert \vert {\bf a} \vert \vert^{2} \vert \vert {\bf b} \vert \vert^{2}$$ </p> </blockquote> <h2 id="a-proof-based-on-first-principles">a proof based on first principles</h2> <p><strong>proof 1:</strong></p> <p>Let \(\mathbf{a}\) and \(\mathbf{b}\) be vectors living in \(\mathbb{R}^{n}\) such that \(\mathbf{a} = (a_1, a_2, a_3, \dots, a_n)\) and \(\mathbf{b} = (b_1, b_2, b_3, \dots, b_n)\). We first simplify both sides of the inequality, and proceed from there.</p> <p>We have that,</p> <p style="overflow-x:auto"> $$ \begin{align*} \lvert\ {\bf a} \cdot {\bf b}\rvert^{2} &amp;= ({\bf a} \cdot {\bf b}) ({\bf a} \cdot {\bf b}) \\ &amp;= \langle a_1, a_2, a_3, \dots, a_n \rangle \langle b_1, b_2, b_3, \dots, b_n \rangle \\ &amp;= (a_1 b_1 + a_2 b_2 + \dots + a_n b_n) (a_1 b_1 + a_2 b_2 + \dots + a_n b_n) \\ &amp;= (a_1 b_1 + a_2 b_2 + \dots + a_n b_n)^{2} \\ &amp;= \left(\sum_{i = 1}^{n} a_i b_i\right)^2 = D^2 \\ ({\bf a} \cdot {\bf a}) &amp;= \langle a_1, a_2, a_3, \dots, a_n \rangle \langle a_1, a_2, a_3, \dots, a_n \rangle \\ &amp;= a_{1}^{2} + a_{2}^{2} + \dots a_{n}^{2} \\ &amp;= \sum_{i = 1}^{n} a_{i}^{2} = A \\ ({\bf b} \cdot {\bf b}) &amp;= \langle b_1, b_2, b_3, \dots, b_n \rangle \langle b_1, b_2, b_3, \dots, b_n \rangle \\ ({\bf b} \cdot {\bf b}) &amp;= b_{1}^{2} + b_{2}^{2} + \dots + b_{n}^{2} \\ &amp;= \sum_{i = 1}^{n} b_{i}^{2} = B \end{align*} $$ </p> <p>So we are therefore tasked with proving the following inequality, \(D^{2} \leq AB\). Rearranging provides</p> <p style="overflow-x:auto"> $$ \begin{align*} &amp;\ D^{2} \leq AB \\ &amp;\ D^{2} - AB \leq 0 \\ &amp;\ 4D^{2} - 4AB \leq 0 &amp;&amp;\text{Multiplying both sides by 4} \\ &amp;\ (2D)^{2} - 4AB \leq 0 \end{align*} $$ </p> <p>This might look familiar to some people already but given a quadratic equation \(ax^{2} + bx + c = 0\), the discrimanant \(D_{Q}\) is given by \(b^{2} - 4ac\).</p> <p>Given this observation, we can prove an equivalent statement to the inequality we wish to show, namely that the quadratic equation given by \(Ax^2 + 2Dx + B\) has no real roots or exactly one real root. This is because the discrimanant of this quadratic equation is given by \(4D^{2} - 4AB\), and the same would imply that \(4D^{2} - 4AB \leq 0\), which is exactly what we want :)</p> <p>Substituting in the values for \(A\), \(B\), and \(D\), we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} Ax^2 + 2Dx + B &amp;= \left(a_1^2 + \dots + a_n^2\right) x^2 + 2 \left(a_1 b_1 + \dots + a_n b_n\right) x + \left(b_1^2 + \dots + b_n^2\right) \\ &amp;= \left(a_1^2 x^2 + \dots + a_n x^2\right) + 2 \left(a_1 b_1 x + \dots + a_n b_n x\right) + \left(b_1^2 x^2 + \dots + b_n^2 x^2\right) \\ &amp;= \left(a_1^2 x^2 + 2a_1 b_1 x + b_1^2\right) + \dots + \left(a_n^2 x^2 + 2a_n b_n x + b_n^2\right) \\ &amp;= \left(a_1 x + b_1\right)^2 + \dots + \left(a_n x + b_n\right)^2 \\ &amp;\geq 0 \end{align*} $$ </p> <ul> <li><strong>case 1:</strong> \(Ax^2 + 2Dx + B &gt; 0\) In this case, we have that the quadratic equation has no real roots, which is exactly one of the conditions we want.</li> <li><strong>case 2:</strong> \(Ax^2 + 2Dx + B = 0\) Note that this is equivalent to saying that \(\left(a_1 x + b_1\right)^2 + \dots + \left(a_n x + b_n\right)^2 = 0\) which necessarily implies that for all \(i \in \{1, 2, \dots, n\}\), \(a_i x + b_i = 0\), which implies that the equation only has one real root, thereby satisfying the second condition we want and completing the proof.</li> </ul> <h2 id="calculus-is-nice-i-guess">calculus is nice, i guess</h2> <p><strong>proof 2:</strong> This is a proof that at least to me feels slightly more natural, and personally I think this proof technique is cooler (I may be biased here though).</p> <p>We begin by defining the function \(f(t) = \|\mathbf{x} - t\mathbf{y}\|^2 = \langle \mathbf{x} - t\mathbf{y}, \mathbf{x} - t\mathbf{y} \rangle\). Simplifying, this gives us</p> <p style="overflow-x:auto"> $$ \begin{align*} f(t) &amp;= \langle \mathbf{x} - ty, \mathbf{x} - t\mathbf{y} \rangle \\ &amp;= \langle \mathbf{x}, \mathbf{x} - t\mathbf{y} \rangle - t \langle y, \mathbf{y} - t\mathbf{y} \rangle \\ &amp;= \langle \mathbf{x}, \mathbf{x} \rangle - t\langle \mathbf{x}, \mathbf{y} \rangle - t \left(\langle \mathbf{y}, \mathbf{x} \rangle - t \langle \mathbf{y}, \mathbf{y} \rangle \right) \\ &amp;= \langle \mathbf{x}, \mathbf{x} \rangle - 2t \langle \mathbf{x}, \mathbf{y} \rangle + t^2 \langle \mathbf{y}, \mathbf{y} \rangle \\ &amp;= \|\mathbf{x}\|^2 - 2t \langle \mathbf{x}, \mathbf{y} \rangle + t^2 \|\mathbf{y}\|^2 \end{align*} $$ </p> <p>Since \(f(t) = \|\mathbf{x} - t\mathbf{y}\|^2\), we have that \(f(t) \geq 0\) for all \(t \in \mathbb{R}\). We make the clever observation of exploiting this fact for the minima of \(f\), and proceed to find the derivative, set it equal to 0, and solve for \(t_{\text{min}}\).</p> <p style="overflow-x:auto"> $$ \begin{align*} f'(t) &amp;= -2 \langle \mathbf{x}, \mathbf{y} \rangle + 2t \|\mathbf{y}\|^2\\ 0 &amp;= -2 \langle \mathbf{x}, \mathbf{y} \rangle + 2t_{\text{min}} \|\mathbf{y}\|^2 &amp;&amp;\text{Setting $$f'(t) = 0$$} \\ -2 \langle \mathbf{x}, \mathbf{y} \rangle &amp;= 2t_{\text{min}} \|\mathbf{y}\|^2 \\ \langle \mathbf{x}, \mathbf{y} \rangle &amp;= t_{\text{min}} \|\mathbf{y}\|^2 &amp;&amp;\text{Dividing both sides by 2} \\ t_{\text{min}} &amp;= \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2} &amp;&amp;\text{Dividing both sides by $$|\mathbf{y}|^2$$} \end{align*} $$ </p> <p>Evaluating, we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} f(t_{\text{min}}) &amp;= \|\mathbf{x}\|^2 - 2t_{\text{min}} \langle \mathbf{x}, \mathbf{y} \rangle + t_{\text{min}}^2 \|\mathbf{y}\|^2 \\ &amp;= \|\mathbf{x}\|^2 - 2 \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2} \langle \mathbf{x}, \mathbf{y} \rangle + \left(\frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2}\right)^2 \|\mathbf{y}\|^2 \\ &amp;= \|\mathbf{x}\|^2 - 2 \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{y}\|^2} \langle \mathbf{x}, \mathbf{y} \rangle + \left(\frac{\langle \mathbf{x}, \mathbf{y} \rangle^2}{\|\mathbf{y}\|^2}\right) \\ &amp;= \frac{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2 - 2 \langle \mathbf{x}, \mathbf{y} \rangle^2 + \langle \mathbf{x}, \mathbf{y}\rangle^2}{\|\mathbf{y}\|^2} \\ &amp;= \frac{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2 - \langle \mathbf{x}, \mathbf{y} \rangle^2}{\|\mathbf{y}\|^2} \geq 0 \\ &amp;\implies \|\mathbf{x}\|^2 \|\mathbf{y}\|^2 - \langle \mathbf{x}, \mathbf{y}\rangle^2 \geq 0 \\ &amp;\implies \langle \mathbf{x}, \mathbf{y} \rangle \leq \|\mathbf{x}\| \|\mathbf{y}\| \end{align*} $$ </p> <h2 id="the-promised-probabilistic-result">the promised probabilistic result</h2> <blockquote> <p><strong>fun lemma :D</strong></p> <p>For a non-negative random variable \(Y\), we have that</p> <p style="overflow-x:auto"> $$\frac{\mathbb{E}[Y]^2}{\mathbb{E}[Y^2]} \leq \Pr(Y \neq 0) \leq \mathbb{E}[Y]$$ </p> </blockquote> <p><strong>proof:</strong> We first prove the right-hand side of the inequality, i.e. \(\Pr(Y \neq 0) \leq \mathbb{E}[Y]\). Using the definition of expectation, we have that</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y] &amp;= \sum_{y \in \Omega_{y}} y \cdot \Pr(Y = y) \\ &amp;= \sum_{\substack{y \in \Omega_{y} \\ y \neq 0}} y \cdot \Pr(Y = y) \geq \sum_{y \in \Omega_{y}, y \neq 0} \Pr(Y = y) \geq \Pr(Y \neq 0) \end{align*} $$ </p> <p>We now prove the left-hand side of the inequality, Consider the expression \(\mathbb{E}[Y^2] \Pr(Y \neq 0)\). We have that the following is true</p> <p style="overflow-x:auto"> $$ \begin{align*} \mathbb{E}[Y]^2 \Pr(Y \neq 0) &amp;= \sum_{y \neq 0} \underbrace{\Pr(Y = y)}_{a_i = \sqrt{\Pr(Y = y)}} \; \sum_{y \neq 0} \underbrace{y^{2} \Pr(Y = y)}_{b_i = y \sqrt{\Pr(Y = y)}} \\ &amp;\geq \left(\sum_{y \neq 0} y \sqrt{\Pr(Y = y)} \sqrt{\Pr(Y = y)}\right)^2 &amp;&amp;\text{Via Cauchy Schwarz} \\ &amp;\geq \left(\sum_{y \neq 0} y \Pr(Y = y)\right)^2 = \mathbb{E}[Y]^2 \end{align*} $$ which implies the result. </p> <h2 id="references">references</h2> <ul> <li><a href="http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/CSMC_index.html">the cauchy schwarz masterclass</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[an inequality so ubiquitious, folks have written books on its usage.]]></summary></entry></feed>